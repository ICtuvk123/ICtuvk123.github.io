[{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f} \\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$: $$ \\mathcal{R} = \\mathbb{E}_{(x,y) \\sim P}[L(\\widetilde{f}(x), f(x))] $$\nSuch constructed functions can approximate almost any function (Universal Approximation Theorem), but this does not mean we do not need inductive biases to constrain the learning problem. Given a function class $\\mathcal{F}$ with universal approximation capabilities, we can define a complexity function: $$c: \\mathcal{F} \\rightarrow \\mathbb{R}$$ defining our interpolation problem as: $$\\widetilde{f} = \\arg\\min_{g \\in \\mathcal{F}} c(g) \\quad s.t. \\quad \\widetilde{f}(x_i) = f(x_i), \\quad i = 1, . . . , N$$\nTherefore, we not only hope for good function fitting but also low function complexity. Such a complexity function is what we call the norm of the function. The benefit of this definition is that $\\mathcal{F}$ becomes a Banach space (a complete vector space equipped with a norm), allowing us to use tools from functional analysis to study their properties.\nGeometric Priors To overcome the \u0026ldquo;curse of dimensionality\u0026rdquo; of high-dimensional data, we must utilize the physical characteristics of the data (symmetry and scale separation); although the geometry of the data (Domain) may be complex, the signals defined on it constitute a Hilbert space with good mathematical properties, allowing us to still use tools from linear algebra and functional analysis to handle them.\nFor example, in image processing, data is typically represented as signals (pixel values) defined on a two-dimensional Euclidean space $\\mathbb{R}^2$. This space possesses translation and rotation symmetry, meaning that if we translate or rotate the image, its content and structure remain unchanged. Convolutional Neural Networks (CNNs) exploit this translation symmetry, capturing local features through shared weights, thereby improving learning efficiency and generalization ability.\nSymmetry and Invariance In machine learning, symmetry refers to the property that data or tasks remain unchanged under certain transformations. These transformations can be translation, rotation, scaling, etc. For example, in an image classification task, if we translate or rotate an image, the category of the image usually does not change. This invariance is what we hope the model can capture and utilize.\nSymmetry Groups Symmetry can be formalized through group theory. A group is a set equipped with a binary operation that satisfies properties such as closure, associativity, identity, and inverse.\nSymmetry operations form a group, called a symmetry group, proven as follows:\nClosure: If $g_1$ and $g_2$ are symmetry operations, then their composition $g_1 \\circ g_2$ is also a symmetry operation, because applying two symmetry transformations consecutively still preserves the invariance of the data. Associativity: If $g_1$ and $g_2$ are symmetry operations, then for any $g_3$, $(g_1 \\circ g_2) \\circ g_3 = g_1 \\circ (g_2 \\circ g_3)$, because the order of transformations does not affect the final result. Identity: There exists an identity operation $e$ such that for any symmetry operation $g$, $e \\circ g = g \\circ e = g$. This identity operation corresponds to performing no transformation. Inverse: For every symmetry operation $g$, there exists an inverse operation $g^{-1}$ such that $g \\circ g^{-1} = g^{-1} \\circ g = e$. This inverse operation corresponds to undoing the transformation. Note: $g \\circ h$ denotes applying transformation $g$ first, then transformation $h$.\nLet\u0026rsquo;s take an equilateral triangle as an example to illustrate the concept of symmetry groups: In this example, the symmetry group of the equilateral triangle contains six elements: three rotation operations (0 degrees, 120 degrees, 240 degrees) and three reflection operations (perpendicular lines through each vertex). These operations satisfy the four properties of a group, thus forming a group called $D_3$, the dihedral group of the triangle. Mathematically, we can view these operations as mappings: $$g: \\mathcal{X} \\rightarrow \\mathcal{X}$$ where $\\mathcal{X}$ is the data space. For each element $g \\in G$ in a symmetry group $G$, we have a corresponding mapping $g$. For example, $D_3$ can also be represented as a matrix group: $$ D_3 = \\left\\{ \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 2 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 3 \u0026amp; 1 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 1 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 3 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 1 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 2 \u0026amp; 1 \\end{pmatrix} \\right\\} $$\nReferences Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., \u0026amp; Vandergheynst, P. (2017). Geometric deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4), 18-42. Goodfellow, I., Bengio, Y., \u0026amp; Courville, A. (2016). Deep learning. MIT press. Cohen, T. S., \u0026amp; Welling, M. (2016). Group equivariant convolutional networks. In International conference on machine learning (pp. 2990-2999). PMLR. Kondor, R., \u0026amp; Trivedi, S. (2018). On the generalization of equivariance and convolution in neural networks to the action of compact groups. In International conference on machine learning (pp. 2747-2755). PMLR. Wood, T., \u0026amp; Shawe-Taylor, J. (1996). Representation theory and invariant neural networks. Neural computation, 8(5), 1003-1013. Mallat, S. (2016). Understanding deep convolutional networks. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065), 20150203. Lee, J. M. (2013). Introduction to smooth manifolds. Springer Science \u0026amp; Business Media. ","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f} \\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$: $$ \\mathcal{R} = \\mathbb{E}_{(x,y) \\sim P}[L(\\widetilde{f}(x), f(x))] $$\nSuch constructed functions can approximate almost any function (Universal Approximation Theorem), but this does not mean we do not need inductive biases to constrain the learning problem. Given a function class $\\mathcal{F}$ with universal approximation capabilities, we can define a complexity function: $$c: \\mathcal{F} \\rightarrow \\mathbb{R}$$ defining our interpolation problem as: $$\\widetilde{f} = \\arg\\min_{g \\in \\mathcal{F}} c(g) \\quad s.t. \\quad \\widetilde{f}(x_i) = f(x_i), \\quad i = 1, . . . , N$$\nTherefore, we not only hope for good function fitting but also low function complexity. Such a complexity function is what we call the norm of the function. The benefit of this definition is that $\\mathcal{F}$ becomes a Banach space (a complete vector space equipped with a norm), allowing us to use tools from functional analysis to study their properties.\nGeometric Priors To overcome the \u0026ldquo;curse of dimensionality\u0026rdquo; of high-dimensional data, we must utilize the physical characteristics of the data (symmetry and scale separation); although the geometry of the data (Domain) may be complex, the signals defined on it constitute a Hilbert space with good mathematical properties, allowing us to still use tools from linear algebra and functional analysis to handle them.\nFor example, in image processing, data is typically represented as signals (pixel values) defined on a two-dimensional Euclidean space $\\mathbb{R}^2$. This space possesses translation and rotation symmetry, meaning that if we translate or rotate the image, its content and structure remain unchanged. Convolutional Neural Networks (CNNs) exploit this translation symmetry, capturing local features through shared weights, thereby improving learning efficiency and generalization ability.\nSymmetry and Invariance In machine learning, symmetry refers to the property that data or tasks remain unchanged under certain transformations. These transformations can be translation, rotation, scaling, etc. For example, in an image classification task, if we translate or rotate an image, the category of the image usually does not change. This invariance is what we hope the model can capture and utilize.\nSymmetry Groups Symmetry can be formalized through group theory. A group is a set equipped with a binary operation that satisfies properties such as closure, associativity, identity, and inverse.\nSymmetry operations form a group, called a symmetry group, proven as follows:\nClosure: If $g_1$ and $g_2$ are symmetry operations, then their composition $g_1 \\circ g_2$ is also a symmetry operation, because applying two symmetry transformations consecutively still preserves the invariance of the data. Associativity: If $g_1$ and $g_2$ are symmetry operations, then for any $g_3$, $(g_1 \\circ g_2) \\circ g_3 = g_1 \\circ (g_2 \\circ g_3)$, because the order of transformations does not affect the final result. Identity: There exists an identity operation $e$ such that for any symmetry operation $g$, $e \\circ g = g \\circ e = g$. This identity operation corresponds to performing no transformation. Inverse: For every symmetry operation $g$, there exists an inverse operation $g^{-1}$ such that $g \\circ g^{-1} = g^{-1} \\circ g = e$. This inverse operation corresponds to undoing the transformation. Note: $g \\circ h$ denotes applying transformation $g$ first, then transformation $h$.\nLet\u0026rsquo;s take an equilateral triangle as an example to illustrate the concept of symmetry groups: In this example, the symmetry group of the equilateral triangle contains six elements: three rotation operations (0 degrees, 120 degrees, 240 degrees) and three reflection operations (perpendicular lines through each vertex). These operations satisfy the four properties of a group, thus forming a group called $D_3$, the dihedral group of the triangle. Mathematically, we can view these operations as mappings: $$g: \\mathcal{X} \\rightarrow \\mathcal{X}$$ where $\\mathcal{X}$ is the data space. For each element $g \\in G$ in a symmetry group $G$, we have a corresponding mapping $g$. For example, $D_3$ can also be represented as a matrix group: $$ D_3 = \\left\\{ \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 2 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 3 \u0026amp; 1 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 1 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 3 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 1 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 2 \u0026amp; 1 \\end{pmatrix} \\right\\} $$\nReferences Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., \u0026amp; Vandergheynst, P. (2017). Geometric deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4), 18-42. Goodfellow, I., Bengio, Y., \u0026amp; Courville, A. (2016). Deep learning. MIT press. Cohen, T. S., \u0026amp; Welling, M. (2016). Group equivariant convolutional networks. In International conference on machine learning (pp. 2990-2999). PMLR. Kondor, R., \u0026amp; Trivedi, S. (2018). On the generalization of equivariance and convolution in neural networks to the action of compact groups. In International conference on machine learning (pp. 2747-2755). PMLR. Wood, T., \u0026amp; Shawe-Taylor, J. (1996). Representation theory and invariant neural networks. Neural computation, 8(5), 1003-1013. Mallat, S. (2016). Understanding deep convolutional networks. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065), 20150203. Lee, J. M. (2013). Introduction to smooth manifolds. Springer Science \u0026amp; Business Media. ","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f} \\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$: $$ \\mathcal{R} = \\mathbb{E}_{(x,y) \\sim P}[L(\\widetilde{f}(x), f(x))] $$\nSuch constructed functions can approximate almost any function (Universal Approximation Theorem), but this does not mean we do not need inductive biases to constrain the learning problem. Given a function class $\\mathcal{F}$ with universal approximation capabilities, we can define a complexity function: $$c: \\mathcal{F} \\rightarrow \\mathbb{R}$$ defining our interpolation problem as: $$\\widetilde{f} = \\arg\\min_{g \\in \\mathcal{F}} c(g) \\quad s.t. \\quad \\widetilde{f}(x_i) = f(x_i), \\quad i = 1, . . . , N$$\nTherefore, we not only hope for good function fitting but also low function complexity. Such a complexity function is what we call the norm of the function. The benefit of this definition is that $\\mathcal{F}$ becomes a Banach space (a complete vector space equipped with a norm), allowing us to use tools from functional analysis to study their properties.\nGeometric Priors To overcome the \u0026ldquo;curse of dimensionality\u0026rdquo; of high-dimensional data, we must utilize the physical characteristics of the data (symmetry and scale separation); although the geometry of the data (Domain) may be complex, the signals defined on it constitute a Hilbert space with good mathematical properties, allowing us to still use tools from linear algebra and functional analysis to handle them.\nFor example, in image processing, data is typically represented as signals (pixel values) defined on a two-dimensional Euclidean space $\\mathbb{R}^2$. This space possesses translation and rotation symmetry, meaning that if we translate or rotate the image, its content and structure remain unchanged. Convolutional Neural Networks (CNNs) exploit this translation symmetry, capturing local features through shared weights, thereby improving learning efficiency and generalization ability.\nSymmetry and Invariance In machine learning, symmetry refers to the property that data or tasks remain unchanged under certain transformations. These transformations can be translation, rotation, scaling, etc. For example, in an image classification task, if we translate or rotate an image, the category of the image usually does not change. This invariance is what we hope the model can capture and utilize.\nSymmetry Groups Symmetry can be formalized through group theory. A group is a set equipped with a binary operation that satisfies properties such as closure, associativity, identity, and inverse.\nSymmetry operations form a group, called a symmetry group, proven as follows:\nClosure: If $g_1$ and $g_2$ are symmetry operations, then their composition $g_1 \\circ g_2$ is also a symmetry operation, because applying two symmetry transformations consecutively still preserves the invariance of the data. Associativity: If $g_1$ and $g_2$ are symmetry operations, then for any $g_3$, $(g_1 \\circ g_2) \\circ g_3 = g_1 \\circ (g_2 \\circ g_3)$, because the order of transformations does not affect the final result. Identity: There exists an identity operation $e$ such that for any symmetry operation $g$, $e \\circ g = g \\circ e = g$. This identity operation corresponds to performing no transformation. Inverse: For every symmetry operation $g$, there exists an inverse operation $g^{-1}$ such that $g \\circ g^{-1} = g^{-1} \\circ g = e$. This inverse operation corresponds to undoing the transformation. Note: $g \\circ h$ denotes applying transformation $g$ first, then transformation $h$.\nLet\u0026rsquo;s take an equilateral triangle as an example to illustrate the concept of symmetry groups: In this example, the symmetry group of the equilateral triangle contains six elements: three rotation operations (0 degrees, 120 degrees, 240 degrees) and three reflection operations (perpendicular lines through each vertex). These operations satisfy the four properties of a group, thus forming a group called $D_3$, the dihedral group of the triangle. Mathematically, we can view these operations as mappings: $$g: \\mathcal{X} \\rightarrow \\mathcal{X}$$ where $\\mathcal{X}$ is the data space. For each element $g \\in G$ in a symmetry group $G$, we have a corresponding mapping $g$. For example, $D_3$ can also be represented as a matrix group: $$ D_3 = \\left\\{ \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 2 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 3 \u0026amp; 1 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 1 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 3 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 1 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 2 \u0026amp; 1 \\end{pmatrix} \\right\\} $$\nReferences Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., \u0026amp; Vandergheynst, P. (2017). Geometric deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4), 18-42. Goodfellow, I., Bengio, Y., \u0026amp; Courville, A. (2016). Deep learning. MIT press. Cohen, T. S., \u0026amp; Welling, M. (2016). Group equivariant convolutional networks. In International conference on machine learning (pp. 2990-2999). PMLR. Kondor, R., \u0026amp; Trivedi, S. (2018). On the generalization of equivariance and convolution in neural networks to the action of compact groups. In International conference on machine learning (pp. 2747-2755). PMLR. Wood, T., \u0026amp; Shawe-Taylor, J. (1996). Representation theory and invariant neural networks. Neural computation, 8(5), 1003-1013. Mallat, S. (2016). Understanding deep convolutional networks. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065), 20150203. Lee, J. M. (2013). Introduction to smooth manifolds. Springer Science \u0026amp; Business Media. ","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"","permalink":"http://localhost:1313/posts/geo_deeplearning1/","summary":"","title":""},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f} \\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$: $$ \\mathcal{R} = \\mathbb{E}_{(x,y) \\sim P}[L(\\widetilde{f}(x), f(x))] $$\nSuch constructed functions can approximate almost any function (Universal Approximation Theorem), but this does not mean we do not need inductive biases to constrain the learning problem. Given a function class $\\mathcal{F}$ with universal approximation capabilities, we can define a complexity function: $$c: \\mathcal{F} \\rightarrow \\mathbb{R}$$ defining our interpolation problem as: $$\\widetilde{f} = \\arg\\min_{g \\in \\mathcal{F}} c(g) \\quad s.t. \\quad \\widetilde{f}(x_i) = f(x_i), \\quad i = 1, . . . , N$$\nTherefore, we not only hope for good function fitting but also low function complexity. Such a complexity function is what we call the norm of the function. The benefit of this definition is that $\\mathcal{F}$ becomes a Banach space (a complete vector space equipped with a norm), allowing us to use tools from functional analysis to study their properties.\nGeometric Priors To overcome the \u0026ldquo;curse of dimensionality\u0026rdquo; of high-dimensional data, we must utilize the physical characteristics of the data (symmetry and scale separation); although the geometry of the data (Domain) may be complex, the signals defined on it constitute a Hilbert space with good mathematical properties, allowing us to still use tools from linear algebra and functional analysis to handle them.\nFor example, in image processing, data is typically represented as signals (pixel values) defined on a two-dimensional Euclidean space $\\mathbb{R}^2$. This space possesses translation and rotation symmetry, meaning that if we translate or rotate the image, its content and structure remain unchanged. Convolutional Neural Networks (CNNs) exploit this translation symmetry, capturing local features through shared weights, thereby improving learning efficiency and generalization ability.\nSymmetry and Invariance In machine learning, symmetry refers to the property that data or tasks remain unchanged under certain transformations. These transformations can be translation, rotation, scaling, etc. For example, in an image classification task, if we translate or rotate an image, the category of the image usually does not change. This invariance is what we hope the model can capture and utilize.\nSymmetry Groups Symmetry can be formalized through group theory. A group is a set equipped with a binary operation that satisfies properties such as closure, associativity, identity, and inverse.\nSymmetry operations form a group, called a symmetry group, proven as follows:\nClosure: If $g_1$ and $g_2$ are symmetry operations, then their composition $g_1 \\circ g_2$ is also a symmetry operation, because applying two symmetry transformations consecutively still preserves the invariance of the data. Associativity: If $g_1$ and $g_2$ are symmetry operations, then for any $g_3$, $(g_1 \\circ g_2) \\circ g_3 = g_1 \\circ (g_2 \\circ g_3)$, because the order of transformations does not affect the final result. Identity: There exists an identity operation $e$ such that for any symmetry operation $g$, $e \\circ g = g \\circ e = g$. This identity operation corresponds to performing no transformation. Inverse: For every symmetry operation $g$, there exists an inverse operation $g^{-1}$ such that $g \\circ g^{-1} = g^{-1} \\circ g = e$. This inverse operation corresponds to undoing the transformation. Note: $g \\circ h$ denotes applying transformation $g$ first, then transformation $h$.\nLet\u0026rsquo;s take an equilateral triangle as an example to illustrate the concept of symmetry groups: In this example, the symmetry group of the equilateral triangle contains six elements: three rotation operations (0 degrees, 120 degrees, 240 degrees) and three reflection operations (perpendicular lines through each vertex). These operations satisfy the four properties of a group, thus forming a group called $D_3$, the dihedral group of the triangle. Mathematically, we can view these operations as mappings: $$g: \\mathcal{X} \\rightarrow \\mathcal{X}$$ where $\\mathcal{X}$ is the data space. For each element $g \\in G$ in a symmetry group $G$, we have a corresponding mapping $g$. For example, $D_3$ can also be represented as a matrix group: $$ D_3 = \\left\\{ \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 2 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 3 \u0026amp; 1 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 1 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 3 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 1 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 2 \u0026amp; 1 \\end{pmatrix} \\right\\} $$\nReferences Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., \u0026amp; Vandergheynst, P. (2017). Geometric deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4), 18-42. Goodfellow, I., Bengio, Y., \u0026amp; Courville, A. (2016). Deep learning. MIT press. Cohen, T. S., \u0026amp; Welling, M. (2016). Group equivariant convolutional networks. In International conference on machine learning (pp. 2990-2999). PMLR. Kondor, R., \u0026amp; Trivedi, S. (2018). On the generalization of equivariance and convolution in neural networks to the action of compact groups. In International conference on machine learning (pp. 2747-2755). PMLR. Wood, T., \u0026amp; Shawe-Taylor, J. (1996). Representation theory and invariant neural networks. Neural computation, 8(5), 1003-1013. Mallat, S. (2016). Understanding deep convolutional networks. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065), 20150203. Lee, J. M. (2013). Introduction to smooth manifolds. Springer Science \u0026amp; Business Media. ","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"","permalink":"http://localhost:1313/posts/geo_deeplearning1/","summary":"","title":"intro to Geo-deep-learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f} \\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$: $$ \\mathcal{R} = \\mathbb{E}_{(x,y) \\sim P}[L(\\widetilde{f}(x), f(x))] $$\nSuch constructed functions can approximate almost any function (Universal Approximation Theorem), but this does not mean we do not need inductive biases to constrain the learning problem. Given a function class $\\mathcal{F}$ with universal approximation capabilities, we can define a complexity function: $$c: \\mathcal{F} \\rightarrow \\mathbb{R}$$ defining our interpolation problem as: $$\\widetilde{f} = \\arg\\min_{g \\in \\mathcal{F}} c(g) \\quad s.t. \\quad \\widetilde{f}(x_i) = f(x_i), \\quad i = 1, . . . , N$$\nTherefore, we not only hope for good function fitting but also low function complexity. Such a complexity function is what we call the norm of the function. The benefit of this definition is that $\\mathcal{F}$ becomes a Banach space (a complete vector space equipped with a norm), allowing us to use tools from functional analysis to study their properties.\nGeometric Priors To overcome the \u0026ldquo;curse of dimensionality\u0026rdquo; of high-dimensional data, we must utilize the physical characteristics of the data (symmetry and scale separation); although the geometry of the data (Domain) may be complex, the signals defined on it constitute a Hilbert space with good mathematical properties, allowing us to still use tools from linear algebra and functional analysis to handle them.\nFor example, in image processing, data is typically represented as signals (pixel values) defined on a two-dimensional Euclidean space $\\mathbb{R}^2$. This space possesses translation and rotation symmetry, meaning that if we translate or rotate the image, its content and structure remain unchanged. Convolutional Neural Networks (CNNs) exploit this translation symmetry, capturing local features through shared weights, thereby improving learning efficiency and generalization ability.\nSymmetry and Invariance In machine learning, symmetry refers to the property that data or tasks remain unchanged under certain transformations. These transformations can be translation, rotation, scaling, etc. For example, in an image classification task, if we translate or rotate an image, the category of the image usually does not change. This invariance is what we hope the model can capture and utilize.\nSymmetry Groups Symmetry can be formalized through group theory. A group is a set equipped with a binary operation that satisfies properties such as closure, associativity, identity, and inverse.\nSymmetry operations form a group, called a symmetry group, proven as follows:\nClosure: If $g_1$ and $g_2$ are symmetry operations, then their composition $g_1 \\circ g_2$ is also a symmetry operation, because applying two symmetry transformations consecutively still preserves the invariance of the data. Associativity: If $g_1$ and $g_2$ are symmetry operations, then for any $g_3$, $(g_1 \\circ g_2) \\circ g_3 = g_1 \\circ (g_2 \\circ g_3)$, because the order of transformations does not affect the final result. Identity: There exists an identity operation $e$ such that for any symmetry operation $g$, $e \\circ g = g \\circ e = g$. This identity operation corresponds to performing no transformation. Inverse: For every symmetry operation $g$, there exists an inverse operation $g^{-1}$ such that $g \\circ g^{-1} = g^{-1} \\circ g = e$. This inverse operation corresponds to undoing the transformation. Note: $g \\circ h$ denotes applying transformation $g$ first, then transformation $h$.\nLet\u0026rsquo;s take an equilateral triangle as an example to illustrate the concept of symmetry groups: In this example, the symmetry group of the equilateral triangle contains six elements: three rotation operations (0 degrees, 120 degrees, 240 degrees) and three reflection operations (perpendicular lines through each vertex). These operations satisfy the four properties of a group, thus forming a group called $D_3$, the dihedral group of the triangle. Mathematically, we can view these operations as mappings: $$g: \\mathcal{X} \\rightarrow \\mathcal{X}$$ where $\\mathcal{X}$ is the data space. For each element $g \\in G$ in a symmetry group $G$, we have a corresponding mapping $g$. For example, $D_3$ can also be represented as a matrix group: $$ D_3 = \\left\\{ \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 2 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 3 \u0026amp; 1 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 1 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 3 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 1 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 2 \u0026amp; 1 \\end{pmatrix} \\right\\} $$\nReferences Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., \u0026amp; Vandergheynst, P. (2017). Geometric deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4), 18-42. Goodfellow, I., Bengio, Y., \u0026amp; Courville, A. (2016). Deep learning. MIT press. Cohen, T. S., \u0026amp; Welling, M. (2016). Group equivariant convolutional networks. In International conference on machine learning (pp. 2990-2999). PMLR. Kondor, R., \u0026amp; Trivedi, S. (2018). On the generalization of equivariance and convolution in neural networks to the action of compact groups. In International conference on machine learning (pp. 2747-2755). PMLR. Wood, T., \u0026amp; Shawe-Taylor, J. (1996). Representation theory and invariant neural networks. Neural computation, 8(5), 1003-1013. Mallat, S. (2016). Understanding deep convolutional networks. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065), 20150203. Lee, J. M. (2013). Introduction to smooth manifolds. Springer Science \u0026amp; Business Media. ","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"","permalink":"http://localhost:1313/posts/geo_deeplearning1/","summary":"","title":"intro to Geo-deep-learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f} \\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$: $$ \\mathcal{R} = \\mathbb{E}_{(x,y) \\sim P}[L(\\widetilde{f}(x), f(x))] $$\nSuch constructed functions can approximate almost any function (Universal Approximation Theorem), but this does not mean we do not need inductive biases to constrain the learning problem. Given a function class $\\mathcal{F}$ with universal approximation capabilities, we can define a complexity function: $$c: \\mathcal{F} \\rightarrow \\mathbb{R}$$ defining our interpolation problem as: $$\\widetilde{f} = \\arg\\min_{g \\in \\mathcal{F}} c(g) \\quad s.t. \\quad \\widetilde{f}(x_i) = f(x_i), \\quad i = 1, . . . , N$$\nTherefore, we not only hope for good function fitting but also low function complexity. Such a complexity function is what we call the norm of the function. The benefit of this definition is that $\\mathcal{F}$ becomes a Banach space (a complete vector space equipped with a norm), allowing us to use tools from functional analysis to study their properties.\nGeometric Priors To overcome the \u0026ldquo;curse of dimensionality\u0026rdquo; of high-dimensional data, we must utilize the physical characteristics of the data (symmetry and scale separation); although the geometry of the data (Domain) may be complex, the signals defined on it constitute a Hilbert space with good mathematical properties, allowing us to still use tools from linear algebra and functional analysis to handle them.\nFor example, in image processing, data is typically represented as signals (pixel values) defined on a two-dimensional Euclidean space $\\mathbb{R}^2$. This space possesses translation and rotation symmetry, meaning that if we translate or rotate the image, its content and structure remain unchanged. Convolutional Neural Networks (CNNs) exploit this translation symmetry, capturing local features through shared weights, thereby improving learning efficiency and generalization ability.\nSymmetry and Invariance In machine learning, symmetry refers to the property that data or tasks remain unchanged under certain transformations. These transformations can be translation, rotation, scaling, etc. For example, in an image classification task, if we translate or rotate an image, the category of the image usually does not change. This invariance is what we hope the model can capture and utilize.\nSymmetry Groups Symmetry can be formalized through group theory. A group is a set equipped with a binary operation that satisfies properties such as closure, associativity, identity, and inverse.\nSymmetry operations form a group, called a symmetry group, proven as follows:\nClosure: If $g_1$ and $g_2$ are symmetry operations, then their composition $g_1 \\circ g_2$ is also a symmetry operation, because applying two symmetry transformations consecutively still preserves the invariance of the data. Associativity: If $g_1$ and $g_2$ are symmetry operations, then for any $g_3$, $(g_1 \\circ g_2) \\circ g_3 = g_1 \\circ (g_2 \\circ g_3)$, because the order of transformations does not affect the final result. Identity: There exists an identity operation $e$ such that for any symmetry operation $g$, $e \\circ g = g \\circ e = g$. This identity operation corresponds to performing no transformation. Inverse: For every symmetry operation $g$, there exists an inverse operation $g^{-1}$ such that $g \\circ g^{-1} = g^{-1} \\circ g = e$. This inverse operation corresponds to undoing the transformation. Note: $g \\circ h$ denotes applying transformation $g$ first, then transformation $h$.\nLet\u0026rsquo;s take an equilateral triangle as an example to illustrate the concept of symmetry groups: In this example, the symmetry group of the equilateral triangle contains six elements: three rotation operations (0 degrees, 120 degrees, 240 degrees) and three reflection operations (perpendicular lines through each vertex). These operations satisfy the four properties of a group, thus forming a group called $D_3$, the dihedral group of the triangle. Mathematically, we can view these operations as mappings: $$g: \\mathcal{X} \\rightarrow \\mathcal{X}$$ where $\\mathcal{X}$ is the data space. For each element $g \\in G$ in a symmetry group $G$, we have a corresponding mapping $g$. For example, $D_3$ can also be represented as a matrix group: $$ D_3 = \\left\\{ \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 2 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 3 \u0026amp; 1 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 1 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 3 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 1 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 2 \u0026amp; 1 \\end{pmatrix} \\right\\} $$\nReferences Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., \u0026amp; Vandergheynst, P. (2017). Geometric deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4), 18-42. Goodfellow, I., Bengio, Y., \u0026amp; Courville, A. (2016). Deep learning. MIT press. Cohen, T. S., \u0026amp; Welling, M. (2016). Group equivariant convolutional networks. In International conference on machine learning (pp. 2990-2999). PMLR. Kondor, R., \u0026amp; Trivedi, S. (2018). On the generalization of equivariance and convolution in neural networks to the action of compact groups. In International conference on machine learning (pp. 2747-2755). PMLR. Wood, T., \u0026amp; Shawe-Taylor, J. (1996). Representation theory and invariant neural networks. Neural computation, 8(5), 1003-1013. Mallat, S. (2016). Understanding deep convolutional networks. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065), 20150203. Lee, J. M. (2013). Introduction to smooth manifolds. Springer Science \u0026amp; Business Media. ","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"","permalink":"http://localhost:1313/posts/geo_deeplearning1/","summary":"","title":"intro to Geo-deep-learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f} \\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$: $$ \\mathcal{R} = \\mathbb{E}_{(x,y) \\sim P}[L(\\widetilde{f}(x), f(x))] $$\nSuch constructed functions can approximate almost any function (Universal Approximation Theorem), but this does not mean we do not need inductive biases to constrain the learning problem. Given a function class $\\mathcal{F}$ with universal approximation capabilities, we can define a complexity function: $$c: \\mathcal{F} \\rightarrow \\mathbb{R}$$ defining our interpolation problem as: $$\\widetilde{f} = \\arg\\min_{g \\in \\mathcal{F}} c(g) \\quad s.t. \\quad \\widetilde{f}(x_i) = f(x_i), \\quad i = 1, . . . , N$$\nTherefore, we not only hope for good function fitting but also low function complexity. Such a complexity function is what we call the norm of the function. The benefit of this definition is that $\\mathcal{F}$ becomes a Banach space (a complete vector space equipped with a norm), allowing us to use tools from functional analysis to study their properties.\nGeometric Priors To overcome the \u0026ldquo;curse of dimensionality\u0026rdquo; of high-dimensional data, we must utilize the physical characteristics of the data (symmetry and scale separation); although the geometry of the data (Domain) may be complex, the signals defined on it constitute a Hilbert space with good mathematical properties, allowing us to still use tools from linear algebra and functional analysis to handle them.\nFor example, in image processing, data is typically represented as signals (pixel values) defined on a two-dimensional Euclidean space $\\mathbb{R}^2$. This space possesses translation and rotation symmetry, meaning that if we translate or rotate the image, its content and structure remain unchanged. Convolutional Neural Networks (CNNs) exploit this translation symmetry, capturing local features through shared weights, thereby improving learning efficiency and generalization ability.\nSymmetry and Invariance In machine learning, symmetry refers to the property that data or tasks remain unchanged under certain transformations. These transformations can be translation, rotation, scaling, etc. For example, in an image classification task, if we translate or rotate an image, the category of the image usually does not change. This invariance is what we hope the model can capture and utilize.\nSymmetry Groups Symmetry can be formalized through group theory. A group is a set equipped with a binary operation that satisfies properties such as closure, associativity, identity, and inverse.\nSymmetry operations form a group, called a symmetry group, proven as follows:\nClosure: If $g_1$ and $g_2$ are symmetry operations, then their composition $g_1 \\circ g_2$ is also a symmetry operation, because applying two symmetry transformations consecutively still preserves the invariance of the data. Associativity: If $g_1$ and $g_2$ are symmetry operations, then for any $g_3$, $(g_1 \\circ g_2) \\circ g_3 = g_1 \\circ (g_2 \\circ g_3)$, because the order of transformations does not affect the final result. Identity: There exists an identity operation $e$ such that for any symmetry operation $g$, $e \\circ g = g \\circ e = g$. This identity operation corresponds to performing no transformation. Inverse: For every symmetry operation $g$, there exists an inverse operation $g^{-1}$ such that $g \\circ g^{-1} = g^{-1} \\circ g = e$. This inverse operation corresponds to undoing the transformation. Note: $g \\circ h$ denotes applying transformation $g$ first, then transformation $h$.\nLet\u0026rsquo;s take an equilateral triangle as an example to illustrate the concept of symmetry groups: In this example, the symmetry group of the equilateral triangle contains six elements: three rotation operations (0 degrees, 120 degrees, 240 degrees) and three reflection operations (perpendicular lines through each vertex). These operations satisfy the four properties of a group, thus forming a group called $D_3$, the dihedral group of the triangle. Mathematically, we can view these operations as mappings: $$g: \\mathcal{X} \\rightarrow \\mathcal{X}$$ where $\\mathcal{X}$ is the data space. For each element $g \\in G$ in a symmetry group $G$, we have a corresponding mapping $g$. For example, $D_3$ can also be represented as a matrix group: $$ D_3 = \\left\\{ \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 2 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 3 \u0026amp; 1 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 1 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 3 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 1 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 2 \u0026amp; 1 \\end{pmatrix} \\right\\} $$\nReferences Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., \u0026amp; Vandergheynst, P. (2017). Geometric deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4), 18-42. Goodfellow, I., Bengio, Y., \u0026amp; Courville, A. (2016). Deep learning. MIT press. Cohen, T. S., \u0026amp; Welling, M. (2016). Group equivariant convolutional networks. In International conference on machine learning (pp. 2990-2999). PMLR. Kondor, R., \u0026amp; Trivedi, S. (2018). On the generalization of equivariance and convolution in neural networks to the action of compact groups. In International conference on machine learning (pp. 2747-2755). PMLR. Wood, T., \u0026amp; Shawe-Taylor, J. (1996). Representation theory and invariant neural networks. Neural computation, 8(5), 1003-1013. Mallat, S. (2016). Understanding deep convolutional networks. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065), 20150203. Lee, J. M. (2013). Introduction to smooth manifolds. Springer Science \u0026amp; Business Media. ","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"","permalink":"http://localhost:1313/posts/geo_deeplearning1/","summary":"","title":"intro to Geo-deep-learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f} \\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$: $$ \\mathcal{R} = \\mathbb{E}_{(x,y) \\sim P}[L(\\widetilde{f}(x), f(x))] $$\nSuch constructed functions can approximate almost any function (Universal Approximation Theorem), but this does not mean we do not need inductive biases to constrain the learning problem. Given a function class $\\mathcal{F}$ with universal approximation capabilities, we can define a complexity function: $$c: \\mathcal{F} \\rightarrow \\mathbb{R}$$ defining our interpolation problem as: $$\\widetilde{f} = \\arg\\min_{g \\in \\mathcal{F}} c(g) \\quad s.t. \\quad \\widetilde{f}(x_i) = f(x_i), \\quad i = 1, . . . , N$$\nTherefore, we not only hope for good function fitting but also low function complexity. Such a complexity function is what we call the norm of the function. The benefit of this definition is that $\\mathcal{F}$ becomes a Banach space (a complete vector space equipped with a norm), allowing us to use tools from functional analysis to study their properties.\nGeometric Priors To overcome the \u0026ldquo;curse of dimensionality\u0026rdquo; of high-dimensional data, we must utilize the physical characteristics of the data (symmetry and scale separation); although the geometry of the data (Domain) may be complex, the signals defined on it constitute a Hilbert space with good mathematical properties, allowing us to still use tools from linear algebra and functional analysis to handle them.\nFor example, in image processing, data is typically represented as signals (pixel values) defined on a two-dimensional Euclidean space $\\mathbb{R}^2$. This space possesses translation and rotation symmetry, meaning that if we translate or rotate the image, its content and structure remain unchanged. Convolutional Neural Networks (CNNs) exploit this translation symmetry, capturing local features through shared weights, thereby improving learning efficiency and generalization ability.\nSymmetry and Invariance In machine learning, symmetry refers to the property that data or tasks remain unchanged under certain transformations. These transformations can be translation, rotation, scaling, etc. For example, in an image classification task, if we translate or rotate an image, the category of the image usually does not change. This invariance is what we hope the model can capture and utilize.\nSymmetry Groups Symmetry can be formalized through group theory. A group is a set equipped with a binary operation that satisfies properties such as closure, associativity, identity, and inverse.\nSymmetry operations form a group, called a symmetry group, proven as follows:\nClosure: If $g_1$ and $g_2$ are symmetry operations, then their composition $g_1 \\circ g_2$ is also a symmetry operation, because applying two symmetry transformations consecutively still preserves the invariance of the data. Associativity: If $g_1$ and $g_2$ are symmetry operations, then for any $g_3$, $(g_1 \\circ g_2) \\circ g_3 = g_1 \\circ (g_2 \\circ g_3)$, because the order of transformations does not affect the final result. Identity: There exists an identity operation $e$ such that for any symmetry operation $g$, $e \\circ g = g \\circ e = g$. This identity operation corresponds to performing no transformation. Inverse: For every symmetry operation $g$, there exists an inverse operation $g^{-1}$ such that $g \\circ g^{-1} = g^{-1} \\circ g = e$. This inverse operation corresponds to undoing the transformation. Note: $g \\circ h$ denotes applying transformation $g$ first, then transformation $h$.\nLet\u0026rsquo;s take an equilateral triangle as an example to illustrate the concept of symmetry groups: In this example, the symmetry group of the equilateral triangle contains six elements: three rotation operations (0 degrees, 120 degrees, 240 degrees) and three reflection operations (perpendicular lines through each vertex). These operations satisfy the four properties of a group, thus forming a group called $D_3$, the dihedral group of the triangle. Mathematically, we can view these operations as mappings: $$g: \\mathcal{X} \\rightarrow \\mathcal{X}$$ where $\\mathcal{X}$ is the data space. For each element $g \\in G$ in a symmetry group $G$, we have a corresponding mapping $g$. For example, $D_3$ can also be represented as a matrix group: $$ D_3 = \\left\\{ \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 2 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 3 \u0026amp; 1 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 1 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 3 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 1 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 2 \u0026amp; 1 \\end{pmatrix} \\right\\} $$\nReferences Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., \u0026amp; Vandergheynst, P. (2017). Geometric deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4), 18-42. Goodfellow, I., Bengio, Y., \u0026amp; Courville, A. (2016). Deep learning. MIT press. Cohen, T. S., \u0026amp; Welling, M. (2016). Group equivariant convolutional networks. In International conference on machine learning (pp. 2990-2999). PMLR. Kondor, R., \u0026amp; Trivedi, S. (2018). On the generalization of equivariance and convolution in neural networks to the action of compact groups. In International conference on machine learning (pp. 2747-2755). PMLR. Wood, T., \u0026amp; Shawe-Taylor, J. (1996). Representation theory and invariant neural networks. Neural computation, 8(5), 1003-1013. Mallat, S. (2016). Understanding deep convolutional networks. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065), 20150203. Lee, J. M. (2013). Introduction to smooth manifolds. Springer Science \u0026amp; Business Media. ","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"","permalink":"http://localhost:1313/posts/geo_deeplearning1/","summary":"","title":"intro to Geo-deep-learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f} \\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$: $$ \\mathcal{R} = \\mathbb{E}_{(x,y) \\sim P}[L(\\widetilde{f}(x), f(x))] $$\nSuch constructed functions can approximate almost any function (Universal Approximation Theorem), but this does not mean we do not need inductive biases to constrain the learning problem. Given a function class $\\mathcal{F}$ with universal approximation capabilities, we can define a complexity function: $$c: \\mathcal{F} \\rightarrow \\mathbb{R}$$ defining our interpolation problem as: $$\\widetilde{f} = \\arg\\min_{g \\in \\mathcal{F}} c(g) \\quad s.t. \\quad \\widetilde{f}(x_i) = f(x_i), \\quad i = 1, . . . , N$$\nTherefore, we not only hope for good function fitting but also low function complexity. Such a complexity function is what we call the norm of the function. The benefit of this definition is that $\\mathcal{F}$ becomes a Banach space (a complete vector space equipped with a norm), allowing us to use tools from functional analysis to study their properties.\nGeometric Priors To overcome the \u0026ldquo;curse of dimensionality\u0026rdquo; of high-dimensional data, we must utilize the physical characteristics of the data (symmetry and scale separation); although the geometry of the data (Domain) may be complex, the signals defined on it constitute a Hilbert space with good mathematical properties, allowing us to still use tools from linear algebra and functional analysis to handle them.\nFor example, in image processing, data is typically represented as signals (pixel values) defined on a two-dimensional Euclidean space $\\mathbb{R}^2$. This space possesses translation and rotation symmetry, meaning that if we translate or rotate the image, its content and structure remain unchanged. Convolutional Neural Networks (CNNs) exploit this translation symmetry, capturing local features through shared weights, thereby improving learning efficiency and generalization ability.\nSymmetry and Invariance In machine learning, symmetry refers to the property that data or tasks remain unchanged under certain transformations. These transformations can be translation, rotation, scaling, etc. For example, in an image classification task, if we translate or rotate an image, the category of the image usually does not change. This invariance is what we hope the model can capture and utilize.\nSymmetry Groups Symmetry can be formalized through group theory. A group is a set equipped with a binary operation that satisfies properties such as closure, associativity, identity, and inverse.\nSymmetry operations form a group, called a symmetry group, proven as follows:\nClosure: If $g_1$ and $g_2$ are symmetry operations, then their composition $g_1 \\circ g_2$ is also a symmetry operation, because applying two symmetry transformations consecutively still preserves the invariance of the data. Associativity: If $g_1$ and $g_2$ are symmetry operations, then for any $g_3$, $(g_1 \\circ g_2) \\circ g_3 = g_1 \\circ (g_2 \\circ g_3)$, because the order of transformations does not affect the final result. Identity: There exists an identity operation $e$ such that for any symmetry operation $g$, $e \\circ g = g \\circ e = g$. This identity operation corresponds to performing no transformation. Inverse: For every symmetry operation $g$, there exists an inverse operation $g^{-1}$ such that $g \\circ g^{-1} = g^{-1} \\circ g = e$. This inverse operation corresponds to undoing the transformation. Note: $g \\circ h$ denotes applying transformation $g$ first, then transformation $h$.\nLet\u0026rsquo;s take an equilateral triangle as an example to illustrate the concept of symmetry groups: In this example, the symmetry group of the equilateral triangle contains six elements: three rotation operations (0 degrees, 120 degrees, 240 degrees) and three reflection operations (perpendicular lines through each vertex). These operations satisfy the four properties of a group, thus forming a group called $D_3$, the dihedral group of the triangle. Mathematically, we can view these operations as mappings: $$g: \\mathcal{X} \\rightarrow \\mathcal{X}$$ where $\\mathcal{X}$ is the data space. For each element $g \\in G$ in a symmetry group $G$, we have a corresponding mapping $g$. For example, $D_3$ can also be represented as a matrix group: $$ D_3 = \\left\\{ \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 2 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 3 \u0026amp; 1 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 1 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 3 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 1 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 2 \u0026amp; 1 \\end{pmatrix} \\right\\} $$\nReferences Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., \u0026amp; Vandergheynst, P. (2017). Geometric deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4), 18-42. Goodfellow, I., Bengio, Y., \u0026amp; Courville, A. (2016). Deep learning. MIT press. Cohen, T. S., \u0026amp; Welling, M. (2016). Group equivariant convolutional networks. In International conference on machine learning (pp. 2990-2999). PMLR. Kondor, R., \u0026amp; Trivedi, S. (2018). On the generalization of equivariance and convolution in neural networks to the action of compact groups. In International conference on machine learning (pp. 2747-2755). PMLR. Wood, T., \u0026amp; Shawe-Taylor, J. (1996). Representation theory and invariant neural networks. Neural computation, 8(5), 1003-1013. Mallat, S. (2016). Understanding deep convolutional networks. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065), 20150203. Lee, J. M. (2013). Introduction to smooth manifolds. Springer Science \u0026amp; Business Media. ","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"","permalink":"http://localhost:1313/posts/geo_deeplearning1/","summary":"","title":"intro to Geo-deep-learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f} \\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$: $$ \\mathcal{R} = \\mathbb{E}_{(x,y) \\sim P}[L(\\widetilde{f}(x), f(x))] $$\nSuch constructed functions can approximate almost any function (Universal Approximation Theorem), but this does not mean we do not need inductive biases to constrain the learning problem. Given a function class $\\mathcal{F}$ with universal approximation capabilities, we can define a complexity function: $$c: \\mathcal{F} \\rightarrow \\mathbb{R}$$ defining our interpolation problem as: $$\\widetilde{f} = \\arg\\min_{g \\in \\mathcal{F}} c(g) \\quad s.t. \\quad \\widetilde{f}(x_i) = f(x_i), \\quad i = 1, . . . , N$$\nTherefore, we not only hope for good function fitting but also low function complexity. Such a complexity function is what we call the norm of the function. The benefit of this definition is that $\\mathcal{F}$ becomes a Banach space (a complete vector space equipped with a norm), allowing us to use tools from functional analysis to study their properties.\nGeometric Priors To overcome the \u0026ldquo;curse of dimensionality\u0026rdquo; of high-dimensional data, we must utilize the physical characteristics of the data (symmetry and scale separation); although the geometry of the data (Domain) may be complex, the signals defined on it constitute a Hilbert space with good mathematical properties, allowing us to still use tools from linear algebra and functional analysis to handle them.\nFor example, in image processing, data is typically represented as signals (pixel values) defined on a two-dimensional Euclidean space $\\mathbb{R}^2$. This space possesses translation and rotation symmetry, meaning that if we translate or rotate the image, its content and structure remain unchanged. Convolutional Neural Networks (CNNs) exploit this translation symmetry, capturing local features through shared weights, thereby improving learning efficiency and generalization ability.\nSymmetry and Invariance In machine learning, symmetry refers to the property that data or tasks remain unchanged under certain transformations. These transformations can be translation, rotation, scaling, etc. For example, in an image classification task, if we translate or rotate an image, the category of the image usually does not change. This invariance is what we hope the model can capture and utilize.\nSymmetry Groups Symmetry can be formalized through group theory. A group is a set equipped with a binary operation that satisfies properties such as closure, associativity, identity, and inverse.\nSymmetry operations form a group, called a symmetry group, proven as follows:\nClosure: If $g_1$ and $g_2$ are symmetry operations, then their composition $g_1 \\circ g_2$ is also a symmetry operation, because applying two symmetry transformations consecutively still preserves the invariance of the data. Associativity: If $g_1$ and $g_2$ are symmetry operations, then for any $g_3$, $(g_1 \\circ g_2) \\circ g_3 = g_1 \\circ (g_2 \\circ g_3)$, because the order of transformations does not affect the final result. Identity: There exists an identity operation $e$ such that for any symmetry operation $g$, $e \\circ g = g \\circ e = g$. This identity operation corresponds to performing no transformation. Inverse: For every symmetry operation $g$, there exists an inverse operation $g^{-1}$ such that $g \\circ g^{-1} = g^{-1} \\circ g = e$. This inverse operation corresponds to undoing the transformation. Note: $g \\circ h$ denotes applying transformation $g$ first, then transformation $h$.\nLet\u0026rsquo;s take an equilateral triangle as an example to illustrate the concept of symmetry groups: In this example, the symmetry group of the equilateral triangle contains six elements: three rotation operations (0 degrees, 120 degrees, 240 degrees) and three reflection operations (perpendicular lines through each vertex). These operations satisfy the four properties of a group, thus forming a group called $D_3$, the dihedral group of the triangle. Mathematically, we can view these operations as mappings: $$g: \\mathcal{X} \\rightarrow \\mathcal{X}$$ where $\\mathcal{X}$ is the data space. For each element $g \\in G$ in a symmetry group $G$, we have a corresponding mapping $g$. For example, $D_3$ can also be represented as a matrix group: $$ D_3 = \\left\\{ \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 2 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 3 \u0026amp; 1 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 1 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 3 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 1 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 2 \u0026amp; 1 \\end{pmatrix} \\right\\} $$\nReferences Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., \u0026amp; Vandergheynst, P. (2017). Geometric deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4), 18-42. Goodfellow, I., Bengio, Y., \u0026amp; Courville, A. (2016). Deep learning. MIT press. Cohen, T. S., \u0026amp; Welling, M. (2016). Group equivariant convolutional networks. In International conference on machine learning (pp. 2990-2999). PMLR. Kondor, R., \u0026amp; Trivedi, S. (2018). On the generalization of equivariance and convolution in neural networks to the action of compact groups. In International conference on machine learning (pp. 2747-2755). PMLR. Wood, T., \u0026amp; Shawe-Taylor, J. (1996). Representation theory and invariant neural networks. Neural computation, 8(5), 1003-1013. Mallat, S. (2016). Understanding deep convolutional networks. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065), 20150203. Lee, J. M. (2013). Introduction to smooth manifolds. Springer Science \u0026amp; Business Media. ","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"","permalink":"http://localhost:1313/posts/geo_deeplearning1/","summary":"","title":"intro to Geo-deep-learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f} \\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$: $$ \\mathcal{R} = \\mathbb{E}_{(x,y) \\sim P}[L(\\widetilde{f}(x), f(x))] $$\nSuch constructed functions can approximate almost any function (Universal Approximation Theorem), but this does not mean we do not need inductive biases to constrain the learning problem. Given a function class $\\mathcal{F}$ with universal approximation capabilities, we can define a complexity function: $$c: \\mathcal{F} \\rightarrow \\mathbb{R}$$ defining our interpolation problem as: $$\\widetilde{f} = \\arg\\min_{g \\in \\mathcal{F}} c(g) \\quad s.t. \\quad \\widetilde{f}(x_i) = f(x_i), \\quad i = 1, . . . , N$$\nTherefore, we not only hope for good function fitting but also low function complexity. Such a complexity function is what we call the norm of the function. The benefit of this definition is that $\\mathcal{F}$ becomes a Banach space (a complete vector space equipped with a norm), allowing us to use tools from functional analysis to study their properties.\nGeometric Priors To overcome the \u0026ldquo;curse of dimensionality\u0026rdquo; of high-dimensional data, we must utilize the physical characteristics of the data (symmetry and scale separation); although the geometry of the data (Domain) may be complex, the signals defined on it constitute a Hilbert space with good mathematical properties, allowing us to still use tools from linear algebra and functional analysis to handle them.\nFor example, in image processing, data is typically represented as signals (pixel values) defined on a two-dimensional Euclidean space $\\mathbb{R}^2$. This space possesses translation and rotation symmetry, meaning that if we translate or rotate the image, its content and structure remain unchanged. Convolutional Neural Networks (CNNs) exploit this translation symmetry, capturing local features through shared weights, thereby improving learning efficiency and generalization ability.\nSymmetry and Invariance In machine learning, symmetry refers to the property that data or tasks remain unchanged under certain transformations. These transformations can be translation, rotation, scaling, etc. For example, in an image classification task, if we translate or rotate an image, the category of the image usually does not change. This invariance is what we hope the model can capture and utilize.\nSymmetry Groups Symmetry can be formalized through group theory. A group is a set equipped with a binary operation that satisfies properties such as closure, associativity, identity, and inverse.\nSymmetry operations form a group, called a symmetry group, proven as follows:\nClosure: If $g_1$ and $g_2$ are symmetry operations, then their composition $g_1 \\circ g_2$ is also a symmetry operation, because applying two symmetry transformations consecutively still preserves the invariance of the data. Associativity: If $g_1$ and $g_2$ are symmetry operations, then for any $g_3$, $(g_1 \\circ g_2) \\circ g_3 = g_1 \\circ (g_2 \\circ g_3)$, because the order of transformations does not affect the final result. Identity: There exists an identity operation $e$ such that for any symmetry operation $g$, $e \\circ g = g \\circ e = g$. This identity operation corresponds to performing no transformation. Inverse: For every symmetry operation $g$, there exists an inverse operation $g^{-1}$ such that $g \\circ g^{-1} = g^{-1} \\circ g = e$. This inverse operation corresponds to undoing the transformation. Note: $g \\circ h$ denotes applying transformation $g$ first, then transformation $h$.\nLet\u0026rsquo;s take an equilateral triangle as an example to illustrate the concept of symmetry groups: In this example, the symmetry group of the equilateral triangle contains six elements: three rotation operations (0 degrees, 120 degrees, 240 degrees) and three reflection operations (perpendicular lines through each vertex). These operations satisfy the four properties of a group, thus forming a group called $D_3$, the dihedral group of the triangle. Mathematically, we can view these operations as mappings: $$g: \\mathcal{X} \\rightarrow \\mathcal{X}$$ where $\\mathcal{X}$ is the data space. For each element $g \\in G$ in a symmetry group $G$, we have a corresponding mapping $g$. For example, $D_3$ can also be represented as a matrix group: $$ D_3 = \\left\\{ \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 2 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 3 \u0026amp; 1 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 1 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 3 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 1 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 2 \u0026amp; 1 \\end{pmatrix} \\right\\} $$\nReferences Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., \u0026amp; Vandergheynst, P. (2017). Geometric deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4), 18-42. Goodfellow, I., Bengio, Y., \u0026amp; Courville, A. (2016). Deep learning. MIT press. Cohen, T. S., \u0026amp; Welling, M. (2016). Group equivariant convolutional networks. In International conference on machine learning (pp. 2990-2999). PMLR. Kondor, R., \u0026amp; Trivedi, S. (2018). On the generalization of equivariance and convolution in neural networks to the action of compact groups. In International conference on machine learning (pp. 2747-2755). PMLR. Wood, T., \u0026amp; Shawe-Taylor, J. (1996). Representation theory and invariant neural networks. Neural computation, 8(5), 1003-1013. Mallat, S. (2016). Understanding deep convolutional networks. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065), 20150203. Lee, J. M. (2013). Introduction to smooth manifolds. Springer Science \u0026amp; Business Media. ","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"","permalink":"http://localhost:1313/posts/geo_deeplearning1/","summary":"","title":"intro to Geo-deep-learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f} \\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$: $$ \\mathcal{R} = \\mathbb{E}_{(x,y) \\sim P}[L(\\widetilde{f}(x), f(x))] $$\nSuch constructed functions can approximate almost any function (Universal Approximation Theorem), but this does not mean we do not need inductive biases to constrain the learning problem. Given a function class $\\mathcal{F}$ with universal approximation capabilities, we can define a complexity function: $$c: \\mathcal{F} \\rightarrow \\mathbb{R}$$ defining our interpolation problem as: $$\\widetilde{f} = \\arg\\min_{g \\in \\mathcal{F}} c(g) \\quad s.t. \\quad \\widetilde{f}(x_i) = f(x_i), \\quad i = 1, . . . , N$$\nTherefore, we not only hope for good function fitting but also low function complexity. Such a complexity function is what we call the norm of the function. The benefit of this definition is that $\\mathcal{F}$ becomes a Banach space (a complete vector space equipped with a norm), allowing us to use tools from functional analysis to study their properties.\nGeometric Priors To overcome the \u0026ldquo;curse of dimensionality\u0026rdquo; of high-dimensional data, we must utilize the physical characteristics of the data (symmetry and scale separation); although the geometry of the data (Domain) may be complex, the signals defined on it constitute a Hilbert space with good mathematical properties, allowing us to still use tools from linear algebra and functional analysis to handle them.\nFor example, in image processing, data is typically represented as signals (pixel values) defined on a two-dimensional Euclidean space $\\mathbb{R}^2$. This space possesses translation and rotation symmetry, meaning that if we translate or rotate the image, its content and structure remain unchanged. Convolutional Neural Networks (CNNs) exploit this translation symmetry, capturing local features through shared weights, thereby improving learning efficiency and generalization ability.\nSymmetry and Invariance In machine learning, symmetry refers to the property that data or tasks remain unchanged under certain transformations. These transformations can be translation, rotation, scaling, etc. For example, in an image classification task, if we translate or rotate an image, the category of the image usually does not change. This invariance is what we hope the model can capture and utilize.\nSymmetry Groups Symmetry can be formalized through group theory. A group is a set equipped with a binary operation that satisfies properties such as closure, associativity, identity, and inverse.\nSymmetry operations form a group, called a symmetry group, proven as follows:\nClosure: If $g_1$ and $g_2$ are symmetry operations, then their composition $g_1 \\circ g_2$ is also a symmetry operation, because applying two symmetry transformations consecutively still preserves the invariance of the data. Associativity: If $g_1$ and $g_2$ are symmetry operations, then for any $g_3$, $(g_1 \\circ g_2) \\circ g_3 = g_1 \\circ (g_2 \\circ g_3)$, because the order of transformations does not affect the final result. Identity: There exists an identity operation $e$ such that for any symmetry operation $g$, $e \\circ g = g \\circ e = g$. This identity operation corresponds to performing no transformation. Inverse: For every symmetry operation $g$, there exists an inverse operation $g^{-1}$ such that $g \\circ g^{-1} = g^{-1} \\circ g = e$. This inverse operation corresponds to undoing the transformation. Note: $g \\circ h$ denotes applying transformation $g$ first, then transformation $h$.\nLet\u0026rsquo;s take an equilateral triangle as an example to illustrate the concept of symmetry groups: In this example, the symmetry group of the equilateral triangle contains six elements: three rotation operations (0 degrees, 120 degrees, 240 degrees) and three reflection operations (perpendicular lines through each vertex). These operations satisfy the four properties of a group, thus forming a group called $D_3$, the dihedral group of the triangle. Mathematically, we can view these operations as mappings: $$g: \\mathcal{X} \\rightarrow \\mathcal{X}$$ where $\\mathcal{X}$ is the data space. For each element $g \\in G$ in a symmetry group $G$, we have a corresponding mapping $g$. For example, $D_3$ can also be represented as a matrix group: $$ D_3 = \\left\\{ \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 2 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 3 \u0026amp; 1 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 1 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 3 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 1 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 2 \u0026amp; 1 \\end{pmatrix} \\right\\} $$\nReferences Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., \u0026amp; Vandergheynst, P. (2017). Geometric deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4), 18-42. Goodfellow, I., Bengio, Y., \u0026amp; Courville, A. (2016). Deep learning. MIT press. Cohen, T. S., \u0026amp; Welling, M. (2016). Group equivariant convolutional networks. In International conference on machine learning (pp. 2990-2999). PMLR. Kondor, R., \u0026amp; Trivedi, S. (2018). On the generalization of equivariance and convolution in neural networks to the action of compact groups. In International conference on machine learning (pp. 2747-2755). PMLR. Wood, T., \u0026amp; Shawe-Taylor, J. (1996). Representation theory and invariant neural networks. Neural computation, 8(5), 1003-1013. Mallat, S. (2016). Understanding deep convolutional networks. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065), 20150203. Lee, J. M. (2013). Introduction to smooth manifolds. Springer Science \u0026amp; Business Media. ","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"","permalink":"http://localhost:1313/posts/geo_deeplearning1/","summary":"","title":"intro to Geo-deep-learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f} \\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$: $$ \\mathcal{R} = \\mathbb{E}_{(x,y) \\sim P}[L(\\widetilde{f}(x), f(x))] $$\nSuch constructed functions can approximate almost any function (Universal Approximation Theorem), but this does not mean we do not need inductive biases to constrain the learning problem. Given a function class $\\mathcal{F}$ with universal approximation capabilities, we can define a complexity function: $$c: \\mathcal{F} \\rightarrow \\mathbb{R}$$ defining our interpolation problem as: $$\\widetilde{f} = \\arg\\min_{g \\in \\mathcal{F}} c(g) \\quad s.t. \\quad \\widetilde{f}(x_i) = f(x_i), \\quad i = 1, . . . , N$$\nTherefore, we not only hope for good function fitting but also low function complexity. Such a complexity function is what we call the norm of the function. The benefit of this definition is that $\\mathcal{F}$ becomes a Banach space (a complete vector space equipped with a norm), allowing us to use tools from functional analysis to study their properties.\nGeometric Priors To overcome the \u0026ldquo;curse of dimensionality\u0026rdquo; of high-dimensional data, we must utilize the physical characteristics of the data (symmetry and scale separation); although the geometry of the data (Domain) may be complex, the signals defined on it constitute a Hilbert space with good mathematical properties, allowing us to still use tools from linear algebra and functional analysis to handle them.\nFor example, in image processing, data is typically represented as signals (pixel values) defined on a two-dimensional Euclidean space $\\mathbb{R}^2$. This space possesses translation and rotation symmetry, meaning that if we translate or rotate the image, its content and structure remain unchanged. Convolutional Neural Networks (CNNs) exploit this translation symmetry, capturing local features through shared weights, thereby improving learning efficiency and generalization ability.\nSymmetry and Invariance In machine learning, symmetry refers to the property that data or tasks remain unchanged under certain transformations. These transformations can be translation, rotation, scaling, etc. For example, in an image classification task, if we translate or rotate an image, the category of the image usually does not change. This invariance is what we hope the model can capture and utilize.\nSymmetry Groups Symmetry can be formalized through group theory. A group is a set equipped with a binary operation that satisfies properties such as closure, associativity, identity, and inverse.\nSymmetry operations form a group, called a symmetry group, proven as follows:\nClosure: If $g_1$ and $g_2$ are symmetry operations, then their composition $g_1 \\circ g_2$ is also a symmetry operation, because applying two symmetry transformations consecutively still preserves the invariance of the data. Associativity: If $g_1$ and $g_2$ are symmetry operations, then for any $g_3$, $(g_1 \\circ g_2) \\circ g_3 = g_1 \\circ (g_2 \\circ g_3)$, because the order of transformations does not affect the final result. Identity: There exists an identity operation $e$ such that for any symmetry operation $g$, $e \\circ g = g \\circ e = g$. This identity operation corresponds to performing no transformation. Inverse: For every symmetry operation $g$, there exists an inverse operation $g^{-1}$ such that $g \\circ g^{-1} = g^{-1} \\circ g = e$. This inverse operation corresponds to undoing the transformation. Note: $g \\circ h$ denotes applying transformation $g$ first, then transformation $h$.\nLet\u0026rsquo;s take an equilateral triangle as an example to illustrate the concept of symmetry groups: In this example, the symmetry group of the equilateral triangle contains six elements: three rotation operations (0 degrees, 120 degrees, 240 degrees) and three reflection operations (perpendicular lines through each vertex). These operations satisfy the four properties of a group, thus forming a group called $D_3$, the dihedral group of the triangle. Mathematically, we can view these operations as mappings: $$g: \\mathcal{X} \\rightarrow \\mathcal{X}$$ where $\\mathcal{X}$ is the data space. For each element $g \\in G$ in a symmetry group $G$, we have a corresponding mapping $g$. For example, $D_3$ can also be represented as a matrix group: $$ D_3 = \\left\\{ \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 2 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 3 \u0026amp; 1 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 1 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 3 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 1 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 2 \u0026amp; 1 \\end{pmatrix} \\right\\} $$\nReferences Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., \u0026amp; Vandergheynst, P. (2017). Geometric deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4), 18-42. Goodfellow, I., Bengio, Y., \u0026amp; Courville, A. (2016). Deep learning. MIT press. Cohen, T. S., \u0026amp; Welling, M. (2016). Group equivariant convolutional networks. In International conference on machine learning (pp. 2990-2999). PMLR. Kondor, R., \u0026amp; Trivedi, S. (2018). On the generalization of equivariance and convolution in neural networks to the action of compact groups. In International conference on machine learning (pp. 2747-2755). PMLR. Wood, T., \u0026amp; Shawe-Taylor, J. (1996). Representation theory and invariant neural networks. Neural computation, 8(5), 1003-1013. Mallat, S. (2016). Understanding deep convolutional networks. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065), 20150203. Lee, J. M. (2013). Introduction to smooth manifolds. Springer Science \u0026amp; Business Media. ","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"","permalink":"http://localhost:1313/posts/geo_deeplearning1/","summary":"","title":"intro to Geo-deep-learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f} \\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$: $$ \\mathcal{R} = \\mathbb{E}_{(x,y) \\sim P}[L(\\widetilde{f}(x), f(x))] $$\nSuch constructed functions can approximate almost any function (Universal Approximation Theorem), but this does not mean we do not need inductive biases to constrain the learning problem. Given a function class $\\mathcal{F}$ with universal approximation capabilities, we can define a complexity function: $$c: \\mathcal{F} \\rightarrow \\mathbb{R}$$ defining our interpolation problem as: $$\\widetilde{f} = \\arg\\min_{g \\in \\mathcal{F}} c(g) \\quad s.t. \\quad \\widetilde{f}(x_i) = f(x_i), \\quad i = 1, . . . , N$$\nTherefore, we not only hope for good function fitting but also low function complexity. Such a complexity function is what we call the norm of the function. The benefit of this definition is that $\\mathcal{F}$ becomes a Banach space (a complete vector space equipped with a norm), allowing us to use tools from functional analysis to study their properties.\nGeometric Priors To overcome the \u0026ldquo;curse of dimensionality\u0026rdquo; of high-dimensional data, we must utilize the physical characteristics of the data (symmetry and scale separation); although the geometry of the data (Domain) may be complex, the signals defined on it constitute a Hilbert space with good mathematical properties, allowing us to still use tools from linear algebra and functional analysis to handle them.\nFor example, in image processing, data is typically represented as signals (pixel values) defined on a two-dimensional Euclidean space $\\mathbb{R}^2$. This space possesses translation and rotation symmetry, meaning that if we translate or rotate the image, its content and structure remain unchanged. Convolutional Neural Networks (CNNs) exploit this translation symmetry, capturing local features through shared weights, thereby improving learning efficiency and generalization ability.\nSymmetry and Invariance In machine learning, symmetry refers to the property that data or tasks remain unchanged under certain transformations. These transformations can be translation, rotation, scaling, etc. For example, in an image classification task, if we translate or rotate an image, the category of the image usually does not change. This invariance is what we hope the model can capture and utilize.\nSymmetry Groups Symmetry can be formalized through group theory. A group is a set equipped with a binary operation that satisfies properties such as closure, associativity, identity, and inverse.\nSymmetry operations form a group, called a symmetry group, proven as follows:\nClosure: If $g_1$ and $g_2$ are symmetry operations, then their composition $g_1 \\circ g_2$ is also a symmetry operation, because applying two symmetry transformations consecutively still preserves the invariance of the data. Associativity: If $g_1$ and $g_2$ are symmetry operations, then for any $g_3$, $(g_1 \\circ g_2) \\circ g_3 = g_1 \\circ (g_2 \\circ g_3)$, because the order of transformations does not affect the final result. Identity: There exists an identity operation $e$ such that for any symmetry operation $g$, $e \\circ g = g \\circ e = g$. This identity operation corresponds to performing no transformation. Inverse: For every symmetry operation $g$, there exists an inverse operation $g^{-1}$ such that $g \\circ g^{-1} = g^{-1} \\circ g = e$. This inverse operation corresponds to undoing the transformation. Note: $g \\circ h$ denotes applying transformation $g$ first, then transformation $h$.\nLet\u0026rsquo;s take an equilateral triangle as an example to illustrate the concept of symmetry groups: In this example, the symmetry group of the equilateral triangle contains six elements: three rotation operations (0 degrees, 120 degrees, 240 degrees) and three reflection operations (perpendicular lines through each vertex). These operations satisfy the four properties of a group, thus forming a group called $D_3$, the dihedral group of the triangle. Mathematically, we can view these operations as mappings: $$g: \\mathcal{X} \\rightarrow \\mathcal{X}$$ where $\\mathcal{X}$ is the data space. For each element $g \\in G$ in a symmetry group $G$, we have a corresponding mapping $g$. For example, $D_3$ can also be represented as a matrix group: $$ D_3 = \\left\\{ \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 2 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 3 \u0026amp; 1 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 1 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 3 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 1 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 2 \u0026amp; 1 \\end{pmatrix} \\right\\} $$\nReferences Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., \u0026amp; Vandergheynst, P. (2017). Geometric deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4), 18-42. Goodfellow, I., Bengio, Y., \u0026amp; Courville, A. (2016). Deep learning. MIT press. Cohen, T. S., \u0026amp; Welling, M. (2016). Group equivariant convolutional networks. In International conference on machine learning (pp. 2990-2999). PMLR. Kondor, R., \u0026amp; Trivedi, S. (2018). On the generalization of equivariance and convolution in neural networks to the action of compact groups. In International conference on machine learning (pp. 2747-2755). PMLR. Wood, T., \u0026amp; Shawe-Taylor, J. (1996). Representation theory and invariant neural networks. Neural computation, 8(5), 1003-1013. Mallat, S. (2016). Understanding deep convolutional networks. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065), 20150203. Lee, J. M. (2013). Introduction to smooth manifolds. Springer Science \u0026amp; Business Media. ","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"","permalink":"http://localhost:1313/posts/geo_deeplearning1/","summary":"","title":"intro to Geo-deep-learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f} \\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$: $$ \\mathcal{R} = \\mathbb{E}_{(x,y) \\sim P}[L(\\widetilde{f}(x), f(x))] $$\nSuch constructed functions can approximate almost any function (Universal Approximation Theorem), but this does not mean we do not need inductive biases to constrain the learning problem. Given a function class $\\mathcal{F}$ with universal approximation capabilities, we can define a complexity function: $$c: \\mathcal{F} \\rightarrow \\mathbb{R}$$ defining our interpolation problem as: $$\\widetilde{f} = \\arg\\min_{g \\in \\mathcal{F}} c(g) \\quad s.t. \\quad \\widetilde{f}(x_i) = f(x_i), \\quad i = 1, . . . , N$$\nTherefore, we not only hope for good function fitting but also low function complexity. Such a complexity function is what we call the norm of the function. The benefit of this definition is that $\\mathcal{F}$ becomes a Banach space (a complete vector space equipped with a norm), allowing us to use tools from functional analysis to study their properties.\nGeometric Priors To overcome the \u0026ldquo;curse of dimensionality\u0026rdquo; of high-dimensional data, we must utilize the physical characteristics of the data (symmetry and scale separation); although the geometry of the data (Domain) may be complex, the signals defined on it constitute a Hilbert space with good mathematical properties, allowing us to still use tools from linear algebra and functional analysis to handle them.\nFor example, in image processing, data is typically represented as signals (pixel values) defined on a two-dimensional Euclidean space $\\mathbb{R}^2$. This space possesses translation and rotation symmetry, meaning that if we translate or rotate the image, its content and structure remain unchanged. Convolutional Neural Networks (CNNs) exploit this translation symmetry, capturing local features through shared weights, thereby improving learning efficiency and generalization ability.\nSymmetry and Invariance In machine learning, symmetry refers to the property that data or tasks remain unchanged under certain transformations. These transformations can be translation, rotation, scaling, etc. For example, in an image classification task, if we translate or rotate an image, the category of the image usually does not change. This invariance is what we hope the model can capture and utilize.\nSymmetry Groups Symmetry can be formalized through group theory. A group is a set equipped with a binary operation that satisfies properties such as closure, associativity, identity, and inverse.\nSymmetry operations form a group, called a symmetry group, proven as follows:\nClosure: If $g_1$ and $g_2$ are symmetry operations, then their composition $g_1 \\circ g_2$ is also a symmetry operation, because applying two symmetry transformations consecutively still preserves the invariance of the data. Associativity: If $g_1$ and $g_2$ are symmetry operations, then for any $g_3$, $(g_1 \\circ g_2) \\circ g_3 = g_1 \\circ (g_2 \\circ g_3)$, because the order of transformations does not affect the final result. Identity: There exists an identity operation $e$ such that for any symmetry operation $g$, $e \\circ g = g \\circ e = g$. This identity operation corresponds to performing no transformation. Inverse: For every symmetry operation $g$, there exists an inverse operation $g^{-1}$ such that $g \\circ g^{-1} = g^{-1} \\circ g = e$. This inverse operation corresponds to undoing the transformation. Note: $g \\circ h$ denotes applying transformation $g$ first, then transformation $h$.\nLet\u0026rsquo;s take an equilateral triangle as an example to illustrate the concept of symmetry groups: In this example, the symmetry group of the equilateral triangle contains six elements: three rotation operations (0 degrees, 120 degrees, 240 degrees) and three reflection operations (perpendicular lines through each vertex). These operations satisfy the four properties of a group, thus forming a group called $D_3$, the dihedral group of the triangle. Mathematically, we can view these operations as mappings: $$g: \\mathcal{X} \\rightarrow \\mathcal{X}$$ where $\\mathcal{X}$ is the data space. For each element $g \\in G$ in a symmetry group $G$, we have a corresponding mapping $g$. For example, $D_3$ can also be represented as a matrix group: $$ D_3 = \\left\\{ \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 2 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 3 \u0026amp; 1 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 1 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 3 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 1 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 2 \u0026amp; 1 \\end{pmatrix} \\right\\} $$\nReferences Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., \u0026amp; Vandergheynst, P. (2017). Geometric deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4), 18-42. Goodfellow, I., Bengio, Y., \u0026amp; Courville, A. (2016). Deep learning. MIT press. Cohen, T. S., \u0026amp; Welling, M. (2016). Group equivariant convolutional networks. In International conference on machine learning (pp. 2990-2999). PMLR. Kondor, R., \u0026amp; Trivedi, S. (2018). On the generalization of equivariance and convolution in neural networks to the action of compact groups. In International conference on machine learning (pp. 2747-2755). PMLR. Wood, T., \u0026amp; Shawe-Taylor, J. (1996). Representation theory and invariant neural networks. Neural computation, 8(5), 1003-1013. Mallat, S. (2016). Understanding deep convolutional networks. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065), 20150203. Lee, J. M. (2013). Introduction to smooth manifolds. Springer Science \u0026amp; Business Media. ","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"","permalink":"http://localhost:1313/posts/geo_deeplearning1/","summary":"","title":"intro to Geo-deep-learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f} \\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$: $$ \\mathcal{R} = \\mathbb{E}_{(x,y) \\sim P}[L(\\widetilde{f}(x), f(x))] $$\nSuch constructed functions can approximate almost any function (Universal Approximation Theorem), but this does not mean we do not need inductive biases to constrain the learning problem. Given a function class $\\mathcal{F}$ with universal approximation capabilities, we can define a complexity function: $$c: \\mathcal{F} \\rightarrow \\mathbb{R}$$ defining our interpolation problem as: $$\\widetilde{f} = \\arg\\min_{g \\in \\mathcal{F}} c(g) \\quad s.t. \\quad \\widetilde{f}(x_i) = f(x_i), \\quad i = 1, . . . , N$$\nTherefore, we not only hope for good function fitting but also low function complexity. Such a complexity function is what we call the norm of the function. The benefit of this definition is that $\\mathcal{F}$ becomes a Banach space (a complete vector space equipped with a norm), allowing us to use tools from functional analysis to study their properties.\nGeometric Priors To overcome the \u0026ldquo;curse of dimensionality\u0026rdquo; of high-dimensional data, we must utilize the physical characteristics of the data (symmetry and scale separation); although the geometry of the data (Domain) may be complex, the signals defined on it constitute a Hilbert space with good mathematical properties, allowing us to still use tools from linear algebra and functional analysis to handle them.\nFor example, in image processing, data is typically represented as signals (pixel values) defined on a two-dimensional Euclidean space $\\mathbb{R}^2$. This space possesses translation and rotation symmetry, meaning that if we translate or rotate the image, its content and structure remain unchanged. Convolutional Neural Networks (CNNs) exploit this translation symmetry, capturing local features through shared weights, thereby improving learning efficiency and generalization ability.\nSymmetry and Invariance In machine learning, symmetry refers to the property that data or tasks remain unchanged under certain transformations. These transformations can be translation, rotation, scaling, etc. For example, in an image classification task, if we translate or rotate an image, the category of the image usually does not change. This invariance is what we hope the model can capture and utilize.\nSymmetry Groups Symmetry can be formalized through group theory. A group is a set equipped with a binary operation that satisfies properties such as closure, associativity, identity, and inverse.\nSymmetry operations form a group, called a symmetry group, proven as follows:\nClosure: If $g_1$ and $g_2$ are symmetry operations, then their composition $g_1 \\circ g_2$ is also a symmetry operation, because applying two symmetry transformations consecutively still preserves the invariance of the data. Associativity: If $g_1$ and $g_2$ are symmetry operations, then for any $g_3$, $(g_1 \\circ g_2) \\circ g_3 = g_1 \\circ (g_2 \\circ g_3)$, because the order of transformations does not affect the final result. Identity: There exists an identity operation $e$ such that for any symmetry operation $g$, $e \\circ g = g \\circ e = g$. This identity operation corresponds to performing no transformation. Inverse: For every symmetry operation $g$, there exists an inverse operation $g^{-1}$ such that $g \\circ g^{-1} = g^{-1} \\circ g = e$. This inverse operation corresponds to undoing the transformation. Note: $g \\circ h$ denotes applying transformation $g$ first, then transformation $h$.\nLet\u0026rsquo;s take an equilateral triangle as an example to illustrate the concept of symmetry groups: In this example, the symmetry group of the equilateral triangle contains six elements: three rotation operations (0 degrees, 120 degrees, 240 degrees) and three reflection operations (perpendicular lines through each vertex). These operations satisfy the four properties of a group, thus forming a group called $D_3$, the dihedral group of the triangle. Mathematically, we can view these operations as mappings: $$g: \\mathcal{X} \\rightarrow \\mathcal{X}$$ where $\\mathcal{X}$ is the data space. For each element $g \\in G$ in a symmetry group $G$, we have a corresponding mapping $g$. For example, $D_3$ can also be represented as a matrix group: $$ D_3 = \\left\\{ \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 2 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 3 \u0026amp; 1 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 1 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 3 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 1 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 2 \u0026amp; 1 \\end{pmatrix} \\right\\} $$\nReferences Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., \u0026amp; Vandergheynst, P. (2017). Geometric deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4), 18-42. Goodfellow, I., Bengio, Y., \u0026amp; Courville, A. (2016). Deep learning. MIT press. Cohen, T. S., \u0026amp; Welling, M. (2016). Group equivariant convolutional networks. In International conference on machine learning (pp. 2990-2999). PMLR. Kondor, R., \u0026amp; Trivedi, S. (2018). On the generalization of equivariance and convolution in neural networks to the action of compact groups. In International conference on machine learning (pp. 2747-2755). PMLR. Wood, T., \u0026amp; Shawe-Taylor, J. (1996). Representation theory and invariant neural networks. Neural computation, 8(5), 1003-1013. Mallat, S. (2016). Understanding deep convolutional networks. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065), 20150203. Lee, J. M. (2013). Introduction to smooth manifolds. Springer Science \u0026amp; Business Media. ","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"","permalink":"http://localhost:1313/posts/geo_deeplearning1/","summary":"","title":"intro to Geo-deep-learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f} \\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$: $$ \\mathcal{R} = \\mathbb{E}_{(x,y) \\sim P}[L(\\widetilde{f}(x), f(x))] $$\nSuch constructed functions can approximate almost any function (Universal Approximation Theorem), but this does not mean we do not need inductive biases to constrain the learning problem. Given a function class $\\mathcal{F}$ with universal approximation capabilities, we can define a complexity function: $$c: \\mathcal{F} \\rightarrow \\mathbb{R}$$ defining our interpolation problem as: $$\\widetilde{f} = \\arg\\min_{g \\in \\mathcal{F}} c(g) \\quad s.t. \\quad \\widetilde{f}(x_i) = f(x_i), \\quad i = 1, . . . , N$$\nTherefore, we not only hope for good function fitting but also low function complexity. Such a complexity function is what we call the norm of the function. The benefit of this definition is that $\\mathcal{F}$ becomes a Banach space (a complete vector space equipped with a norm), allowing us to use tools from functional analysis to study their properties.\nGeometric Priors To overcome the \u0026ldquo;curse of dimensionality\u0026rdquo; of high-dimensional data, we must utilize the physical characteristics of the data (symmetry and scale separation); although the geometry of the data (Domain) may be complex, the signals defined on it constitute a Hilbert space with good mathematical properties, allowing us to still use tools from linear algebra and functional analysis to handle them.\nFor example, in image processing, data is typically represented as signals (pixel values) defined on a two-dimensional Euclidean space $\\mathbb{R}^2$. This space possesses translation and rotation symmetry, meaning that if we translate or rotate the image, its content and structure remain unchanged. Convolutional Neural Networks (CNNs) exploit this translation symmetry, capturing local features through shared weights, thereby improving learning efficiency and generalization ability.\nSymmetry and Invariance In machine learning, symmetry refers to the property that data or tasks remain unchanged under certain transformations. These transformations can be translation, rotation, scaling, etc. For example, in an image classification task, if we translate or rotate an image, the category of the image usually does not change. This invariance is what we hope the model can capture and utilize.\nSymmetry Groups Symmetry can be formalized through group theory. A group is a set equipped with a binary operation that satisfies properties such as closure, associativity, identity, and inverse.\nSymmetry operations form a group, called a symmetry group, proven as follows:\nClosure: If $g_1$ and $g_2$ are symmetry operations, then their composition $g_1 \\circ g_2$ is also a symmetry operation, because applying two symmetry transformations consecutively still preserves the invariance of the data. Associativity: If $g_1$ and $g_2$ are symmetry operations, then for any $g_3$, $(g_1 \\circ g_2) \\circ g_3 = g_1 \\circ (g_2 \\circ g_3)$, because the order of transformations does not affect the final result. Identity: There exists an identity operation $e$ such that for any symmetry operation $g$, $e \\circ g = g \\circ e = g$. This identity operation corresponds to performing no transformation. Inverse: For every symmetry operation $g$, there exists an inverse operation $g^{-1}$ such that $g \\circ g^{-1} = g^{-1} \\circ g = e$. This inverse operation corresponds to undoing the transformation. Note: $g \\circ h$ denotes applying transformation $g$ first, then transformation $h$.\nLet\u0026rsquo;s take an equilateral triangle as an example to illustrate the concept of symmetry groups: In this example, the symmetry group of the equilateral triangle contains six elements: three rotation operations (0 degrees, 120 degrees, 240 degrees) and three reflection operations (perpendicular lines through each vertex). These operations satisfy the four properties of a group, thus forming a group called $D_3$, the dihedral group of the triangle. Mathematically, we can view these operations as mappings: $$g: \\mathcal{X} \\rightarrow \\mathcal{X}$$ where $\\mathcal{X}$ is the data space. For each element $g \\in G$ in a symmetry group $G$, we have a corresponding mapping $g$. For example, $D_3$ can also be represented as a matrix group: $$ D_3 = \\left\\{ \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 2 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 3 \u0026amp; 1 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 1 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 3 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 1 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 2 \u0026amp; 1 \\end{pmatrix} \\right\\} $$\nReferences Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., \u0026amp; Vandergheynst, P. (2017). Geometric deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4), 18-42. Goodfellow, I., Bengio, Y., \u0026amp; Courville, A. (2016). Deep learning. MIT press. Cohen, T. S., \u0026amp; Welling, M. (2016). Group equivariant convolutional networks. In International conference on machine learning (pp. 2990-2999). PMLR. Kondor, R., \u0026amp; Trivedi, S. (2018). On the generalization of equivariance and convolution in neural networks to the action of compact groups. In International conference on machine learning (pp. 2747-2755). PMLR. Wood, T., \u0026amp; Shawe-Taylor, J. (1996). Representation theory and invariant neural networks. Neural computation, 8(5), 1003-1013. Mallat, S. (2016). Understanding deep convolutional networks. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065), 20150203. Lee, J. M. (2013). Introduction to smooth manifolds. Springer Science \u0026amp; Business Media. ","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"","permalink":"http://localhost:1313/posts/geo_deeplearning1/","summary":"","title":"intro to Geo-deep-learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f} \\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$: $$ \\mathcal{R} = \\mathbb{E}_{(x,y) \\sim P}[L(\\widetilde{f}(x), f(x))] $$\nSuch constructed functions can approximate almost any function (Universal Approximation Theorem), but this does not mean we do not need inductive biases to constrain the learning problem. Given a function class $\\mathcal{F}$ with universal approximation capabilities, we can define a complexity function: $$c: \\mathcal{F} \\rightarrow \\mathbb{R}$$ defining our interpolation problem as: $$\\widetilde{f} = \\arg\\min_{g \\in \\mathcal{F}} c(g) \\quad s.t. \\quad \\widetilde{f}(x_i) = f(x_i), \\quad i = 1, . . . , N$$\nTherefore, we not only hope for good function fitting but also low function complexity. Such a complexity function is what we call the norm of the function. The benefit of this definition is that $\\mathcal{F}$ becomes a Banach space (a complete vector space equipped with a norm), allowing us to use tools from functional analysis to study their properties.\nGeometric Priors To overcome the \u0026ldquo;curse of dimensionality\u0026rdquo; of high-dimensional data, we must utilize the physical characteristics of the data (symmetry and scale separation); although the geometry of the data (Domain) may be complex, the signals defined on it constitute a Hilbert space with good mathematical properties, allowing us to still use tools from linear algebra and functional analysis to handle them.\nFor example, in image processing, data is typically represented as signals (pixel values) defined on a two-dimensional Euclidean space $\\mathbb{R}^2$. This space possesses translation and rotation symmetry, meaning that if we translate or rotate the image, its content and structure remain unchanged. Convolutional Neural Networks (CNNs) exploit this translation symmetry, capturing local features through shared weights, thereby improving learning efficiency and generalization ability.\nSymmetry and Invariance In machine learning, symmetry refers to the property that data or tasks remain unchanged under certain transformations. These transformations can be translation, rotation, scaling, etc. For example, in an image classification task, if we translate or rotate an image, the category of the image usually does not change. This invariance is what we hope the model can capture and utilize.\nSymmetry Groups Symmetry can be formalized through group theory. A group is a set equipped with a binary operation that satisfies properties such as closure, associativity, identity, and inverse.\nSymmetry operations form a group, called a symmetry group, proven as follows:\nClosure: If $g_1$ and $g_2$ are symmetry operations, then their composition $g_1 \\circ g_2$ is also a symmetry operation, because applying two symmetry transformations consecutively still preserves the invariance of the data. Associativity: If $g_1$ and $g_2$ are symmetry operations, then for any $g_3$, $(g_1 \\circ g_2) \\circ g_3 = g_1 \\circ (g_2 \\circ g_3)$, because the order of transformations does not affect the final result. Identity: There exists an identity operation $e$ such that for any symmetry operation $g$, $e \\circ g = g \\circ e = g$. This identity operation corresponds to performing no transformation. Inverse: For every symmetry operation $g$, there exists an inverse operation $g^{-1}$ such that $g \\circ g^{-1} = g^{-1} \\circ g = e$. This inverse operation corresponds to undoing the transformation. Note: $g \\circ h$ denotes applying transformation $g$ first, then transformation $h$.\nLet\u0026rsquo;s take an equilateral triangle as an example to illustrate the concept of symmetry groups: In this example, the symmetry group of the equilateral triangle contains six elements: three rotation operations (0 degrees, 120 degrees, 240 degrees) and three reflection operations (perpendicular lines through each vertex). These operations satisfy the four properties of a group, thus forming a group called $D_3$, the dihedral group of the triangle. Mathematically, we can view these operations as mappings: $$g: \\mathcal{X} \\rightarrow \\mathcal{X}$$ where $\\mathcal{X}$ is the data space. For each element $g \\in G$ in a symmetry group $G$, we have a corresponding mapping $g$. For example, $D_3$ can also be represented as a matrix group: $$ D_3 = \\left\\{ \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 2 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 3 \u0026amp; 1 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 1 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 3 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 1 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 2 \u0026amp; 1 \\end{pmatrix} \\right\\} $$\nReferences Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., \u0026amp; Vandergheynst, P. (2017). Geometric deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4), 18-42. Goodfellow, I., Bengio, Y., \u0026amp; Courville, A. (2016). Deep learning. MIT press. Cohen, T. S., \u0026amp; Welling, M. (2016). Group equivariant convolutional networks. In International conference on machine learning (pp. 2990-2999). PMLR. Kondor, R., \u0026amp; Trivedi, S. (2018). On the generalization of equivariance and convolution in neural networks to the action of compact groups. In International conference on machine learning (pp. 2747-2755). PMLR. Wood, T., \u0026amp; Shawe-Taylor, J. (1996). Representation theory and invariant neural networks. Neural computation, 8(5), 1003-1013. Mallat, S. (2016). Understanding deep convolutional networks. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065), 20150203. Lee, J. M. (2013). Introduction to smooth manifolds. Springer Science \u0026amp; Business Media. ","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"","permalink":"http://localhost:1313/posts/geo_deeplearning1/","summary":"","title":"intro to Geo-deep-learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f} \\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$: $$ \\mathcal{R} = \\mathbb{E}_{(x,y) \\sim P}[L(\\widetilde{f}(x), f(x))] $$\nSuch constructed functions can approximate almost any function (Universal Approximation Theorem), but this does not mean we do not need inductive biases to constrain the learning problem. Given a function class $\\mathcal{F}$ with universal approximation capabilities, we can define a complexity function: $$c: \\mathcal{F} \\rightarrow \\mathbb{R}$$ defining our interpolation problem as: $$\\widetilde{f} = \\arg\\min_{g \\in \\mathcal{F}} c(g) \\quad s.t. \\quad \\widetilde{f}(x_i) = f(x_i), \\quad i = 1, . . . , N$$\nTherefore, we not only hope for good function fitting but also low function complexity. Such a complexity function is what we call the norm of the function. The benefit of this definition is that $\\mathcal{F}$ becomes a Banach space (a complete vector space equipped with a norm), allowing us to use tools from functional analysis to study their properties.\nGeometric Priors To overcome the \u0026ldquo;curse of dimensionality\u0026rdquo; of high-dimensional data, we must utilize the physical characteristics of the data (symmetry and scale separation); although the geometry of the data (Domain) may be complex, the signals defined on it constitute a Hilbert space with good mathematical properties, allowing us to still use tools from linear algebra and functional analysis to handle them.\nFor example, in image processing, data is typically represented as signals (pixel values) defined on a two-dimensional Euclidean space $\\mathbb{R}^2$. This space possesses translation and rotation symmetry, meaning that if we translate or rotate the image, its content and structure remain unchanged. Convolutional Neural Networks (CNNs) exploit this translation symmetry, capturing local features through shared weights, thereby improving learning efficiency and generalization ability.\nSymmetry and Invariance In machine learning, symmetry refers to the property that data or tasks remain unchanged under certain transformations. These transformations can be translation, rotation, scaling, etc. For example, in an image classification task, if we translate or rotate an image, the category of the image usually does not change. This invariance is what we hope the model can capture and utilize.\nSymmetry Groups Symmetry can be formalized through group theory. A group is a set equipped with a binary operation that satisfies properties such as closure, associativity, identity, and inverse.\nSymmetry operations form a group, called a symmetry group, proven as follows:\nClosure: If $g_1$ and $g_2$ are symmetry operations, then their composition $g_1 \\circ g_2$ is also a symmetry operation, because applying two symmetry transformations consecutively still preserves the invariance of the data. Associativity: If $g_1$ and $g_2$ are symmetry operations, then for any $g_3$, $(g_1 \\circ g_2) \\circ g_3 = g_1 \\circ (g_2 \\circ g_3)$, because the order of transformations does not affect the final result. Identity: There exists an identity operation $e$ such that for any symmetry operation $g$, $e \\circ g = g \\circ e = g$. This identity operation corresponds to performing no transformation. Inverse: For every symmetry operation $g$, there exists an inverse operation $g^{-1}$ such that $g \\circ g^{-1} = g^{-1} \\circ g = e$. This inverse operation corresponds to undoing the transformation. Note: $g \\circ h$ denotes applying transformation $g$ first, then transformation $h$.\nLet\u0026rsquo;s take an equilateral triangle as an example to illustrate the concept of symmetry groups: In this example, the symmetry group of the equilateral triangle contains six elements: three rotation operations (0 degrees, 120 degrees, 240 degrees) and three reflection operations (perpendicular lines through each vertex). These operations satisfy the four properties of a group, thus forming a group called $D_3$, the dihedral group of the triangle. Mathematically, we can view these operations as mappings: $$g: \\mathcal{X} \\rightarrow \\mathcal{X}$$ where $\\mathcal{X}$ is the data space. For each element $g \\in G$ in a symmetry group $G$, we have a corresponding mapping $g$. For example, $D_3$ can also be represented as a matrix group: $$ D_3 = \\left\\{ \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 2 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 3 \u0026amp; 1 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 1 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 3 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 1 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 2 \u0026amp; 1 \\end{pmatrix} \\right\\} $$\nReferences Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., \u0026amp; Vandergheynst, P. (2017). Geometric deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4), 18-42. Goodfellow, I., Bengio, Y., \u0026amp; Courville, A. (2016). Deep learning. MIT press. Cohen, T. S., \u0026amp; Welling, M. (2016). Group equivariant convolutional networks. In International conference on machine learning (pp. 2990-2999). PMLR. Kondor, R., \u0026amp; Trivedi, S. (2018). On the generalization of equivariance and convolution in neural networks to the action of compact groups. In International conference on machine learning (pp. 2747-2755). PMLR. Wood, T., \u0026amp; Shawe-Taylor, J. (1996). Representation theory and invariant neural networks. Neural computation, 8(5), 1003-1013. Mallat, S. (2016). Understanding deep convolutional networks. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065), 20150203. Lee, J. M. (2013). Introduction to smooth manifolds. Springer Science \u0026amp; Business Media. ","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"","permalink":"http://localhost:1313/posts/geo_deeplearning1/","summary":"","title":"intro to Geo-deep-learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f} \\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$: $$ \\mathcal{R} = \\mathbb{E}_{(x,y) \\sim P}[L(\\widetilde{f}(x), f(x))] $$\nSuch constructed functions can approximate almost any function (Universal Approximation Theorem), but this does not mean we do not need inductive biases to constrain the learning problem. Given a function class $\\mathcal{F}$ with universal approximation capabilities, we can define a complexity function: $$c: \\mathcal{F} \\rightarrow \\mathbb{R}$$ defining our interpolation problem as: $$\\widetilde{f} = \\arg\\min_{g \\in \\mathcal{F}} c(g) \\quad s.t. \\quad \\widetilde{f}(x_i) = f(x_i), \\quad i = 1, . . . , N$$\nTherefore, we not only hope for good function fitting but also low function complexity. Such a complexity function is what we call the norm of the function. The benefit of this definition is that $\\mathcal{F}$ becomes a Banach space (a complete vector space equipped with a norm), allowing us to use tools from functional analysis to study their properties.\nGeometric Priors To overcome the \u0026ldquo;curse of dimensionality\u0026rdquo; of high-dimensional data, we must utilize the physical characteristics of the data (symmetry and scale separation); although the geometry of the data (Domain) may be complex, the signals defined on it constitute a Hilbert space with good mathematical properties, allowing us to still use tools from linear algebra and functional analysis to handle them.\nFor example, in image processing, data is typically represented as signals (pixel values) defined on a two-dimensional Euclidean space $\\mathbb{R}^2$. This space possesses translation and rotation symmetry, meaning that if we translate or rotate the image, its content and structure remain unchanged. Convolutional Neural Networks (CNNs) exploit this translation symmetry, capturing local features through shared weights, thereby improving learning efficiency and generalization ability.\nSymmetry and Invariance In machine learning, symmetry refers to the property that data or tasks remain unchanged under certain transformations. These transformations can be translation, rotation, scaling, etc. For example, in an image classification task, if we translate or rotate an image, the category of the image usually does not change. This invariance is what we hope the model can capture and utilize.\nSymmetry Groups Symmetry can be formalized through group theory. A group is a set equipped with a binary operation that satisfies properties such as closure, associativity, identity, and inverse.\nSymmetry operations form a group, called a symmetry group, proven as follows:\nClosure: If $g_1$ and $g_2$ are symmetry operations, then their composition $g_1 \\circ g_2$ is also a symmetry operation, because applying two symmetry transformations consecutively still preserves the invariance of the data. Associativity: If $g_1$ and $g_2$ are symmetry operations, then for any $g_3$, $(g_1 \\circ g_2) \\circ g_3 = g_1 \\circ (g_2 \\circ g_3)$, because the order of transformations does not affect the final result. Identity: There exists an identity operation $e$ such that for any symmetry operation $g$, $e \\circ g = g \\circ e = g$. This identity operation corresponds to performing no transformation. Inverse: For every symmetry operation $g$, there exists an inverse operation $g^{-1}$ such that $g \\circ g^{-1} = g^{-1} \\circ g = e$. This inverse operation corresponds to undoing the transformation. Note: $g \\circ h$ denotes applying transformation $g$ first, then transformation $h$.\nLet\u0026rsquo;s take an equilateral triangle as an example to illustrate the concept of symmetry groups: In this example, the symmetry group of the equilateral triangle contains six elements: three rotation operations (0 degrees, 120 degrees, 240 degrees) and three reflection operations (perpendicular lines through each vertex). These operations satisfy the four properties of a group, thus forming a group called $D_3$, the dihedral group of the triangle. Mathematically, we can view these operations as mappings: $$g: \\mathcal{X} \\rightarrow \\mathcal{X}$$ where $\\mathcal{X}$ is the data space. For each element $g \\in G$ in a symmetry group $G$, we have a corresponding mapping $g$. For example, $D_3$ can also be represented as a matrix group: $$ D_3 = \\left\\{ \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 2 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 3 \u0026amp; 1 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 1 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 3 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 1 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 2 \u0026amp; 1 \\end{pmatrix} \\right\\} $$\nReferences Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., \u0026amp; Vandergheynst, P. (2017). Geometric deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4), 18-42. Goodfellow, I., Bengio, Y., \u0026amp; Courville, A. (2016). Deep learning. MIT press. Cohen, T. S., \u0026amp; Welling, M. (2016). Group equivariant convolutional networks. In International conference on machine learning (pp. 2990-2999). PMLR. Kondor, R., \u0026amp; Trivedi, S. (2018). On the generalization of equivariance and convolution in neural networks to the action of compact groups. In International conference on machine learning (pp. 2747-2755). PMLR. Wood, T., \u0026amp; Shawe-Taylor, J. (1996). Representation theory and invariant neural networks. Neural computation, 8(5), 1003-1013. Mallat, S. (2016). Understanding deep convolutional networks. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065), 20150203. Lee, J. M. (2013). Introduction to smooth manifolds. Springer Science \u0026amp; Business Media. ","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"","permalink":"http://localhost:1313/posts/geo_deeplearning1/","summary":"","title":"intro to Geo-deep-learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f} \\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$: $$ \\mathcal{R} = \\mathbb{E}_{(x,y) \\sim P}[L(\\widetilde{f}(x), f(x))] $$\nSuch constructed functions can approximate almost any function (Universal Approximation Theorem), but this does not mean we do not need inductive biases to constrain the learning problem. Given a function class $\\mathcal{F}$ with universal approximation capabilities, we can define a complexity function: $$c: \\mathcal{F} \\rightarrow \\mathbb{R}$$ defining our interpolation problem as: $$\\widetilde{f} = \\arg\\min_{g \\in \\mathcal{F}} c(g) \\quad s.t. \\quad \\widetilde{f}(x_i) = f(x_i), \\quad i = 1, . . . , N$$\nTherefore, we not only hope for good function fitting but also low function complexity. Such a complexity function is what we call the norm of the function. The benefit of this definition is that $\\mathcal{F}$ becomes a Banach space (a complete vector space equipped with a norm), allowing us to use tools from functional analysis to study their properties.\nGeometric Priors To overcome the \u0026ldquo;curse of dimensionality\u0026rdquo; of high-dimensional data, we must utilize the physical characteristics of the data (symmetry and scale separation); although the geometry of the data (Domain) may be complex, the signals defined on it constitute a Hilbert space with good mathematical properties, allowing us to still use tools from linear algebra and functional analysis to handle them.\nFor example, in image processing, data is typically represented as signals (pixel values) defined on a two-dimensional Euclidean space $\\mathbb{R}^2$. This space possesses translation and rotation symmetry, meaning that if we translate or rotate the image, its content and structure remain unchanged. Convolutional Neural Networks (CNNs) exploit this translation symmetry, capturing local features through shared weights, thereby improving learning efficiency and generalization ability.\nSymmetry and Invariance In machine learning, symmetry refers to the property that data or tasks remain unchanged under certain transformations. These transformations can be translation, rotation, scaling, etc. For example, in an image classification task, if we translate or rotate an image, the category of the image usually does not change. This invariance is what we hope the model can capture and utilize.\nSymmetry Groups Symmetry can be formalized through group theory. A group is a set equipped with a binary operation that satisfies properties such as closure, associativity, identity, and inverse.\nSymmetry operations form a group, called a symmetry group, proven as follows:\nClosure: If $g_1$ and $g_2$ are symmetry operations, then their composition $g_1 \\circ g_2$ is also a symmetry operation, because applying two symmetry transformations consecutively still preserves the invariance of the data. Associativity: If $g_1$ and $g_2$ are symmetry operations, then for any $g_3$, $(g_1 \\circ g_2) \\circ g_3 = g_1 \\circ (g_2 \\circ g_3)$, because the order of transformations does not affect the final result. Identity: There exists an identity operation $e$ such that for any symmetry operation $g$, $e \\circ g = g \\circ e = g$. This identity operation corresponds to performing no transformation. Inverse: For every symmetry operation $g$, there exists an inverse operation $g^{-1}$ such that $g \\circ g^{-1} = g^{-1} \\circ g = e$. This inverse operation corresponds to undoing the transformation. Note: $g \\circ h$ denotes applying transformation $g$ first, then transformation $h$.\nLet\u0026rsquo;s take an equilateral triangle as an example to illustrate the concept of symmetry groups: In this example, the symmetry group of the equilateral triangle contains six elements: three rotation operations (0 degrees, 120 degrees, 240 degrees) and three reflection operations (perpendicular lines through each vertex). These operations satisfy the four properties of a group, thus forming a group called $D_3$, the dihedral group of the triangle. Mathematically, we can view these operations as mappings: $$g: \\mathcal{X} \\rightarrow \\mathcal{X}$$ where $\\mathcal{X}$ is the data space. For each element $g \\in G$ in a symmetry group $G$, we have a corresponding mapping $g$. For example, $D_3$ can also be represented as a matrix group: $$ D_3 = \\left\\{ \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 2 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 3 \u0026amp; 1 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 1 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 3 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 1 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 2 \u0026amp; 1 \\end{pmatrix} \\right\\} $$\nReferences Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., \u0026amp; Vandergheynst, P. (2017). Geometric deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4), 18-42. Goodfellow, I., Bengio, Y., \u0026amp; Courville, A. (2016). Deep learning. MIT press. Cohen, T. S., \u0026amp; Welling, M. (2016). Group equivariant convolutional networks. In International conference on machine learning (pp. 2990-2999). PMLR. Kondor, R., \u0026amp; Trivedi, S. (2018). On the generalization of equivariance and convolution in neural networks to the action of compact groups. In International conference on machine learning (pp. 2747-2755). PMLR. Wood, T., \u0026amp; Shawe-Taylor, J. (1996). Representation theory and invariant neural networks. Neural computation, 8(5), 1003-1013. Mallat, S. (2016). Understanding deep convolutional networks. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065), 20150203. Lee, J. M. (2013). Introduction to smooth manifolds. Springer Science \u0026amp; Business Media. ","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"","permalink":"http://localhost:1313/posts/geo_deeplearning1/","summary":"","title":"intro to Geo-deep-learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f} \\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$: $$ \\mathcal{R} = \\mathbb{E}_{(x,y) \\sim P}[L(\\widetilde{f}(x), f(x))] $$\nSuch constructed functions can approximate almost any function (Universal Approximation Theorem), but this does not mean we do not need inductive biases to constrain the learning problem. Given a function class $\\mathcal{F}$ with universal approximation capabilities, we can define a complexity function: $$c: \\mathcal{F} \\rightarrow \\mathbb{R}$$ defining our interpolation problem as: $$\\widetilde{f} = \\arg\\min_{g \\in \\mathcal{F}} c(g) \\quad s.t. \\quad \\widetilde{f}(x_i) = f(x_i), \\quad i = 1, . . . , N$$\nTherefore, we not only hope for good function fitting but also low function complexity. Such a complexity function is what we call the norm of the function. The benefit of this definition is that $\\mathcal{F}$ becomes a Banach space (a complete vector space equipped with a norm), allowing us to use tools from functional analysis to study their properties.\nGeometric Priors To overcome the \u0026ldquo;curse of dimensionality\u0026rdquo; of high-dimensional data, we must utilize the physical characteristics of the data (symmetry and scale separation); although the geometry of the data (Domain) may be complex, the signals defined on it constitute a Hilbert space with good mathematical properties, allowing us to still use tools from linear algebra and functional analysis to handle them.\nFor example, in image processing, data is typically represented as signals (pixel values) defined on a two-dimensional Euclidean space $\\mathbb{R}^2$. This space possesses translation and rotation symmetry, meaning that if we translate or rotate the image, its content and structure remain unchanged. Convolutional Neural Networks (CNNs) exploit this translation symmetry, capturing local features through shared weights, thereby improving learning efficiency and generalization ability.\nSymmetry and Invariance In machine learning, symmetry refers to the property that data or tasks remain unchanged under certain transformations. These transformations can be translation, rotation, scaling, etc. For example, in an image classification task, if we translate or rotate an image, the category of the image usually does not change. This invariance is what we hope the model can capture and utilize.\nSymmetry Groups Symmetry can be formalized through group theory. A group is a set equipped with a binary operation that satisfies properties such as closure, associativity, identity, and inverse.\nSymmetry operations form a group, called a symmetry group, proven as follows:\nClosure: If $g_1$ and $g_2$ are symmetry operations, then their composition $g_1 \\circ g_2$ is also a symmetry operation, because applying two symmetry transformations consecutively still preserves the invariance of the data. Associativity: If $g_1$ and $g_2$ are symmetry operations, then for any $g_3$, $(g_1 \\circ g_2) \\circ g_3 = g_1 \\circ (g_2 \\circ g_3)$, because the order of transformations does not affect the final result. Identity: There exists an identity operation $e$ such that for any symmetry operation $g$, $e \\circ g = g \\circ e = g$. This identity operation corresponds to performing no transformation. Inverse: For every symmetry operation $g$, there exists an inverse operation $g^{-1}$ such that $g \\circ g^{-1} = g^{-1} \\circ g = e$. This inverse operation corresponds to undoing the transformation. Note: $g \\circ h$ denotes applying transformation $g$ first, then transformation $h$.\nLet\u0026rsquo;s take an equilateral triangle as an example to illustrate the concept of symmetry groups: In this example, the symmetry group of the equilateral triangle contains six elements: three rotation operations (0 degrees, 120 degrees, 240 degrees) and three reflection operations (perpendicular lines through each vertex). These operations satisfy the four properties of a group, thus forming a group called $D_3$, the dihedral group of the triangle. Mathematically, we can view these operations as mappings: $$g: \\mathcal{X} \\rightarrow \\mathcal{X}$$ where $\\mathcal{X}$ is the data space. For each element $g \\in G$ in a symmetry group $G$, we have a corresponding mapping $g$. For example, $D_3$ can also be represented as a matrix group: $$ D_3 = \\left\\{ \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 2 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 3 \u0026amp; 1 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 1 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 3 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 1 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 2 \u0026amp; 1 \\end{pmatrix} \\right\\} $$\nReferences Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., \u0026amp; Vandergheynst, P. (2017). Geometric deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4), 18-42. Goodfellow, I., Bengio, Y., \u0026amp; Courville, A. (2016). Deep learning. MIT press. Cohen, T. S., \u0026amp; Welling, M. (2016). Group equivariant convolutional networks. In International conference on machine learning (pp. 2990-2999). PMLR. Kondor, R., \u0026amp; Trivedi, S. (2018). On the generalization of equivariance and convolution in neural networks to the action of compact groups. In International conference on machine learning (pp. 2747-2755). PMLR. Wood, T., \u0026amp; Shawe-Taylor, J. (1996). Representation theory and invariant neural networks. Neural computation, 8(5), 1003-1013. Mallat, S. (2016). Understanding deep convolutional networks. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065), 20150203. Lee, J. M. (2013). Introduction to smooth manifolds. Springer Science \u0026amp; Business Media. ","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"","permalink":"http://localhost:1313/posts/geo_deeplearning1/","summary":"","title":"intro to Geo-deep-learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f} \\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$: $$ \\mathcal{R} = \\mathbb{E}_{(x,y) \\sim P}[L(\\widetilde{f}(x), f(x))] $$\nSuch constructed functions can approximate almost any function (Universal Approximation Theorem), but this does not mean we do not need inductive biases to constrain the learning problem. Given a function class $\\mathcal{F}$ with universal approximation capabilities, we can define a complexity function: $$c: \\mathcal{F} \\rightarrow \\mathbb{R}$$ defining our interpolation problem as: $$\\widetilde{f} = \\arg\\min_{g \\in \\mathcal{F}} c(g) \\quad s.t. \\quad \\widetilde{f}(x_i) = f(x_i), \\quad i = 1, . . . , N$$\nTherefore, we not only hope for good function fitting but also low function complexity. Such a complexity function is what we call the norm of the function. The benefit of this definition is that $\\mathcal{F}$ becomes a Banach space (a complete vector space equipped with a norm), allowing us to use tools from functional analysis to study their properties.\nGeometric Priors To overcome the \u0026ldquo;curse of dimensionality\u0026rdquo; of high-dimensional data, we must utilize the physical characteristics of the data (symmetry and scale separation); although the geometry of the data (Domain) may be complex, the signals defined on it constitute a Hilbert space with good mathematical properties, allowing us to still use tools from linear algebra and functional analysis to handle them.\nFor example, in image processing, data is typically represented as signals (pixel values) defined on a two-dimensional Euclidean space $\\mathbb{R}^2$. This space possesses translation and rotation symmetry, meaning that if we translate or rotate the image, its content and structure remain unchanged. Convolutional Neural Networks (CNNs) exploit this translation symmetry, capturing local features through shared weights, thereby improving learning efficiency and generalization ability.\nSymmetry and Invariance In machine learning, symmetry refers to the property that data or tasks remain unchanged under certain transformations. These transformations can be translation, rotation, scaling, etc. For example, in an image classification task, if we translate or rotate an image, the category of the image usually does not change. This invariance is what we hope the model can capture and utilize.\nSymmetry Groups Symmetry can be formalized through group theory. A group is a set equipped with a binary operation that satisfies properties such as closure, associativity, identity, and inverse.\nSymmetry operations form a group, called a symmetry group, proven as follows:\nClosure: If $g_1$ and $g_2$ are symmetry operations, then their composition $g_1 \\circ g_2$ is also a symmetry operation, because applying two symmetry transformations consecutively still preserves the invariance of the data. Associativity: If $g_1$ and $g_2$ are symmetry operations, then for any $g_3$, $(g_1 \\circ g_2) \\circ g_3 = g_1 \\circ (g_2 \\circ g_3)$, because the order of transformations does not affect the final result. Identity: There exists an identity operation $e$ such that for any symmetry operation $g$, $e \\circ g = g \\circ e = g$. This identity operation corresponds to performing no transformation. Inverse: For every symmetry operation $g$, there exists an inverse operation $g^{-1}$ such that $g \\circ g^{-1} = g^{-1} \\circ g = e$. This inverse operation corresponds to undoing the transformation. Note: $g \\circ h$ denotes applying transformation $g$ first, then transformation $h$.\nLet\u0026rsquo;s take an equilateral triangle as an example to illustrate the concept of symmetry groups: In this example, the symmetry group of the equilateral triangle contains six elements: three rotation operations (0 degrees, 120 degrees, 240 degrees) and three reflection operations (perpendicular lines through each vertex). These operations satisfy the four properties of a group, thus forming a group called $D_3$, the dihedral group of the triangle. Mathematically, we can view these operations as mappings: $$g: \\mathcal{X} \\rightarrow \\mathcal{X}$$ where $\\mathcal{X}$ is the data space. For each element $g \\in G$ in a symmetry group $G$, we have a corresponding mapping $g$. For example, $D_3$ can also be represented as a matrix group: $$ D_3 = \\left\\{ \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 2 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 3 \u0026amp; 1 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 1 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 3 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 1 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 2 \u0026amp; 1 \\end{pmatrix} \\right\\} $$\nReferences Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., \u0026amp; Vandergheynst, P. (2017). Geometric deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4), 18-42. Goodfellow, I., Bengio, Y., \u0026amp; Courville, A. (2016). Deep learning. MIT press. Cohen, T. S., \u0026amp; Welling, M. (2016). Group equivariant convolutional networks. In International conference on machine learning (pp. 2990-2999). PMLR. Kondor, R., \u0026amp; Trivedi, S. (2018). On the generalization of equivariance and convolution in neural networks to the action of compact groups. In International conference on machine learning (pp. 2747-2755). PMLR. Wood, T., \u0026amp; Shawe-Taylor, J. (1996). Representation theory and invariant neural networks. Neural computation, 8(5), 1003-1013. Mallat, S. (2016). Understanding deep convolutional networks. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065), 20150203. Lee, J. M. (2013). Introduction to smooth manifolds. Springer Science \u0026amp; Business Media. ","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"","permalink":"http://localhost:1313/posts/geo_deeplearning1/","summary":"","title":"intro to Geo-deep-learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f} \\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$: $$ \\mathcal{R} = \\mathbb{E}_{(x,y) \\sim P}[L(\\widetilde{f}(x), f(x))] $$\nSuch constructed functions can approximate almost any function (Universal Approximation Theorem), but this does not mean we do not need inductive biases to constrain the learning problem. Given a function class $\\mathcal{F}$ with universal approximation capabilities, we can define a complexity function: $$c: \\mathcal{F} \\rightarrow \\mathbb{R}$$ defining our interpolation problem as: $$\\widetilde{f} = \\arg\\min_{g \\in \\mathcal{F}} c(g) \\quad s.t. \\quad \\widetilde{f}(x_i) = f(x_i), \\quad i = 1, . . . , N$$\nTherefore, we not only hope for good function fitting but also low function complexity. Such a complexity function is what we call the norm of the function. The benefit of this definition is that $\\mathcal{F}$ becomes a Banach space (a complete vector space equipped with a norm), allowing us to use tools from functional analysis to study their properties.\nGeometric Priors To overcome the \u0026ldquo;curse of dimensionality\u0026rdquo; of high-dimensional data, we must utilize the physical characteristics of the data (symmetry and scale separation); although the geometry of the data (Domain) may be complex, the signals defined on it constitute a Hilbert space with good mathematical properties, allowing us to still use tools from linear algebra and functional analysis to handle them.\nFor example, in image processing, data is typically represented as signals (pixel values) defined on a two-dimensional Euclidean space $\\mathbb{R}^2$. This space possesses translation and rotation symmetry, meaning that if we translate or rotate the image, its content and structure remain unchanged. Convolutional Neural Networks (CNNs) exploit this translation symmetry, capturing local features through shared weights, thereby improving learning efficiency and generalization ability.\nSymmetry and Invariance In machine learning, symmetry refers to the property that data or tasks remain unchanged under certain transformations. These transformations can be translation, rotation, scaling, etc. For example, in an image classification task, if we translate or rotate an image, the category of the image usually does not change. This invariance is what we hope the model can capture and utilize.\nSymmetry Groups Symmetry can be formalized through group theory. A group is a set equipped with a binary operation that satisfies properties such as closure, associativity, identity, and inverse.\nSymmetry operations form a group, called a symmetry group, proven as follows:\nClosure: If $g_1$ and $g_2$ are symmetry operations, then their composition $g_1 \\circ g_2$ is also a symmetry operation, because applying two symmetry transformations consecutively still preserves the invariance of the data. Associativity: If $g_1$ and $g_2$ are symmetry operations, then for any $g_3$, $(g_1 \\circ g_2) \\circ g_3 = g_1 \\circ (g_2 \\circ g_3)$, because the order of transformations does not affect the final result. Identity: There exists an identity operation $e$ such that for any symmetry operation $g$, $e \\circ g = g \\circ e = g$. This identity operation corresponds to performing no transformation. Inverse: For every symmetry operation $g$, there exists an inverse operation $g^{-1}$ such that $g \\circ g^{-1} = g^{-1} \\circ g = e$. This inverse operation corresponds to undoing the transformation. Note: $g \\circ h$ denotes applying transformation $g$ first, then transformation $h$.\nLet\u0026rsquo;s take an equilateral triangle as an example to illustrate the concept of symmetry groups: In this example, the symmetry group of the equilateral triangle contains six elements: three rotation operations (0 degrees, 120 degrees, 240 degrees) and three reflection operations (perpendicular lines through each vertex). These operations satisfy the four properties of a group, thus forming a group called $D_3$, the dihedral group of the triangle. Mathematically, we can view these operations as mappings: $$g: \\mathcal{X} \\rightarrow \\mathcal{X}$$ where $\\mathcal{X}$ is the data space. For each element $g \\in G$ in a symmetry group $G$, we have a corresponding mapping $g$. For example, $D_3$ can also be represented as a matrix group: $$ D_3 = \\left\\{ \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 2 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 3 \u0026amp; 1 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 1 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 3 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 1 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 2 \u0026amp; 1 \\end{pmatrix} \\right\\} $$\nReferences Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., \u0026amp; Vandergheynst, P. (2017). Geometric deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4), 18-42. Goodfellow, I., Bengio, Y., \u0026amp; Courville, A. (2016). Deep learning. MIT press. Cohen, T. S., \u0026amp; Welling, M. (2016). Group equivariant convolutional networks. In International conference on machine learning (pp. 2990-2999). PMLR. Kondor, R., \u0026amp; Trivedi, S. (2018). On the generalization of equivariance and convolution in neural networks to the action of compact groups. In International conference on machine learning (pp. 2747-2755). PMLR. Wood, T., \u0026amp; Shawe-Taylor, J. (1996). Representation theory and invariant neural networks. Neural computation, 8(5), 1003-1013. Mallat, S. (2016). Understanding deep convolutional networks. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065), 20150203. Lee, J. M. (2013). Introduction to smooth manifolds. Springer Science \u0026amp; Business Media. ","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"","permalink":"http://localhost:1313/posts/geo_deeplearning1/","summary":"","title":"intro to Geo-deep-learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f} \\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$: $$ \\mathcal{R} = \\mathbb{E}_{(x,y) \\sim P}[L(\\widetilde{f}(x), f(x))] $$\nSuch constructed functions can approximate almost any function (Universal Approximation Theorem), but this does not mean we do not need inductive biases to constrain the learning problem. Given a function class $\\mathcal{F}$ with universal approximation capabilities, we can define a complexity function: $$c: \\mathcal{F} \\rightarrow \\mathbb{R}$$ defining our interpolation problem as: $$\\widetilde{f} = \\arg\\min_{g \\in \\mathcal{F}} c(g) \\quad s.t. \\quad \\widetilde{f}(x_i) = f(x_i), \\quad i = 1, . . . , N$$\nTherefore, we not only hope for good function fitting but also low function complexity. Such a complexity function is what we call the norm of the function. The benefit of this definition is that $\\mathcal{F}$ becomes a Banach space (a complete vector space equipped with a norm), allowing us to use tools from functional analysis to study their properties.\nGeometric Priors To overcome the \u0026ldquo;curse of dimensionality\u0026rdquo; of high-dimensional data, we must utilize the physical characteristics of the data (symmetry and scale separation); although the geometry of the data (Domain) may be complex, the signals defined on it constitute a Hilbert space with good mathematical properties, allowing us to still use tools from linear algebra and functional analysis to handle them.\nFor example, in image processing, data is typically represented as signals (pixel values) defined on a two-dimensional Euclidean space $\\mathbb{R}^2$. This space possesses translation and rotation symmetry, meaning that if we translate or rotate the image, its content and structure remain unchanged. Convolutional Neural Networks (CNNs) exploit this translation symmetry, capturing local features through shared weights, thereby improving learning efficiency and generalization ability.\nSymmetry and Invariance In machine learning, symmetry refers to the property that data or tasks remain unchanged under certain transformations. These transformations can be translation, rotation, scaling, etc. For example, in an image classification task, if we translate or rotate an image, the category of the image usually does not change. This invariance is what we hope the model can capture and utilize.\nSymmetry Groups Symmetry can be formalized through group theory. A group is a set equipped with a binary operation that satisfies properties such as closure, associativity, identity, and inverse.\nSymmetry operations form a group, called a symmetry group, proven as follows:\nClosure: If $g_1$ and $g_2$ are symmetry operations, then their composition $g_1 \\circ g_2$ is also a symmetry operation, because applying two symmetry transformations consecutively still preserves the invariance of the data. Associativity: If $g_1$ and $g_2$ are symmetry operations, then for any $g_3$, $(g_1 \\circ g_2) \\circ g_3 = g_1 \\circ (g_2 \\circ g_3)$, because the order of transformations does not affect the final result. Identity: There exists an identity operation $e$ such that for any symmetry operation $g$, $e \\circ g = g \\circ e = g$. This identity operation corresponds to performing no transformation. Inverse: For every symmetry operation $g$, there exists an inverse operation $g^{-1}$ such that $g \\circ g^{-1} = g^{-1} \\circ g = e$. This inverse operation corresponds to undoing the transformation. Note: $g \\circ h$ denotes applying transformation $g$ first, then transformation $h$.\nLet\u0026rsquo;s take an equilateral triangle as an example to illustrate the concept of symmetry groups: In this example, the symmetry group of the equilateral triangle contains six elements: three rotation operations (0 degrees, 120 degrees, 240 degrees) and three reflection operations (perpendicular lines through each vertex). These operations satisfy the four properties of a group, thus forming a group called $D_3$, the dihedral group of the triangle. Mathematically, we can view these operations as mappings: $$g: \\mathcal{X} \\rightarrow \\mathcal{X}$$ where $\\mathcal{X}$ is the data space. For each element $g \\in G$ in a symmetry group $G$, we have a corresponding mapping $g$. For example, $D_3$ can also be represented as a matrix group: $$ D_3 = \\left\\{ \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 2 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 3 \u0026amp; 1 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 1 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 3 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 1 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 2 \u0026amp; 1 \\end{pmatrix} \\right\\} $$\nReferences Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., \u0026amp; Vandergheynst, P. (2017). Geometric deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4), 18-42. Goodfellow, I., Bengio, Y., \u0026amp; Courville, A. (2016). Deep learning. MIT press. Cohen, T. S., \u0026amp; Welling, M. (2016). Group equivariant convolutional networks. In International conference on machine learning (pp. 2990-2999). PMLR. Kondor, R., \u0026amp; Trivedi, S. (2018). On the generalization of equivariance and convolution in neural networks to the action of compact groups. In International conference on machine learning (pp. 2747-2755). PMLR. Wood, T., \u0026amp; Shawe-Taylor, J. (1996). Representation theory and invariant neural networks. Neural computation, 8(5), 1003-1013. Mallat, S. (2016). Understanding deep convolutional networks. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065), 20150203. Lee, J. M. (2013). Introduction to smooth manifolds. Springer Science \u0026amp; Business Media. ","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"","permalink":"http://localhost:1313/posts/geo_deeplearning1/","summary":"","title":"intro to Geo-deep-learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f} \\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$: $$ \\mathcal{R} = \\mathbb{E}_{(x,y) \\sim P}[L(\\widetilde{f}(x), f(x))] $$\nSuch constructed functions can approximate almost any function (Universal Approximation Theorem), but this does not mean we do not need inductive biases to constrain the learning problem. Given a function class $\\mathcal{F}$ with universal approximation capabilities, we can define a complexity function: $$c: \\mathcal{F} \\rightarrow \\mathbb{R}$$ defining our interpolation problem as: $$\\widetilde{f} = \\arg\\min_{g \\in \\mathcal{F}} c(g) \\quad s.t. \\quad \\widetilde{f}(x_i) = f(x_i), \\quad i = 1, . . . , N$$\nTherefore, we not only hope for good function fitting but also low function complexity. Such a complexity function is what we call the norm of the function. The benefit of this definition is that $\\mathcal{F}$ becomes a Banach space (a complete vector space equipped with a norm), allowing us to use tools from functional analysis to study their properties.\nGeometric Priors To overcome the \u0026ldquo;curse of dimensionality\u0026rdquo; of high-dimensional data, we must utilize the physical characteristics of the data (symmetry and scale separation); although the geometry of the data (Domain) may be complex, the signals defined on it constitute a Hilbert space with good mathematical properties, allowing us to still use tools from linear algebra and functional analysis to handle them.\nFor example, in image processing, data is typically represented as signals (pixel values) defined on a two-dimensional Euclidean space $\\mathbb{R}^2$. This space possesses translation and rotation symmetry, meaning that if we translate or rotate the image, its content and structure remain unchanged. Convolutional Neural Networks (CNNs) exploit this translation symmetry, capturing local features through shared weights, thereby improving learning efficiency and generalization ability.\nSymmetry and Invariance In machine learning, symmetry refers to the property that data or tasks remain unchanged under certain transformations. These transformations can be translation, rotation, scaling, etc. For example, in an image classification task, if we translate or rotate an image, the category of the image usually does not change. This invariance is what we hope the model can capture and utilize.\nSymmetry Groups Symmetry can be formalized through group theory. A group is a set equipped with a binary operation that satisfies properties such as closure, associativity, identity, and inverse.\nSymmetry operations form a group, called a symmetry group, proven as follows:\nClosure: If $g_1$ and $g_2$ are symmetry operations, then their composition $g_1 \\circ g_2$ is also a symmetry operation, because applying two symmetry transformations consecutively still preserves the invariance of the data. Associativity: If $g_1$ and $g_2$ are symmetry operations, then for any $g_3$, $(g_1 \\circ g_2) \\circ g_3 = g_1 \\circ (g_2 \\circ g_3)$, because the order of transformations does not affect the final result. Identity: There exists an identity operation $e$ such that for any symmetry operation $g$, $e \\circ g = g \\circ e = g$. This identity operation corresponds to performing no transformation. Inverse: For every symmetry operation $g$, there exists an inverse operation $g^{-1}$ such that $g \\circ g^{-1} = g^{-1} \\circ g = e$. This inverse operation corresponds to undoing the transformation. Note: $g \\circ h$ denotes applying transformation $g$ first, then transformation $h$.\nLet\u0026rsquo;s take an equilateral triangle as an example to illustrate the concept of symmetry groups: In this example, the symmetry group of the equilateral triangle contains six elements: three rotation operations (0 degrees, 120 degrees, 240 degrees) and three reflection operations (perpendicular lines through each vertex). These operations satisfy the four properties of a group, thus forming a group called $D_3$, the dihedral group of the triangle. Mathematically, we can view these operations as mappings: $$g: \\mathcal{X} \\rightarrow \\mathcal{X}$$ where $\\mathcal{X}$ is the data space. For each element $g \\in G$ in a symmetry group $G$, we have a corresponding mapping $g$. For example, $D_3$ can also be represented as a matrix group: $$ D_3 = \\left\\{ \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 2 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 3 \u0026amp; 1 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 1 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 3 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 1 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 2 \u0026amp; 1 \\end{pmatrix} \\right\\} $$\nReferences Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., \u0026amp; Vandergheynst, P. (2017). Geometric deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4), 18-42. Goodfellow, I., Bengio, Y., \u0026amp; Courville, A. (2016). Deep learning. MIT press. Cohen, T. S., \u0026amp; Welling, M. (2016). Group equivariant convolutional networks. In International conference on machine learning (pp. 2990-2999). PMLR. Kondor, R., \u0026amp; Trivedi, S. (2018). On the generalization of equivariance and convolution in neural networks to the action of compact groups. In International conference on machine learning (pp. 2747-2755). PMLR. Wood, T., \u0026amp; Shawe-Taylor, J. (1996). Representation theory and invariant neural networks. Neural computation, 8(5), 1003-1013. Mallat, S. (2016). Understanding deep convolutional networks. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065), 20150203. Lee, J. M. (2013). Introduction to smooth manifolds. Springer Science \u0026amp; Business Media. ","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"","permalink":"http://localhost:1313/posts/geo_deeplearning1/","summary":"","title":"intro to Geo-deep-learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f} \\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$: $$ \\mathcal{R} = \\mathbb{E}_{(x,y) \\sim P}[L(\\widetilde{f}(x), f(x))] $$\nSuch constructed functions can approximate almost any function (Universal Approximation Theorem), but this does not mean we do not need inductive biases to constrain the learning problem. Given a function class $\\mathcal{F}$ with universal approximation capabilities, we can define a complexity function: $$c: \\mathcal{F} \\rightarrow \\mathbb{R}$$ defining our interpolation problem as: $$\\widetilde{f} = \\arg\\min_{g \\in \\mathcal{F}} c(g) \\quad s.t. \\quad \\widetilde{f}(x_i) = f(x_i), \\quad i = 1, . . . , N$$\nTherefore, we not only hope for good function fitting but also low function complexity. Such a complexity function is what we call the norm of the function. The benefit of this definition is that $\\mathcal{F}$ becomes a Banach space (a complete vector space equipped with a norm), allowing us to use tools from functional analysis to study their properties.\nGeometric Priors To overcome the \u0026ldquo;curse of dimensionality\u0026rdquo; of high-dimensional data, we must utilize the physical characteristics of the data (symmetry and scale separation); although the geometry of the data (Domain) may be complex, the signals defined on it constitute a Hilbert space with good mathematical properties, allowing us to still use tools from linear algebra and functional analysis to handle them.\nFor example, in image processing, data is typically represented as signals (pixel values) defined on a two-dimensional Euclidean space $\\mathbb{R}^2$. This space possesses translation and rotation symmetry, meaning that if we translate or rotate the image, its content and structure remain unchanged. Convolutional Neural Networks (CNNs) exploit this translation symmetry, capturing local features through shared weights, thereby improving learning efficiency and generalization ability.\nSymmetry and Invariance In machine learning, symmetry refers to the property that data or tasks remain unchanged under certain transformations. These transformations can be translation, rotation, scaling, etc. For example, in an image classification task, if we translate or rotate an image, the category of the image usually does not change. This invariance is what we hope the model can capture and utilize.\nSymmetry Groups Symmetry can be formalized through group theory. A group is a set equipped with a binary operation that satisfies properties such as closure, associativity, identity, and inverse.\nSymmetry operations form a group, called a symmetry group, proven as follows:\nClosure: If $g_1$ and $g_2$ are symmetry operations, then their composition $g_1 \\circ g_2$ is also a symmetry operation, because applying two symmetry transformations consecutively still preserves the invariance of the data. Associativity: If $g_1$ and $g_2$ are symmetry operations, then for any $g_3$, $(g_1 \\circ g_2) \\circ g_3 = g_1 \\circ (g_2 \\circ g_3)$, because the order of transformations does not affect the final result. Identity: There exists an identity operation $e$ such that for any symmetry operation $g$, $e \\circ g = g \\circ e = g$. This identity operation corresponds to performing no transformation. Inverse: For every symmetry operation $g$, there exists an inverse operation $g^{-1}$ such that $g \\circ g^{-1} = g^{-1} \\circ g = e$. This inverse operation corresponds to undoing the transformation. Note: $g \\circ h$ denotes applying transformation $g$ first, then transformation $h$.\nLet\u0026rsquo;s take an equilateral triangle as an example to illustrate the concept of symmetry groups: In this example, the symmetry group of the equilateral triangle contains six elements: three rotation operations (0 degrees, 120 degrees, 240 degrees) and three reflection operations (perpendicular lines through each vertex). These operations satisfy the four properties of a group, thus forming a group called $D_3$, the dihedral group of the triangle. Mathematically, we can view these operations as mappings: $$g: \\mathcal{X} \\rightarrow \\mathcal{X}$$ where $\\mathcal{X}$ is the data space. For each element $g \\in G$ in a symmetry group $G$, we have a corresponding mapping $g$. For example, $D_3$ can also be represented as a matrix group: $$ D_3 = \\left\\{ \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 2 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 3 \u0026amp; 1 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 1 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 3 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 1 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 2 \u0026amp; 1 \\end{pmatrix} \\right\\} $$\nReferences Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., \u0026amp; Vandergheynst, P. (2017). Geometric deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4), 18-42. Goodfellow, I., Bengio, Y., \u0026amp; Courville, A. (2016). Deep learning. MIT press. Cohen, T. S., \u0026amp; Welling, M. (2016). Group equivariant convolutional networks. In International conference on machine learning (pp. 2990-2999). PMLR. Kondor, R., \u0026amp; Trivedi, S. (2018). On the generalization of equivariance and convolution in neural networks to the action of compact groups. In International conference on machine learning (pp. 2747-2755). PMLR. Wood, T., \u0026amp; Shawe-Taylor, J. (1996). Representation theory and invariant neural networks. Neural computation, 8(5), 1003-1013. Mallat, S. (2016). Understanding deep convolutional networks. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065), 20150203. Lee, J. M. (2013). Introduction to smooth manifolds. Springer Science \u0026amp; Business Media. ","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"","permalink":"http://localhost:1313/posts/geo_deeplearning1/","summary":"","title":"intro to Geo-deep-learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f} \\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$: $$ \\mathcal{R} = \\mathbb{E}_{(x,y) \\sim P}[L(\\widetilde{f}(x), f(x))] $$\nSuch constructed functions can approximate almost any function (Universal Approximation Theorem), but this does not mean we do not need inductive biases to constrain the learning problem. Given a function class $\\mathcal{F}$ with universal approximation capabilities, we can define a complexity function: $$c: \\mathcal{F} \\rightarrow \\mathbb{R}$$ defining our interpolation problem as: $$\\widetilde{f} = \\arg\\min_{g \\in \\mathcal{F}} c(g) \\quad s.t. \\quad \\widetilde{f}(x_i) = f(x_i), \\quad i = 1, . . . , N$$\nTherefore, we not only hope for good function fitting but also low function complexity. Such a complexity function is what we call the norm of the function. The benefit of this definition is that $\\mathcal{F}$ becomes a Banach space (a complete vector space equipped with a norm), allowing us to use tools from functional analysis to study their properties.\nGeometric Priors To overcome the \u0026ldquo;curse of dimensionality\u0026rdquo; of high-dimensional data, we must utilize the physical characteristics of the data (symmetry and scale separation); although the geometry of the data (Domain) may be complex, the signals defined on it constitute a Hilbert space with good mathematical properties, allowing us to still use tools from linear algebra and functional analysis to handle them.\nFor example, in image processing, data is typically represented as signals (pixel values) defined on a two-dimensional Euclidean space $\\mathbb{R}^2$. This space possesses translation and rotation symmetry, meaning that if we translate or rotate the image, its content and structure remain unchanged. Convolutional Neural Networks (CNNs) exploit this translation symmetry, capturing local features through shared weights, thereby improving learning efficiency and generalization ability.\nSymmetry and Invariance In machine learning, symmetry refers to the property that data or tasks remain unchanged under certain transformations. These transformations can be translation, rotation, scaling, etc. For example, in an image classification task, if we translate or rotate an image, the category of the image usually does not change. This invariance is what we hope the model can capture and utilize.\nSymmetry Groups Symmetry can be formalized through group theory. A group is a set equipped with a binary operation that satisfies properties such as closure, associativity, identity, and inverse.\nSymmetry operations form a group, called a symmetry group, proven as follows:\nClosure: If $g_1$ and $g_2$ are symmetry operations, then their composition $g_1 \\circ g_2$ is also a symmetry operation, because applying two symmetry transformations consecutively still preserves the invariance of the data. Associativity: If $g_1$ and $g_2$ are symmetry operations, then for any $g_3$, $(g_1 \\circ g_2) \\circ g_3 = g_1 \\circ (g_2 \\circ g_3)$, because the order of transformations does not affect the final result. Identity: There exists an identity operation $e$ such that for any symmetry operation $g$, $e \\circ g = g \\circ e = g$. This identity operation corresponds to performing no transformation. Inverse: For every symmetry operation $g$, there exists an inverse operation $g^{-1}$ such that $g \\circ g^{-1} = g^{-1} \\circ g = e$. This inverse operation corresponds to undoing the transformation. Note: $g \\circ h$ denotes applying transformation $g$ first, then transformation $h$.\nLet\u0026rsquo;s take an equilateral triangle as an example to illustrate the concept of symmetry groups: In this example, the symmetry group of the equilateral triangle contains six elements: three rotation operations (0 degrees, 120 degrees, 240 degrees) and three reflection operations (perpendicular lines through each vertex). These operations satisfy the four properties of a group, thus forming a group called $D_3$, the dihedral group of the triangle. Mathematically, we can view these operations as mappings: $$g: \\mathcal{X} \\rightarrow \\mathcal{X}$$ where $\\mathcal{X}$ is the data space. For each element $g \\in G$ in a symmetry group $G$, we have a corresponding mapping $g$. For example, $D_3$ can also be represented as a matrix group: $$ D_3 = \\left\\{ \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 2 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 3 \u0026amp; 1 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 1 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 3 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 1 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 2 \u0026amp; 1 \\end{pmatrix} \\right\\} $$\nReferences Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., \u0026amp; Vandergheynst, P. (2017). Geometric deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4), 18-42. Goodfellow, I., Bengio, Y., \u0026amp; Courville, A. (2016). Deep learning. MIT press. Cohen, T. S., \u0026amp; Welling, M. (2016). Group equivariant convolutional networks. In International conference on machine learning (pp. 2990-2999). PMLR. Kondor, R., \u0026amp; Trivedi, S. (2018). On the generalization of equivariance and convolution in neural networks to the action of compact groups. In International conference on machine learning (pp. 2747-2755). PMLR. Wood, T., \u0026amp; Shawe-Taylor, J. (1996). Representation theory and invariant neural networks. Neural computation, 8(5), 1003-1013. Mallat, S. (2016). Understanding deep convolutional networks. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065), 20150203. Lee, J. M. (2013). Introduction to smooth manifolds. Springer Science \u0026amp; Business Media. ","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"","permalink":"http://localhost:1313/posts/geo_deeplearning1/","summary":"","title":"intro to Geo-deep-learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f} \\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$: $$ \\mathcal{R} = \\mathbb{E}_{(x,y) \\sim P}[L(\\widetilde{f}(x), f(x))] $$\nSuch constructed functions can approximate almost any function (Universal Approximation Theorem), but this does not mean we do not need inductive biases to constrain the learning problem. Given a function class $\\mathcal{F}$ with universal approximation capabilities, we can define a complexity function: $$c: \\mathcal{F} \\rightarrow \\mathbb{R}$$ defining our interpolation problem as: $$\\widetilde{f} = \\arg\\min_{g \\in \\mathcal{F}} c(g) \\quad s.t. \\quad \\widetilde{f}(x_i) = f(x_i), \\quad i = 1, . . . , N$$\nTherefore, we not only hope for good function fitting but also low function complexity. Such a complexity function is what we call the norm of the function. The benefit of this definition is that $\\mathcal{F}$ becomes a Banach space (a complete vector space equipped with a norm), allowing us to use tools from functional analysis to study their properties.\nGeometric Priors To overcome the \u0026ldquo;curse of dimensionality\u0026rdquo; of high-dimensional data, we must utilize the physical characteristics of the data (symmetry and scale separation); although the geometry of the data (Domain) may be complex, the signals defined on it constitute a Hilbert space with good mathematical properties, allowing us to still use tools from linear algebra and functional analysis to handle them.\nFor example, in image processing, data is typically represented as signals (pixel values) defined on a two-dimensional Euclidean space $\\mathbb{R}^2$. This space possesses translation and rotation symmetry, meaning that if we translate or rotate the image, its content and structure remain unchanged. Convolutional Neural Networks (CNNs) exploit this translation symmetry, capturing local features through shared weights, thereby improving learning efficiency and generalization ability.\nSymmetry and Invariance In machine learning, symmetry refers to the property that data or tasks remain unchanged under certain transformations. These transformations can be translation, rotation, scaling, etc. For example, in an image classification task, if we translate or rotate an image, the category of the image usually does not change. This invariance is what we hope the model can capture and utilize.\nSymmetry Groups Symmetry can be formalized through group theory. A group is a set equipped with a binary operation that satisfies properties such as closure, associativity, identity, and inverse.\nSymmetry operations form a group, called a symmetry group, proven as follows:\nClosure: If $g_1$ and $g_2$ are symmetry operations, then their composition $g_1 \\circ g_2$ is also a symmetry operation, because applying two symmetry transformations consecutively still preserves the invariance of the data. Associativity: If $g_1$ and $g_2$ are symmetry operations, then for any $g_3$, $(g_1 \\circ g_2) \\circ g_3 = g_1 \\circ (g_2 \\circ g_3)$, because the order of transformations does not affect the final result. Identity: There exists an identity operation $e$ such that for any symmetry operation $g$, $e \\circ g = g \\circ e = g$. This identity operation corresponds to performing no transformation. Inverse: For every symmetry operation $g$, there exists an inverse operation $g^{-1}$ such that $g \\circ g^{-1} = g^{-1} \\circ g = e$. This inverse operation corresponds to undoing the transformation. Note: $g \\circ h$ denotes applying transformation $g$ first, then transformation $h$.\nLet\u0026rsquo;s take an equilateral triangle as an example to illustrate the concept of symmetry groups: In this example, the symmetry group of the equilateral triangle contains six elements: three rotation operations (0 degrees, 120 degrees, 240 degrees) and three reflection operations (perpendicular lines through each vertex). These operations satisfy the four properties of a group, thus forming a group called $D_3$, the dihedral group of the triangle. Mathematically, we can view these operations as mappings: $$g: \\mathcal{X} \\rightarrow \\mathcal{X}$$ where $\\mathcal{X}$ is the data space. For each element $g \\in G$ in a symmetry group $G$, we have a corresponding mapping $g$. For example, $D_3$ can also be represented as a matrix group: $$ D_3 = \\left\\{ \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 2 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 3 \u0026amp; 1 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 1 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 3 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 1 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 2 \u0026amp; 1 \\end{pmatrix} \\right\\} $$\nReferences Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., \u0026amp; Vandergheynst, P. (2017). Geometric deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4), 18-42. Goodfellow, I., Bengio, Y., \u0026amp; Courville, A. (2016). Deep learning. MIT press. Cohen, T. S., \u0026amp; Welling, M. (2016). Group equivariant convolutional networks. In International conference on machine learning (pp. 2990-2999). PMLR. Kondor, R., \u0026amp; Trivedi, S. (2018). On the generalization of equivariance and convolution in neural networks to the action of compact groups. In International conference on machine learning (pp. 2747-2755). PMLR. Wood, T., \u0026amp; Shawe-Taylor, J. (1996). Representation theory and invariant neural networks. Neural computation, 8(5), 1003-1013. Mallat, S. (2016). Understanding deep convolutional networks. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065), 20150203. Lee, J. M. (2013). Introduction to smooth manifolds. Springer Science \u0026amp; Business Media. ","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"","permalink":"http://localhost:1313/posts/geo_deeplearning1/","summary":"","title":"intro to Geo-deep-learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f} \\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$: $$ \\mathcal{R} = \\mathbb{E}_{(x,y) \\sim P}[L(\\widetilde{f}(x), f(x))] $$\nSuch constructed functions can approximate almost any function (Universal Approximation Theorem), but this does not mean we do not need inductive biases to constrain the learning problem. Given a function class $\\mathcal{F}$ with universal approximation capabilities, we can define a complexity function: $$c: \\mathcal{F} \\rightarrow \\mathbb{R}$$ defining our interpolation problem as: $$\\widetilde{f} = \\arg\\min_{g \\in \\mathcal{F}} c(g) \\quad s.t. \\quad \\widetilde{f}(x_i) = f(x_i), \\quad i = 1, . . . , N$$\nTherefore, we not only hope for good function fitting but also low function complexity. Such a complexity function is what we call the norm of the function. The benefit of this definition is that $\\mathcal{F}$ becomes a Banach space (a complete vector space equipped with a norm), allowing us to use tools from functional analysis to study their properties.\nGeometric Priors To overcome the \u0026ldquo;curse of dimensionality\u0026rdquo; of high-dimensional data, we must utilize the physical characteristics of the data (symmetry and scale separation); although the geometry of the data (Domain) may be complex, the signals defined on it constitute a Hilbert space with good mathematical properties, allowing us to still use tools from linear algebra and functional analysis to handle them.\nFor example, in image processing, data is typically represented as signals (pixel values) defined on a two-dimensional Euclidean space $\\mathbb{R}^2$. This space possesses translation and rotation symmetry, meaning that if we translate or rotate the image, its content and structure remain unchanged. Convolutional Neural Networks (CNNs) exploit this translation symmetry, capturing local features through shared weights, thereby improving learning efficiency and generalization ability.\nSymmetry and Invariance In machine learning, symmetry refers to the property that data or tasks remain unchanged under certain transformations. These transformations can be translation, rotation, scaling, etc. For example, in an image classification task, if we translate or rotate an image, the category of the image usually does not change. This invariance is what we hope the model can capture and utilize.\nSymmetry Groups Symmetry can be formalized through group theory. A group is a set equipped with a binary operation that satisfies properties such as closure, associativity, identity, and inverse.\nSymmetry operations form a group, called a symmetry group, proven as follows:\nClosure: If $g_1$ and $g_2$ are symmetry operations, then their composition $g_1 \\circ g_2$ is also a symmetry operation, because applying two symmetry transformations consecutively still preserves the invariance of the data. Associativity: If $g_1$ and $g_2$ are symmetry operations, then for any $g_3$, $(g_1 \\circ g_2) \\circ g_3 = g_1 \\circ (g_2 \\circ g_3)$, because the order of transformations does not affect the final result. Identity: There exists an identity operation $e$ such that for any symmetry operation $g$, $e \\circ g = g \\circ e = g$. This identity operation corresponds to performing no transformation. Inverse: For every symmetry operation $g$, there exists an inverse operation $g^{-1}$ such that $g \\circ g^{-1} = g^{-1} \\circ g = e$. This inverse operation corresponds to undoing the transformation. Note: $g \\circ h$ denotes applying transformation $g$ first, then transformation $h$.\nLet\u0026rsquo;s take an equilateral triangle as an example to illustrate the concept of symmetry groups: In this example, the symmetry group of the equilateral triangle contains six elements: three rotation operations (0 degrees, 120 degrees, 240 degrees) and three reflection operations (perpendicular lines through each vertex). These operations satisfy the four properties of a group, thus forming a group called $D_3$, the dihedral group of the triangle. Mathematically, we can view these operations as mappings: $$g: \\mathcal{X} \\rightarrow \\mathcal{X}$$ where $\\mathcal{X}$ is the data space. For each element $g \\in G$ in a symmetry group $G$, we have a corresponding mapping $g$. For example, $D_3$ can also be represented as a matrix group: $$ D_3 = \\left\\{ \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 2 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 3 \u0026amp; 1 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 1 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 3 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 1 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 2 \u0026amp; 1 \\end{pmatrix} \\right\\} $$\nReferences Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., \u0026amp; Vandergheynst, P. (2017). Geometric deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4), 18-42. Goodfellow, I., Bengio, Y., \u0026amp; Courville, A. (2016). Deep learning. MIT press. Cohen, T. S., \u0026amp; Welling, M. (2016). Group equivariant convolutional networks. In International conference on machine learning (pp. 2990-2999). PMLR. Kondor, R., \u0026amp; Trivedi, S. (2018). On the generalization of equivariance and convolution in neural networks to the action of compact groups. In International conference on machine learning (pp. 2747-2755). PMLR. Wood, T., \u0026amp; Shawe-Taylor, J. (1996). Representation theory and invariant neural networks. Neural computation, 8(5), 1003-1013. Mallat, S. (2016). Understanding deep convolutional networks. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065), 20150203. Lee, J. M. (2013). Introduction to smooth manifolds. Springer Science \u0026amp; Business Media. ","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"","permalink":"http://localhost:1313/posts/geo_deeplearning1/","summary":"","title":"intro to Geo-deep-learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f} \\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$: $$ \\mathcal{R} = \\mathbb{E}_{(x,y) \\sim P}[L(\\widetilde{f}(x), f(x))] $$\nSuch constructed functions can approximate almost any function (Universal Approximation Theorem), but this does not mean we do not need inductive biases to constrain the learning problem. Given a function class $\\mathcal{F}$ with universal approximation capabilities, we can define a complexity function: $$c: \\mathcal{F} \\rightarrow \\mathbb{R}$$ defining our interpolation problem as: $$\\widetilde{f} = \\arg\\min_{g \\in \\mathcal{F}} c(g) \\quad s.t. \\quad \\widetilde{f}(x_i) = f(x_i), \\quad i = 1, . . . , N$$\nTherefore, we not only hope for good function fitting but also low function complexity. Such a complexity function is what we call the norm of the function. The benefit of this definition is that $\\mathcal{F}$ becomes a Banach space (a complete vector space equipped with a norm), allowing us to use tools from functional analysis to study their properties.\nGeometric Priors To overcome the \u0026ldquo;curse of dimensionality\u0026rdquo; of high-dimensional data, we must utilize the physical characteristics of the data (symmetry and scale separation); although the geometry of the data (Domain) may be complex, the signals defined on it constitute a Hilbert space with good mathematical properties, allowing us to still use tools from linear algebra and functional analysis to handle them.\nFor example, in image processing, data is typically represented as signals (pixel values) defined on a two-dimensional Euclidean space $\\mathbb{R}^2$. This space possesses translation and rotation symmetry, meaning that if we translate or rotate the image, its content and structure remain unchanged. Convolutional Neural Networks (CNNs) exploit this translation symmetry, capturing local features through shared weights, thereby improving learning efficiency and generalization ability.\nSymmetry and Invariance In machine learning, symmetry refers to the property that data or tasks remain unchanged under certain transformations. These transformations can be translation, rotation, scaling, etc. For example, in an image classification task, if we translate or rotate an image, the category of the image usually does not change. This invariance is what we hope the model can capture and utilize.\nSymmetry Groups Symmetry can be formalized through group theory. A group is a set equipped with a binary operation that satisfies properties such as closure, associativity, identity, and inverse.\nSymmetry operations form a group, called a symmetry group, proven as follows:\nClosure: If $g_1$ and $g_2$ are symmetry operations, then their composition $g_1 \\circ g_2$ is also a symmetry operation, because applying two symmetry transformations consecutively still preserves the invariance of the data. Associativity: If $g_1$ and $g_2$ are symmetry operations, then for any $g_3$, $(g_1 \\circ g_2) \\circ g_3 = g_1 \\circ (g_2 \\circ g_3)$, because the order of transformations does not affect the final result. Identity: There exists an identity operation $e$ such that for any symmetry operation $g$, $e \\circ g = g \\circ e = g$. This identity operation corresponds to performing no transformation. Inverse: For every symmetry operation $g$, there exists an inverse operation $g^{-1}$ such that $g \\circ g^{-1} = g^{-1} \\circ g = e$. This inverse operation corresponds to undoing the transformation. Note: $g \\circ h$ denotes applying transformation $g$ first, then transformation $h$.\nLet\u0026rsquo;s take an equilateral triangle as an example to illustrate the concept of symmetry groups: In this example, the symmetry group of the equilateral triangle contains six elements: three rotation operations (0 degrees, 120 degrees, 240 degrees) and three reflection operations (perpendicular lines through each vertex). These operations satisfy the four properties of a group, thus forming a group called $D_3$, the dihedral group of the triangle. Mathematically, we can view these operations as mappings: $$g: \\mathcal{X} \\rightarrow \\mathcal{X}$$ where $\\mathcal{X}$ is the data space. For each element $g \\in G$ in a symmetry group $G$, we have a corresponding mapping $g$. For example, $D_3$ can also be represented as a matrix group: $$ D_3 = \\left\\{ \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 2 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 3 \u0026amp; 1 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 1 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 3 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 1 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 2 \u0026amp; 1 \\end{pmatrix} \\right\\} $$\nReferences Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., \u0026amp; Vandergheynst, P. (2017). Geometric deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4), 18-42. Goodfellow, I., Bengio, Y., \u0026amp; Courville, A. (2016). Deep learning. MIT press. Cohen, T. S., \u0026amp; Welling, M. (2016). Group equivariant convolutional networks. In International conference on machine learning (pp. 2990-2999). PMLR. Kondor, R., \u0026amp; Trivedi, S. (2018). On the generalization of equivariance and convolution in neural networks to the action of compact groups. In International conference on machine learning (pp. 2747-2755). PMLR. Wood, T., \u0026amp; Shawe-Taylor, J. (1996). Representation theory and invariant neural networks. Neural computation, 8(5), 1003-1013. Mallat, S. (2016). Understanding deep convolutional networks. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065), 20150203. Lee, J. M. (2013). Introduction to smooth manifolds. Springer Science \u0026amp; Business Media. ","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"","permalink":"http://localhost:1313/posts/geo_deeplearning1/","summary":"","title":"intro to Geo-deep-learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f} \\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$: $$ \\mathcal{R} = \\mathbb{E}_{(x,y) \\sim P}[L(\\widetilde{f}(x), f(x))] $$\nSuch constructed functions can approximate almost any function (Universal Approximation Theorem), but this does not mean we do not need inductive biases to constrain the learning problem. Given a function class $\\mathcal{F}$ with universal approximation capabilities, we can define a complexity function: $$c: \\mathcal{F} \\rightarrow \\mathbb{R}$$ defining our interpolation problem as: $$\\widetilde{f} = \\arg\\min_{g \\in \\mathcal{F}} c(g) \\quad s.t. \\quad \\widetilde{f}(x_i) = f(x_i), \\quad i = 1, . . . , N$$\nTherefore, we not only hope for good function fitting but also low function complexity. Such a complexity function is what we call the norm of the function. The benefit of this definition is that $\\mathcal{F}$ becomes a Banach space (a complete vector space equipped with a norm), allowing us to use tools from functional analysis to study their properties.\nGeometric Priors To overcome the \u0026ldquo;curse of dimensionality\u0026rdquo; of high-dimensional data, we must utilize the physical characteristics of the data (symmetry and scale separation); although the geometry of the data (Domain) may be complex, the signals defined on it constitute a Hilbert space with good mathematical properties, allowing us to still use tools from linear algebra and functional analysis to handle them.\nFor example, in image processing, data is typically represented as signals (pixel values) defined on a two-dimensional Euclidean space $\\mathbb{R}^2$. This space possesses translation and rotation symmetry, meaning that if we translate or rotate the image, its content and structure remain unchanged. Convolutional Neural Networks (CNNs) exploit this translation symmetry, capturing local features through shared weights, thereby improving learning efficiency and generalization ability.\nSymmetry and Invariance In machine learning, symmetry refers to the property that data or tasks remain unchanged under certain transformations. These transformations can be translation, rotation, scaling, etc. For example, in an image classification task, if we translate or rotate an image, the category of the image usually does not change. This invariance is what we hope the model can capture and utilize.\nSymmetry Groups Symmetry can be formalized through group theory. A group is a set equipped with a binary operation that satisfies properties such as closure, associativity, identity, and inverse.\nSymmetry operations form a group, called a symmetry group, proven as follows:\nClosure: If $g_1$ and $g_2$ are symmetry operations, then their composition $g_1 \\circ g_2$ is also a symmetry operation, because applying two symmetry transformations consecutively still preserves the invariance of the data. Associativity: If $g_1$ and $g_2$ are symmetry operations, then for any $g_3$, $(g_1 \\circ g_2) \\circ g_3 = g_1 \\circ (g_2 \\circ g_3)$, because the order of transformations does not affect the final result. Identity: There exists an identity operation $e$ such that for any symmetry operation $g$, $e \\circ g = g \\circ e = g$. This identity operation corresponds to performing no transformation. Inverse: For every symmetry operation $g$, there exists an inverse operation $g^{-1}$ such that $g \\circ g^{-1} = g^{-1} \\circ g = e$. This inverse operation corresponds to undoing the transformation. Note: $g \\circ h$ denotes applying transformation $g$ first, then transformation $h$.\nLet\u0026rsquo;s take an equilateral triangle as an example to illustrate the concept of symmetry groups: In this example, the symmetry group of the equilateral triangle contains six elements: three rotation operations (0 degrees, 120 degrees, 240 degrees) and three reflection operations (perpendicular lines through each vertex). These operations satisfy the four properties of a group, thus forming a group called $D_3$, the dihedral group of the triangle. Mathematically, we can view these operations as mappings: $$g: \\mathcal{X} \\rightarrow \\mathcal{X}$$ where $\\mathcal{X}$ is the data space. For each element $g \\in G$ in a symmetry group $G$, we have a corresponding mapping $g$. For example, $D_3$ can also be represented as a matrix group: $$ D_3 = \\left\\{ \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 2 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 3 \u0026amp; 1 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 1 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 3 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 1 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 2 \u0026amp; 1 \\end{pmatrix} \\right\\} $$\nReferences Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., \u0026amp; Vandergheynst, P. (2017). Geometric deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4), 18-42. Goodfellow, I., Bengio, Y., \u0026amp; Courville, A. (2016). Deep learning. MIT press. Cohen, T. S., \u0026amp; Welling, M. (2016). Group equivariant convolutional networks. In International conference on machine learning (pp. 2990-2999). PMLR. Kondor, R., \u0026amp; Trivedi, S. (2018). On the generalization of equivariance and convolution in neural networks to the action of compact groups. In International conference on machine learning (pp. 2747-2755). PMLR. Wood, T., \u0026amp; Shawe-Taylor, J. (1996). Representation theory and invariant neural networks. Neural computation, 8(5), 1003-1013. Mallat, S. (2016). Understanding deep convolutional networks. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065), 20150203. Lee, J. M. (2013). Introduction to smooth manifolds. Springer Science \u0026amp; Business Media. ","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"","permalink":"http://localhost:1313/posts/geo_deeplearning1/","summary":"","title":"intro to Geo-deep-learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f} \\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$: $$ \\mathcal{R} = \\mathbb{E}_{(x,y) \\sim P}[L(\\widetilde{f}(x), f(x))] $$\nSuch constructed functions can approximate almost any function (Universal Approximation Theorem), but this does not mean we do not need inductive biases to constrain the learning problem. Given a function class $\\mathcal{F}$ with universal approximation capabilities, we can define a complexity function: $$c: \\mathcal{F} \\rightarrow \\mathbb{R}$$ defining our interpolation problem as: $$\\widetilde{f} = \\arg\\min_{g \\in \\mathcal{F}} c(g) \\quad s.t. \\quad \\widetilde{f}(x_i) = f(x_i), \\quad i = 1, . . . , N$$\nTherefore, we not only hope for good function fitting but also low function complexity. Such a complexity function is what we call the norm of the function. The benefit of this definition is that $\\mathcal{F}$ becomes a Banach space (a complete vector space equipped with a norm), allowing us to use tools from functional analysis to study their properties.\nGeometric Priors To overcome the \u0026ldquo;curse of dimensionality\u0026rdquo; of high-dimensional data, we must utilize the physical characteristics of the data (symmetry and scale separation); although the geometry of the data (Domain) may be complex, the signals defined on it constitute a Hilbert space with good mathematical properties, allowing us to still use tools from linear algebra and functional analysis to handle them.\nFor example, in image processing, data is typically represented as signals (pixel values) defined on a two-dimensional Euclidean space $\\mathbb{R}^2$. This space possesses translation and rotation symmetry, meaning that if we translate or rotate the image, its content and structure remain unchanged. Convolutional Neural Networks (CNNs) exploit this translation symmetry, capturing local features through shared weights, thereby improving learning efficiency and generalization ability.\nSymmetry and Invariance In machine learning, symmetry refers to the property that data or tasks remain unchanged under certain transformations. These transformations can be translation, rotation, scaling, etc. For example, in an image classification task, if we translate or rotate an image, the category of the image usually does not change. This invariance is what we hope the model can capture and utilize.\nSymmetry Groups Symmetry can be formalized through group theory. A group is a set equipped with a binary operation that satisfies properties such as closure, associativity, identity, and inverse.\nSymmetry operations form a group, called a symmetry group, proven as follows:\nClosure: If $g_1$ and $g_2$ are symmetry operations, then their composition $g_1 \\circ g_2$ is also a symmetry operation, because applying two symmetry transformations consecutively still preserves the invariance of the data. Associativity: If $g_1$ and $g_2$ are symmetry operations, then for any $g_3$, $(g_1 \\circ g_2) \\circ g_3 = g_1 \\circ (g_2 \\circ g_3)$, because the order of transformations does not affect the final result. Identity: There exists an identity operation $e$ such that for any symmetry operation $g$, $e \\circ g = g \\circ e = g$. This identity operation corresponds to performing no transformation. Inverse: For every symmetry operation $g$, there exists an inverse operation $g^{-1}$ such that $g \\circ g^{-1} = g^{-1} \\circ g = e$. This inverse operation corresponds to undoing the transformation. Note: $g \\circ h$ denotes applying transformation $g$ first, then transformation $h$.\nLet\u0026rsquo;s take an equilateral triangle as an example to illustrate the concept of symmetry groups: In this example, the symmetry group of the equilateral triangle contains six elements: three rotation operations (0 degrees, 120 degrees, 240 degrees) and three reflection operations (perpendicular lines through each vertex). These operations satisfy the four properties of a group, thus forming a group called $D_3$, the dihedral group of the triangle. Mathematically, we can view these operations as mappings: $$g: \\mathcal{X} \\rightarrow \\mathcal{X}$$ where $\\mathcal{X}$ is the data space. For each element $g \\in G$ in a symmetry group $G$, we have a corresponding mapping $g$. For example, $D_3$ can also be represented as a matrix group: $$ D_3 = \\left\\{ \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 2 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 3 \u0026amp; 1 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 1 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 3 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 1 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 2 \u0026amp; 1 \\end{pmatrix} \\right\\} $$\nReferences Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., \u0026amp; Vandergheynst, P. (2017). Geometric deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4), 18-42. Goodfellow, I., Bengio, Y., \u0026amp; Courville, A. (2016). Deep learning. MIT press. Cohen, T. S., \u0026amp; Welling, M. (2016). Group equivariant convolutional networks. In International conference on machine learning (pp. 2990-2999). PMLR. Kondor, R., \u0026amp; Trivedi, S. (2018). On the generalization of equivariance and convolution in neural networks to the action of compact groups. In International conference on machine learning (pp. 2747-2755). PMLR. Wood, T., \u0026amp; Shawe-Taylor, J. (1996). Representation theory and invariant neural networks. Neural computation, 8(5), 1003-1013. Mallat, S. (2016). Understanding deep convolutional networks. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065), 20150203. Lee, J. M. (2013). Introduction to smooth manifolds. Springer Science \u0026amp; Business Media. ","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"","permalink":"http://localhost:1313/posts/geo_deeplearning1/","summary":"","title":"intro to Geo-deep-learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f} \\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$: $$ \\mathcal{R} = \\mathbb{E}_{(x,y) \\sim P}[L(\\widetilde{f}(x), f(x))] $$\nSuch constructed functions can approximate almost any function (Universal Approximation Theorem), but this does not mean we do not need inductive biases to constrain the learning problem. Given a function class $\\mathcal{F}$ with universal approximation capabilities, we can define a complexity function: $$c: \\mathcal{F} \\rightarrow \\mathbb{R}$$ defining our interpolation problem as: $$\\widetilde{f} = \\arg\\min_{g \\in \\mathcal{F}} c(g) \\quad s.t. \\quad \\widetilde{f}(x_i) = f(x_i), \\quad i = 1, . . . , N$$\nTherefore, we not only hope for good function fitting but also low function complexity. Such a complexity function is what we call the norm of the function. The benefit of this definition is that $\\mathcal{F}$ becomes a Banach space (a complete vector space equipped with a norm), allowing us to use tools from functional analysis to study their properties.\nGeometric Priors To overcome the \u0026ldquo;curse of dimensionality\u0026rdquo; of high-dimensional data, we must utilize the physical characteristics of the data (symmetry and scale separation); although the geometry of the data (Domain) may be complex, the signals defined on it constitute a Hilbert space with good mathematical properties, allowing us to still use tools from linear algebra and functional analysis to handle them.\nFor example, in image processing, data is typically represented as signals (pixel values) defined on a two-dimensional Euclidean space $\\mathbb{R}^2$. This space possesses translation and rotation symmetry, meaning that if we translate or rotate the image, its content and structure remain unchanged. Convolutional Neural Networks (CNNs) exploit this translation symmetry, capturing local features through shared weights, thereby improving learning efficiency and generalization ability.\nSymmetry and Invariance In machine learning, symmetry refers to the property that data or tasks remain unchanged under certain transformations. These transformations can be translation, rotation, scaling, etc. For example, in an image classification task, if we translate or rotate an image, the category of the image usually does not change. This invariance is what we hope the model can capture and utilize.\nSymmetry Groups Symmetry can be formalized through group theory. A group is a set equipped with a binary operation that satisfies properties such as closure, associativity, identity, and inverse.\nSymmetry operations form a group, called a symmetry group, proven as follows:\nClosure: If $g_1$ and $g_2$ are symmetry operations, then their composition $g_1 \\circ g_2$ is also a symmetry operation, because applying two symmetry transformations consecutively still preserves the invariance of the data. Associativity: If $g_1$ and $g_2$ are symmetry operations, then for any $g_3$, $(g_1 \\circ g_2) \\circ g_3 = g_1 \\circ (g_2 \\circ g_3)$, because the order of transformations does not affect the final result. Identity: There exists an identity operation $e$ such that for any symmetry operation $g$, $e \\circ g = g \\circ e = g$. This identity operation corresponds to performing no transformation. Inverse: For every symmetry operation $g$, there exists an inverse operation $g^{-1}$ such that $g \\circ g^{-1} = g^{-1} \\circ g = e$. This inverse operation corresponds to undoing the transformation. Note: $g \\circ h$ denotes applying transformation $g$ first, then transformation $h$.\nLet\u0026rsquo;s take an equilateral triangle as an example to illustrate the concept of symmetry groups: In this example, the symmetry group of the equilateral triangle contains six elements: three rotation operations (0 degrees, 120 degrees, 240 degrees) and three reflection operations (perpendicular lines through each vertex). These operations satisfy the four properties of a group, thus forming a group called $D_3$, the dihedral group of the triangle. Mathematically, we can view these operations as mappings: $$g: \\mathcal{X} \\rightarrow \\mathcal{X}$$ where $\\mathcal{X}$ is the data space. For each element $g \\in G$ in a symmetry group $G$, we have a corresponding mapping $g$. For example, $D_3$ can also be represented as a matrix group: $$ D_3 = \\left\\{ \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 2 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 3 \u0026amp; 1 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 1 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 3 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 1 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 2 \u0026amp; 1 \\end{pmatrix} \\right\\} $$\nReferences Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., \u0026amp; Vandergheynst, P. (2017). Geometric deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4), 18-42. Goodfellow, I., Bengio, Y., \u0026amp; Courville, A. (2016). Deep learning. MIT press. Cohen, T. S., \u0026amp; Welling, M. (2016). Group equivariant convolutional networks. In International conference on machine learning (pp. 2990-2999). PMLR. Kondor, R., \u0026amp; Trivedi, S. (2018). On the generalization of equivariance and convolution in neural networks to the action of compact groups. In International conference on machine learning (pp. 2747-2755). PMLR. Wood, T., \u0026amp; Shawe-Taylor, J. (1996). Representation theory and invariant neural networks. Neural computation, 8(5), 1003-1013. Mallat, S. (2016). Understanding deep convolutional networks. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065), 20150203. Lee, J. M. (2013). Introduction to smooth manifolds. Springer Science \u0026amp; Business Media. ","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"","permalink":"http://localhost:1313/posts/geo_deeplearning1/","summary":"","title":"intro to Geo-deep-learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f} \\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$: $$ \\mathcal{R} = \\mathbb{E}_{(x,y) \\sim P}[L(\\widetilde{f}(x), f(x))] $$\nSuch constructed functions can approximate almost any function (Universal Approximation Theorem), but this does not mean we do not need inductive biases to constrain the learning problem. Given a function class $\\mathcal{F}$ with universal approximation capabilities, we can define a complexity function: $$c: \\mathcal{F} \\rightarrow \\mathbb{R}$$ defining our interpolation problem as: $$\\widetilde{f} = \\arg\\min_{g \\in \\mathcal{F}} c(g) \\quad s.t. \\quad \\widetilde{f}(x_i) = f(x_i), \\quad i = 1, . . . , N$$\nTherefore, we not only hope for good function fitting but also low function complexity. Such a complexity function is what we call the norm of the function. The benefit of this definition is that $\\mathcal{F}$ becomes a Banach space (a complete vector space equipped with a norm), allowing us to use tools from functional analysis to study their properties.\nGeometric Priors To overcome the \u0026ldquo;curse of dimensionality\u0026rdquo; of high-dimensional data, we must utilize the physical characteristics of the data (symmetry and scale separation); although the geometry of the data (Domain) may be complex, the signals defined on it constitute a Hilbert space with good mathematical properties, allowing us to still use tools from linear algebra and functional analysis to handle them.\nFor example, in image processing, data is typically represented as signals (pixel values) defined on a two-dimensional Euclidean space $\\mathbb{R}^2$. This space possesses translation and rotation symmetry, meaning that if we translate or rotate the image, its content and structure remain unchanged. Convolutional Neural Networks (CNNs) exploit this translation symmetry, capturing local features through shared weights, thereby improving learning efficiency and generalization ability.\nSymmetry and Invariance In machine learning, symmetry refers to the property that data or tasks remain unchanged under certain transformations. These transformations can be translation, rotation, scaling, etc. For example, in an image classification task, if we translate or rotate an image, the category of the image usually does not change. This invariance is what we hope the model can capture and utilize.\nSymmetry Groups Symmetry can be formalized through group theory. A group is a set equipped with a binary operation that satisfies properties such as closure, associativity, identity, and inverse.\nSymmetry operations form a group, called a symmetry group, proven as follows:\nClosure: If $g_1$ and $g_2$ are symmetry operations, then their composition $g_1 \\circ g_2$ is also a symmetry operation, because applying two symmetry transformations consecutively still preserves the invariance of the data. Associativity: If $g_1$ and $g_2$ are symmetry operations, then for any $g_3$, $(g_1 \\circ g_2) \\circ g_3 = g_1 \\circ (g_2 \\circ g_3)$, because the order of transformations does not affect the final result. Identity: There exists an identity operation $e$ such that for any symmetry operation $g$, $e \\circ g = g \\circ e = g$. This identity operation corresponds to performing no transformation. Inverse: For every symmetry operation $g$, there exists an inverse operation $g^{-1}$ such that $g \\circ g^{-1} = g^{-1} \\circ g = e$. This inverse operation corresponds to undoing the transformation. Note: $g \\circ h$ denotes applying transformation $g$ first, then transformation $h$.\nLet\u0026rsquo;s take an equilateral triangle as an example to illustrate the concept of symmetry groups: In this example, the symmetry group of the equilateral triangle contains six elements: three rotation operations (0 degrees, 120 degrees, 240 degrees) and three reflection operations (perpendicular lines through each vertex). These operations satisfy the four properties of a group, thus forming a group called $D_3$, the dihedral group of the triangle. Mathematically, we can view these operations as mappings: $$g: \\mathcal{X} \\rightarrow \\mathcal{X}$$ where $\\mathcal{X}$ is the data space. For each element $g \\in G$ in a symmetry group $G$, we have a corresponding mapping $g$. For example, $D_3$ can also be represented as a matrix group: $$ D_3 = \\left\\{ \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 2 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 3 \u0026amp; 1 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 1 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 3 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 1 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 2 \u0026amp; 1 \\end{pmatrix} \\right\\} $$\nReferences Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., \u0026amp; Vandergheynst, P. (2017). Geometric deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4), 18-42. Goodfellow, I., Bengio, Y., \u0026amp; Courville, A. (2016). Deep learning. MIT press. Cohen, T. S., \u0026amp; Welling, M. (2016). Group equivariant convolutional networks. In International conference on machine learning (pp. 2990-2999). PMLR. Kondor, R., \u0026amp; Trivedi, S. (2018). On the generalization of equivariance and convolution in neural networks to the action of compact groups. In International conference on machine learning (pp. 2747-2755). PMLR. Wood, T., \u0026amp; Shawe-Taylor, J. (1996). Representation theory and invariant neural networks. Neural computation, 8(5), 1003-1013. Mallat, S. (2016). Understanding deep convolutional networks. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065), 20150203. Lee, J. M. (2013). Introduction to smooth manifolds. Springer Science \u0026amp; Business Media. ","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"","permalink":"http://localhost:1313/posts/geo_deeplearning1/","summary":"","title":"intro to Geo-deep-learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f} \\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$: $$ \\mathcal{R} = \\mathbb{E}_{(x,y) \\sim P}[L(\\widetilde{f}(x), f(x))] $$\nSuch constructed functions can approximate almost any function (Universal Approximation Theorem), but this does not mean we do not need inductive biases to constrain the learning problem. Given a function class $\\mathcal{F}$ with universal approximation capabilities, we can define a complexity function: $$c: \\mathcal{F} \\rightarrow \\mathbb{R}$$ defining our interpolation problem as: $$\\widetilde{f} = \\arg\\min_{g \\in \\mathcal{F}} c(g) \\quad s.t. \\quad \\widetilde{f}(x_i) = f(x_i), \\quad i = 1, . . . , N$$\nTherefore, we not only hope for good function fitting but also low function complexity. Such a complexity function is what we call the norm of the function. The benefit of this definition is that $\\mathcal{F}$ becomes a Banach space (a complete vector space equipped with a norm), allowing us to use tools from functional analysis to study their properties.\nGeometric Priors To overcome the \u0026ldquo;curse of dimensionality\u0026rdquo; of high-dimensional data, we must utilize the physical characteristics of the data (symmetry and scale separation); although the geometry of the data (Domain) may be complex, the signals defined on it constitute a Hilbert space with good mathematical properties, allowing us to still use tools from linear algebra and functional analysis to handle them.\nFor example, in image processing, data is typically represented as signals (pixel values) defined on a two-dimensional Euclidean space $\\mathbb{R}^2$. This space possesses translation and rotation symmetry, meaning that if we translate or rotate the image, its content and structure remain unchanged. Convolutional Neural Networks (CNNs) exploit this translation symmetry, capturing local features through shared weights, thereby improving learning efficiency and generalization ability.\nSymmetry and Invariance In machine learning, symmetry refers to the property that data or tasks remain unchanged under certain transformations. These transformations can be translation, rotation, scaling, etc. For example, in an image classification task, if we translate or rotate an image, the category of the image usually does not change. This invariance is what we hope the model can capture and utilize.\nSymmetry Groups Symmetry can be formalized through group theory. A group is a set equipped with a binary operation that satisfies properties such as closure, associativity, identity, and inverse.\nSymmetry operations form a group, called a symmetry group, proven as follows:\nClosure: If $g_1$ and $g_2$ are symmetry operations, then their composition $g_1 \\circ g_2$ is also a symmetry operation, because applying two symmetry transformations consecutively still preserves the invariance of the data. Associativity: If $g_1$ and $g_2$ are symmetry operations, then for any $g_3$, $(g_1 \\circ g_2) \\circ g_3 = g_1 \\circ (g_2 \\circ g_3)$, because the order of transformations does not affect the final result. Identity: There exists an identity operation $e$ such that for any symmetry operation $g$, $e \\circ g = g \\circ e = g$. This identity operation corresponds to performing no transformation. Inverse: For every symmetry operation $g$, there exists an inverse operation $g^{-1}$ such that $g \\circ g^{-1} = g^{-1} \\circ g = e$. This inverse operation corresponds to undoing the transformation. Note: $g \\circ h$ denotes applying transformation $g$ first, then transformation $h$.\nLet\u0026rsquo;s take an equilateral triangle as an example to illustrate the concept of symmetry groups: In this example, the symmetry group of the equilateral triangle contains six elements: three rotation operations (0 degrees, 120 degrees, 240 degrees) and three reflection operations (perpendicular lines through each vertex). These operations satisfy the four properties of a group, thus forming a group called $D_3$, the dihedral group of the triangle. Mathematically, we can view these operations as mappings: $$g: \\mathcal{X} \\rightarrow \\mathcal{X}$$ where $\\mathcal{X}$ is the data space. For each element $g \\in G$ in a symmetry group $G$, we have a corresponding mapping $g$. For example, $D_3$ can also be represented as a matrix group: $$ D_3 = \\left\\{ \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 2 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 3 \u0026amp; 1 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 1 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 3 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 1 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 2 \u0026amp; 1 \\end{pmatrix} \\right\\} $$\nReferences Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., \u0026amp; Vandergheynst, P. (2017). Geometric deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4), 18-42. Goodfellow, I., Bengio, Y., \u0026amp; Courville, A. (2016). Deep learning. MIT press. Cohen, T. S., \u0026amp; Welling, M. (2016). Group equivariant convolutional networks. In International conference on machine learning (pp. 2990-2999). PMLR. Kondor, R., \u0026amp; Trivedi, S. (2018). On the generalization of equivariance and convolution in neural networks to the action of compact groups. In International conference on machine learning (pp. 2747-2755). PMLR. Wood, T., \u0026amp; Shawe-Taylor, J. (1996). Representation theory and invariant neural networks. Neural computation, 8(5), 1003-1013. Mallat, S. (2016). Understanding deep convolutional networks. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065), 20150203. Lee, J. M. (2013). Introduction to smooth manifolds. Springer Science \u0026amp; Business Media. ","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"","permalink":"http://localhost:1313/posts/geo_deeplearning1/","summary":"","title":"intro to Geo-deep-learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f} \\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$: $$ \\mathcal{R} = \\mathbb{E}_{(x,y) \\sim P}[L(\\widetilde{f}(x), f(x))] $$\nSuch constructed functions can approximate almost any function (Universal Approximation Theorem), but this does not mean we do not need inductive biases to constrain the learning problem. Given a function class $\\mathcal{F}$ with universal approximation capabilities, we can define a complexity function: $$c: \\mathcal{F} \\rightarrow \\mathbb{R}$$ defining our interpolation problem as: $$\\widetilde{f} = \\arg\\min_{g \\in \\mathcal{F}} c(g) \\quad s.t. \\quad \\widetilde{f}(x_i) = f(x_i), \\quad i = 1, . . . , N$$\nTherefore, we not only hope for good function fitting but also low function complexity. Such a complexity function is what we call the norm of the function. The benefit of this definition is that $\\mathcal{F}$ becomes a Banach space (a complete vector space equipped with a norm), allowing us to use tools from functional analysis to study their properties.\nGeometric Priors To overcome the \u0026ldquo;curse of dimensionality\u0026rdquo; of high-dimensional data, we must utilize the physical characteristics of the data (symmetry and scale separation); although the geometry of the data (Domain) may be complex, the signals defined on it constitute a Hilbert space with good mathematical properties, allowing us to still use tools from linear algebra and functional analysis to handle them.\nFor example, in image processing, data is typically represented as signals (pixel values) defined on a two-dimensional Euclidean space $\\mathbb{R}^2$. This space possesses translation and rotation symmetry, meaning that if we translate or rotate the image, its content and structure remain unchanged. Convolutional Neural Networks (CNNs) exploit this translation symmetry, capturing local features through shared weights, thereby improving learning efficiency and generalization ability.\nSymmetry and Invariance In machine learning, symmetry refers to the property that data or tasks remain unchanged under certain transformations. These transformations can be translation, rotation, scaling, etc. For example, in an image classification task, if we translate or rotate an image, the category of the image usually does not change. This invariance is what we hope the model can capture and utilize.\nSymmetry Groups Symmetry can be formalized through group theory. A group is a set equipped with a binary operation that satisfies properties such as closure, associativity, identity, and inverse.\nSymmetry operations form a group, called a symmetry group, proven as follows:\nClosure: If $g_1$ and $g_2$ are symmetry operations, then their composition $g_1 \\circ g_2$ is also a symmetry operation, because applying two symmetry transformations consecutively still preserves the invariance of the data. Associativity: If $g_1$ and $g_2$ are symmetry operations, then for any $g_3$, $(g_1 \\circ g_2) \\circ g_3 = g_1 \\circ (g_2 \\circ g_3)$, because the order of transformations does not affect the final result. Identity: There exists an identity operation $e$ such that for any symmetry operation $g$, $e \\circ g = g \\circ e = g$. This identity operation corresponds to performing no transformation. Inverse: For every symmetry operation $g$, there exists an inverse operation $g^{-1}$ such that $g \\circ g^{-1} = g^{-1} \\circ g = e$. This inverse operation corresponds to undoing the transformation. Note: $g \\circ h$ denotes applying transformation $g$ first, then transformation $h$.\nLet\u0026rsquo;s take an equilateral triangle as an example to illustrate the concept of symmetry groups: In this example, the symmetry group of the equilateral triangle contains six elements: three rotation operations (0 degrees, 120 degrees, 240 degrees) and three reflection operations (perpendicular lines through each vertex). These operations satisfy the four properties of a group, thus forming a group called $D_3$, the dihedral group of the triangle. Mathematically, we can view these operations as mappings: $$g: \\mathcal{X} \\rightarrow \\mathcal{X}$$ where $\\mathcal{X}$ is the data space. For each element $g \\in G$ in a symmetry group $G$, we have a corresponding mapping $g$. For example, $D_3$ can also be represented as a matrix group: $$ D_3 = \\left\\{ \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 2 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 3 \u0026amp; 1 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 1 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 3 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 1 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 2 \u0026amp; 1 \\end{pmatrix} \\right\\} $$\nReferences Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., \u0026amp; Vandergheynst, P. (2017). Geometric deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4), 18-42. Goodfellow, I., Bengio, Y., \u0026amp; Courville, A. (2016). Deep learning. MIT press. Cohen, T. S., \u0026amp; Welling, M. (2016). Group equivariant convolutional networks. In International conference on machine learning (pp. 2990-2999). PMLR. Kondor, R., \u0026amp; Trivedi, S. (2018). On the generalization of equivariance and convolution in neural networks to the action of compact groups. In International conference on machine learning (pp. 2747-2755). PMLR. Wood, T., \u0026amp; Shawe-Taylor, J. (1996). Representation theory and invariant neural networks. Neural computation, 8(5), 1003-1013. Mallat, S. (2016). Understanding deep convolutional networks. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065), 20150203. Lee, J. M. (2013). Introduction to smooth manifolds. Springer Science \u0026amp; Business Media. ","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"","permalink":"http://localhost:1313/posts/geo_deeplearning1/","summary":"","title":"intro to Geo-deep-learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f} \\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$: $$ \\mathcal{R} = \\mathbb{E}_{(x,y) \\sim P}[L(\\widetilde{f}(x), f(x))] $$\nSuch constructed functions can approximate almost any function (Universal Approximation Theorem), but this does not mean we do not need inductive biases to constrain the learning problem. Given a function class $\\mathcal{F}$ with universal approximation capabilities, we can define a complexity function: $$c: \\mathcal{F} \\rightarrow \\mathbb{R}$$ defining our interpolation problem as: $$\\widetilde{f} = \\arg\\min_{g \\in \\mathcal{F}} c(g) \\quad s.t. \\quad \\widetilde{f}(x_i) = f(x_i), \\quad i = 1, . . . , N$$\nTherefore, we not only hope for good function fitting but also low function complexity. Such a complexity function is what we call the norm of the function. The benefit of this definition is that $\\mathcal{F}$ becomes a Banach space (a complete vector space equipped with a norm), allowing us to use tools from functional analysis to study their properties.\nGeometric Priors To overcome the \u0026ldquo;curse of dimensionality\u0026rdquo; of high-dimensional data, we must utilize the physical characteristics of the data (symmetry and scale separation); although the geometry of the data (Domain) may be complex, the signals defined on it constitute a Hilbert space with good mathematical properties, allowing us to still use tools from linear algebra and functional analysis to handle them.\nFor example, in image processing, data is typically represented as signals (pixel values) defined on a two-dimensional Euclidean space $\\mathbb{R}^2$. This space possesses translation and rotation symmetry, meaning that if we translate or rotate the image, its content and structure remain unchanged. Convolutional Neural Networks (CNNs) exploit this translation symmetry, capturing local features through shared weights, thereby improving learning efficiency and generalization ability.\nSymmetry and Invariance In machine learning, symmetry refers to the property that data or tasks remain unchanged under certain transformations. These transformations can be translation, rotation, scaling, etc. For example, in an image classification task, if we translate or rotate an image, the category of the image usually does not change. This invariance is what we hope the model can capture and utilize.\nSymmetry Groups Symmetry can be formalized through group theory. A group is a set equipped with a binary operation that satisfies properties such as closure, associativity, identity, and inverse.\nSymmetry operations form a group, called a symmetry group, proven as follows:\nClosure: If $g_1$ and $g_2$ are symmetry operations, then their composition $g_1 \\circ g_2$ is also a symmetry operation, because applying two symmetry transformations consecutively still preserves the invariance of the data. Associativity: If $g_1$ and $g_2$ are symmetry operations, then for any $g_3$, $(g_1 \\circ g_2) \\circ g_3 = g_1 \\circ (g_2 \\circ g_3)$, because the order of transformations does not affect the final result. Identity: There exists an identity operation $e$ such that for any symmetry operation $g$, $e \\circ g = g \\circ e = g$. This identity operation corresponds to performing no transformation. Inverse: For every symmetry operation $g$, there exists an inverse operation $g^{-1}$ such that $g \\circ g^{-1} = g^{-1} \\circ g = e$. This inverse operation corresponds to undoing the transformation. Note: $g \\circ h$ denotes applying transformation $g$ first, then transformation $h$.\nLet\u0026rsquo;s take an equilateral triangle as an example to illustrate the concept of symmetry groups: In this example, the symmetry group of the equilateral triangle contains six elements: three rotation operations (0 degrees, 120 degrees, 240 degrees) and three reflection operations (perpendicular lines through each vertex). These operations satisfy the four properties of a group, thus forming a group called $D_3$, the dihedral group of the triangle. Mathematically, we can view these operations as mappings: $$g: \\mathcal{X} \\rightarrow \\mathcal{X}$$ where $\\mathcal{X}$ is the data space. For each element $g \\in G$ in a symmetry group $G$, we have a corresponding mapping $g$. For example, $D_3$ can also be represented as a matrix group: $$ D_3 = \\left\\{ \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 2 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 3 \u0026amp; 1 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 1 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 3 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 1 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 2 \u0026amp; 1 \\end{pmatrix} \\right\\} $$\nReferences Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., \u0026amp; Vandergheynst, P. (2017). Geometric deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4), 18-42. Goodfellow, I., Bengio, Y., \u0026amp; Courville, A. (2016). Deep learning. MIT press. Cohen, T. S., \u0026amp; Welling, M. (2016). Group equivariant convolutional networks. In International conference on machine learning (pp. 2990-2999). PMLR. Kondor, R., \u0026amp; Trivedi, S. (2018). On the generalization of equivariance and convolution in neural networks to the action of compact groups. In International conference on machine learning (pp. 2747-2755). PMLR. Wood, T., \u0026amp; Shawe-Taylor, J. (1996). Representation theory and invariant neural networks. Neural computation, 8(5), 1003-1013. Mallat, S. (2016). Understanding deep convolutional networks. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065), 20150203. Lee, J. M. (2013). Introduction to smooth manifolds. Springer Science \u0026amp; Business Media. ","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"","permalink":"http://localhost:1313/posts/geo_deeplearning1/","summary":"","title":"intro to Geo-deep-learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f} \\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$: $$ \\mathcal{R} = \\mathbb{E}_{(x,y) \\sim P}[L(\\widetilde{f}(x), f(x))] $$\nSuch constructed functions can approximate almost any function (Universal Approximation Theorem), but this does not mean we do not need inductive biases to constrain the learning problem. Given a function class $\\mathcal{F}$ with universal approximation capabilities, we can define a complexity function: $$c: \\mathcal{F} \\rightarrow \\mathbb{R}$$ defining our interpolation problem as: $$\\widetilde{f} = \\arg\\min_{g \\in \\mathcal{F}} c(g) \\quad s.t. \\quad \\widetilde{f}(x_i) = f(x_i), \\quad i = 1, . . . , N$$\nTherefore, we not only hope for good function fitting but also low function complexity. Such a complexity function is what we call the norm of the function. The benefit of this definition is that $\\mathcal{F}$ becomes a Banach space (a complete vector space equipped with a norm), allowing us to use tools from functional analysis to study their properties.\nGeometric Priors To overcome the \u0026ldquo;curse of dimensionality\u0026rdquo; of high-dimensional data, we must utilize the physical characteristics of the data (symmetry and scale separation); although the geometry of the data (Domain) may be complex, the signals defined on it constitute a Hilbert space with good mathematical properties, allowing us to still use tools from linear algebra and functional analysis to handle them.\nFor example, in image processing, data is typically represented as signals (pixel values) defined on a two-dimensional Euclidean space $\\mathbb{R}^2$. This space possesses translation and rotation symmetry, meaning that if we translate or rotate the image, its content and structure remain unchanged. Convolutional Neural Networks (CNNs) exploit this translation symmetry, capturing local features through shared weights, thereby improving learning efficiency and generalization ability.\nSymmetry and Invariance In machine learning, symmetry refers to the property that data or tasks remain unchanged under certain transformations. These transformations can be translation, rotation, scaling, etc. For example, in an image classification task, if we translate or rotate an image, the category of the image usually does not change. This invariance is what we hope the model can capture and utilize.\nSymmetry Groups Symmetry can be formalized through group theory. A group is a set equipped with a binary operation that satisfies properties such as closure, associativity, identity, and inverse.\nSymmetry operations form a group, called a symmetry group, proven as follows:\nClosure: If $g_1$ and $g_2$ are symmetry operations, then their composition $g_1 \\circ g_2$ is also a symmetry operation, because applying two symmetry transformations consecutively still preserves the invariance of the data. Associativity: If $g_1$ and $g_2$ are symmetry operations, then for any $g_3$, $(g_1 \\circ g_2) \\circ g_3 = g_1 \\circ (g_2 \\circ g_3)$, because the order of transformations does not affect the final result. Identity: There exists an identity operation $e$ such that for any symmetry operation $g$, $e \\circ g = g \\circ e = g$. This identity operation corresponds to performing no transformation. Inverse: For every symmetry operation $g$, there exists an inverse operation $g^{-1}$ such that $g \\circ g^{-1} = g^{-1} \\circ g = e$. This inverse operation corresponds to undoing the transformation. Note: $g \\circ h$ denotes applying transformation $g$ first, then transformation $h$.\nLet\u0026rsquo;s take an equilateral triangle as an example to illustrate the concept of symmetry groups: In this example, the symmetry group of the equilateral triangle contains six elements: three rotation operations (0 degrees, 120 degrees, 240 degrees) and three reflection operations (perpendicular lines through each vertex). These operations satisfy the four properties of a group, thus forming a group called $D_3$, the dihedral group of the triangle. Mathematically, we can view these operations as mappings: $$g: \\mathcal{X} \\rightarrow \\mathcal{X}$$ where $\\mathcal{X}$ is the data space. For each element $g \\in G$ in a symmetry group $G$, we have a corresponding mapping $g$. For example, $D_3$ can also be represented as a matrix group: $$ D_3 = \\left\\{ \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 2 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 3 \u0026amp; 1 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 1 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 3 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 1 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 2 \u0026amp; 1 \\end{pmatrix} \\right\\} $$\nReferences Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., \u0026amp; Vandergheynst, P. (2017). Geometric deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4), 18-42. Goodfellow, I., Bengio, Y., \u0026amp; Courville, A. (2016). Deep learning. MIT press. Cohen, T. S., \u0026amp; Welling, M. (2016). Group equivariant convolutional networks. In International conference on machine learning (pp. 2990-2999). PMLR. Kondor, R., \u0026amp; Trivedi, S. (2018). On the generalization of equivariance and convolution in neural networks to the action of compact groups. In International conference on machine learning (pp. 2747-2755). PMLR. Wood, T., \u0026amp; Shawe-Taylor, J. (1996). Representation theory and invariant neural networks. Neural computation, 8(5), 1003-1013. Mallat, S. (2016). Understanding deep convolutional networks. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065), 20150203. Lee, J. M. (2013). Introduction to smooth manifolds. Springer Science \u0026amp; Business Media. ","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"","permalink":"http://localhost:1313/posts/geo_deeplearning1/","summary":"","title":"intro to Geo-deep-learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f} \\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$: $$ \\mathcal{R} = \\mathbb{E}_{(x,y) \\sim P}[L(\\widetilde{f}(x), f(x))] $$\nSuch constructed functions can approximate almost any function (Universal Approximation Theorem), but this does not mean we do not need inductive biases to constrain the learning problem. Given a function class $\\mathcal{F}$ with universal approximation capabilities, we can define a complexity function: $$c: \\mathcal{F} \\rightarrow \\mathbb{R}$$ defining our interpolation problem as: $$\\widetilde{f} = \\arg\\min_{g \\in \\mathcal{F}} c(g) \\quad s.t. \\quad \\widetilde{f}(x_i) = f(x_i), \\quad i = 1, . . . , N$$\nTherefore, we not only hope for good function fitting but also low function complexity. Such a complexity function is what we call the norm of the function. The benefit of this definition is that $\\mathcal{F}$ becomes a Banach space (a complete vector space equipped with a norm), allowing us to use tools from functional analysis to study their properties.\nGeometric Priors To overcome the \u0026ldquo;curse of dimensionality\u0026rdquo; of high-dimensional data, we must utilize the physical characteristics of the data (symmetry and scale separation); although the geometry of the data (Domain) may be complex, the signals defined on it constitute a Hilbert space with good mathematical properties, allowing us to still use tools from linear algebra and functional analysis to handle them.\nFor example, in image processing, data is typically represented as signals (pixel values) defined on a two-dimensional Euclidean space $\\mathbb{R}^2$. This space possesses translation and rotation symmetry, meaning that if we translate or rotate the image, its content and structure remain unchanged. Convolutional Neural Networks (CNNs) exploit this translation symmetry, capturing local features through shared weights, thereby improving learning efficiency and generalization ability.\nSymmetry and Invariance In machine learning, symmetry refers to the property that data or tasks remain unchanged under certain transformations. These transformations can be translation, rotation, scaling, etc. For example, in an image classification task, if we translate or rotate an image, the category of the image usually does not change. This invariance is what we hope the model can capture and utilize.\nSymmetry Groups Symmetry can be formalized through group theory. A group is a set equipped with a binary operation that satisfies properties such as closure, associativity, identity, and inverse.\nSymmetry operations form a group, called a symmetry group, proven as follows:\nClosure: If $g_1$ and $g_2$ are symmetry operations, then their composition $g_1 \\circ g_2$ is also a symmetry operation, because applying two symmetry transformations consecutively still preserves the invariance of the data. Associativity: If $g_1$ and $g_2$ are symmetry operations, then for any $g_3$, $(g_1 \\circ g_2) \\circ g_3 = g_1 \\circ (g_2 \\circ g_3)$, because the order of transformations does not affect the final result. Identity: There exists an identity operation $e$ such that for any symmetry operation $g$, $e \\circ g = g \\circ e = g$. This identity operation corresponds to performing no transformation. Inverse: For every symmetry operation $g$, there exists an inverse operation $g^{-1}$ such that $g \\circ g^{-1} = g^{-1} \\circ g = e$. This inverse operation corresponds to undoing the transformation. Note: $g \\circ h$ denotes applying transformation $g$ first, then transformation $h$.\nLet\u0026rsquo;s take an equilateral triangle as an example to illustrate the concept of symmetry groups: In this example, the symmetry group of the equilateral triangle contains six elements: three rotation operations (0 degrees, 120 degrees, 240 degrees) and three reflection operations (perpendicular lines through each vertex). These operations satisfy the four properties of a group, thus forming a group called $D_3$, the dihedral group of the triangle. Mathematically, we can view these operations as mappings: $$g: \\mathcal{X} \\rightarrow \\mathcal{X}$$ where $\\mathcal{X}$ is the data space. For each element $g \\in G$ in a symmetry group $G$, we have a corresponding mapping $g$. For example, $D_3$ can also be represented as a matrix group: $$ D_3 = \\left\\{ \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 2 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 3 \u0026amp; 1 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 1 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 3 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 1 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 2 \u0026amp; 1 \\end{pmatrix} \\right\\} $$\nReferences Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., \u0026amp; Vandergheynst, P. (2017). Geometric deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4), 18-42. Goodfellow, I., Bengio, Y., \u0026amp; Courville, A. (2016). Deep learning. MIT press. Cohen, T. S., \u0026amp; Welling, M. (2016). Group equivariant convolutional networks. In International conference on machine learning (pp. 2990-2999). PMLR. Kondor, R., \u0026amp; Trivedi, S. (2018). On the generalization of equivariance and convolution in neural networks to the action of compact groups. In International conference on machine learning (pp. 2747-2755). PMLR. Wood, T., \u0026amp; Shawe-Taylor, J. (1996). Representation theory and invariant neural networks. Neural computation, 8(5), 1003-1013. Mallat, S. (2016). Understanding deep convolutional networks. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065), 20150203. Lee, J. M. (2013). Introduction to smooth manifolds. Springer Science \u0026amp; Business Media. ","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"","permalink":"http://localhost:1313/posts/geo_deeplearning1/","summary":"","title":"intro to Geo-deep-learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f} \\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$: $$ \\mathcal{R} = \\mathbb{E}_{(x,y) \\sim P}[L(\\widetilde{f}(x), f(x))] $$\nSuch constructed functions can approximate almost any function (Universal Approximation Theorem), but this does not mean we do not need inductive biases to constrain the learning problem. Given a function class $\\mathcal{F}$ with universal approximation capabilities, we can define a complexity function: $$c: \\mathcal{F} \\rightarrow \\mathbb{R}$$ defining our interpolation problem as: $$\\widetilde{f} = \\arg\\min_{g \\in \\mathcal{F}} c(g) \\quad s.t. \\quad \\widetilde{f}(x_i) = f(x_i), \\quad i = 1, . . . , N$$\nTherefore, we not only hope for good function fitting but also low function complexity. Such a complexity function is what we call the norm of the function. The benefit of this definition is that $\\mathcal{F}$ becomes a Banach space (a complete vector space equipped with a norm), allowing us to use tools from functional analysis to study their properties.\nGeometric Priors To overcome the \u0026ldquo;curse of dimensionality\u0026rdquo; of high-dimensional data, we must utilize the physical characteristics of the data (symmetry and scale separation); although the geometry of the data (Domain) may be complex, the signals defined on it constitute a Hilbert space with good mathematical properties, allowing us to still use tools from linear algebra and functional analysis to handle them.\nFor example, in image processing, data is typically represented as signals (pixel values) defined on a two-dimensional Euclidean space $\\mathbb{R}^2$. This space possesses translation and rotation symmetry, meaning that if we translate or rotate the image, its content and structure remain unchanged. Convolutional Neural Networks (CNNs) exploit this translation symmetry, capturing local features through shared weights, thereby improving learning efficiency and generalization ability.\nSymmetry and Invariance In machine learning, symmetry refers to the property that data or tasks remain unchanged under certain transformations. These transformations can be translation, rotation, scaling, etc. For example, in an image classification task, if we translate or rotate an image, the category of the image usually does not change. This invariance is what we hope the model can capture and utilize.\nSymmetry Groups Symmetry can be formalized through group theory. A group is a set equipped with a binary operation that satisfies properties such as closure, associativity, identity, and inverse.\nSymmetry operations form a group, called a symmetry group, proven as follows:\nClosure: If $g_1$ and $g_2$ are symmetry operations, then their composition $g_1 \\circ g_2$ is also a symmetry operation, because applying two symmetry transformations consecutively still preserves the invariance of the data. Associativity: If $g_1$ and $g_2$ are symmetry operations, then for any $g_3$, $(g_1 \\circ g_2) \\circ g_3 = g_1 \\circ (g_2 \\circ g_3)$, because the order of transformations does not affect the final result. Identity: There exists an identity operation $e$ such that for any symmetry operation $g$, $e \\circ g = g \\circ e = g$. This identity operation corresponds to performing no transformation. Inverse: For every symmetry operation $g$, there exists an inverse operation $g^{-1}$ such that $g \\circ g^{-1} = g^{-1} \\circ g = e$. This inverse operation corresponds to undoing the transformation. Note: $g \\circ h$ denotes applying transformation $g$ first, then transformation $h$.\nLet\u0026rsquo;s take an equilateral triangle as an example to illustrate the concept of symmetry groups: In this example, the symmetry group of the equilateral triangle contains six elements: three rotation operations (0 degrees, 120 degrees, 240 degrees) and three reflection operations (perpendicular lines through each vertex). These operations satisfy the four properties of a group, thus forming a group called $D_3$, the dihedral group of the triangle. Mathematically, we can view these operations as mappings: $$g: \\mathcal{X} \\rightarrow \\mathcal{X}$$ where $\\mathcal{X}$ is the data space. For each element $g \\in G$ in a symmetry group $G$, we have a corresponding mapping $g$. For example, $D_3$ can also be represented as a matrix group: $$ D_3 = \\left\\{ \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 2 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 3 \u0026amp; 1 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 1 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 3 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 1 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 2 \u0026amp; 1 \\end{pmatrix} \\right\\} $$\nReferences Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., \u0026amp; Vandergheynst, P. (2017). Geometric deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4), 18-42. Goodfellow, I., Bengio, Y., \u0026amp; Courville, A. (2016). Deep learning. MIT press. Cohen, T. S., \u0026amp; Welling, M. (2016). Group equivariant convolutional networks. In International conference on machine learning (pp. 2990-2999). PMLR. Kondor, R., \u0026amp; Trivedi, S. (2018). On the generalization of equivariance and convolution in neural networks to the action of compact groups. In International conference on machine learning (pp. 2747-2755). PMLR. Wood, T., \u0026amp; Shawe-Taylor, J. (1996). Representation theory and invariant neural networks. Neural computation, 8(5), 1003-1013. Mallat, S. (2016). Understanding deep convolutional networks. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065), 20150203. Lee, J. M. (2013). Introduction to smooth manifolds. Springer Science \u0026amp; Business Media. ","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"","permalink":"http://localhost:1313/posts/geo_deeplearning1/","summary":"","title":"intro to Geo-deep-learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f} \\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$: $$ \\mathcal{R} = \\mathbb{E}_{(x,y) \\sim P}[L(\\widetilde{f}(x), f(x))] $$\nSuch constructed functions can approximate almost any function (Universal Approximation Theorem), but this does not mean we do not need inductive biases to constrain the learning problem. Given a function class $\\mathcal{F}$ with universal approximation capabilities, we can define a complexity function: $$c: \\mathcal{F} \\rightarrow \\mathbb{R}$$ defining our interpolation problem as: $$\\widetilde{f} = \\arg\\min_{g \\in \\mathcal{F}} c(g) \\quad s.t. \\quad \\widetilde{f}(x_i) = f(x_i), \\quad i = 1, . . . , N$$\nTherefore, we not only hope for good function fitting but also low function complexity. Such a complexity function is what we call the norm of the function. The benefit of this definition is that $\\mathcal{F}$ becomes a Banach space (a complete vector space equipped with a norm), allowing us to use tools from functional analysis to study their properties.\nGeometric Priors To overcome the \u0026ldquo;curse of dimensionality\u0026rdquo; of high-dimensional data, we must utilize the physical characteristics of the data (symmetry and scale separation); although the geometry of the data (Domain) may be complex, the signals defined on it constitute a Hilbert space with good mathematical properties, allowing us to still use tools from linear algebra and functional analysis to handle them.\nFor example, in image processing, data is typically represented as signals (pixel values) defined on a two-dimensional Euclidean space $\\mathbb{R}^2$. This space possesses translation and rotation symmetry, meaning that if we translate or rotate the image, its content and structure remain unchanged. Convolutional Neural Networks (CNNs) exploit this translation symmetry, capturing local features through shared weights, thereby improving learning efficiency and generalization ability.\nSymmetry and Invariance In machine learning, symmetry refers to the property that data or tasks remain unchanged under certain transformations. These transformations can be translation, rotation, scaling, etc. For example, in an image classification task, if we translate or rotate an image, the category of the image usually does not change. This invariance is what we hope the model can capture and utilize.\nSymmetry Groups Symmetry can be formalized through group theory. A group is a set equipped with a binary operation that satisfies properties such as closure, associativity, identity, and inverse.\nSymmetry operations form a group, called a symmetry group, proven as follows:\nClosure: If $g_1$ and $g_2$ are symmetry operations, then their composition $g_1 \\circ g_2$ is also a symmetry operation, because applying two symmetry transformations consecutively still preserves the invariance of the data. Associativity: If $g_1$ and $g_2$ are symmetry operations, then for any $g_3$, $(g_1 \\circ g_2) \\circ g_3 = g_1 \\circ (g_2 \\circ g_3)$, because the order of transformations does not affect the final result. Identity: There exists an identity operation $e$ such that for any symmetry operation $g$, $e \\circ g = g \\circ e = g$. This identity operation corresponds to performing no transformation. Inverse: For every symmetry operation $g$, there exists an inverse operation $g^{-1}$ such that $g \\circ g^{-1} = g^{-1} \\circ g = e$. This inverse operation corresponds to undoing the transformation. Note: $g \\circ h$ denotes applying transformation $g$ first, then transformation $h$.\nLet\u0026rsquo;s take an equilateral triangle as an example to illustrate the concept of symmetry groups: In this example, the symmetry group of the equilateral triangle contains six elements: three rotation operations (0 degrees, 120 degrees, 240 degrees) and three reflection operations (perpendicular lines through each vertex). These operations satisfy the four properties of a group, thus forming a group called $D_3$, the dihedral group of the triangle. Mathematically, we can view these operations as mappings: $$g: \\mathcal{X} \\rightarrow \\mathcal{X}$$ where $\\mathcal{X}$ is the data space. For each element $g \\in G$ in a symmetry group $G$, we have a corresponding mapping $g$. For example, $D_3$ can also be represented as a matrix group: $$ D_3 = \\left\\{ \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 2 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 3 \u0026amp; 1 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 1 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 3 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 1 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 2 \u0026amp; 1 \\end{pmatrix} \\right\\} $$\nReferences Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., \u0026amp; Vandergheynst, P. (2017). Geometric deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4), 18-42. Goodfellow, I., Bengio, Y., \u0026amp; Courville, A. (2016). Deep learning. MIT press. Cohen, T. S., \u0026amp; Welling, M. (2016). Group equivariant convolutional networks. In International conference on machine learning (pp. 2990-2999). PMLR. Kondor, R., \u0026amp; Trivedi, S. (2018). On the generalization of equivariance and convolution in neural networks to the action of compact groups. In International conference on machine learning (pp. 2747-2755). PMLR. Wood, T., \u0026amp; Shawe-Taylor, J. (1996). Representation theory and invariant neural networks. Neural computation, 8(5), 1003-1013. Mallat, S. (2016). Understanding deep convolutional networks. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065), 20150203. Lee, J. M. (2013). Introduction to smooth manifolds. Springer Science \u0026amp; Business Media. ","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"","permalink":"http://localhost:1313/posts/geo_deeplearning1/","summary":"","title":"intro to Geo-deep-learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f} \\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$: $$ \\mathcal{R} = \\mathbb{E}_{(x,y) \\sim P}[L(\\widetilde{f}(x), f(x))] $$\nSuch constructed functions can approximate almost any function (Universal Approximation Theorem), but this does not mean we do not need inductive biases to constrain the learning problem. Given a function class $\\mathcal{F}$ with universal approximation capabilities, we can define a complexity function: $$c: \\mathcal{F} \\rightarrow \\mathbb{R}$$ defining our interpolation problem as: $$\\widetilde{f} = \\arg\\min_{g \\in \\mathcal{F}} c(g) \\quad s.t. \\quad \\widetilde{f}(x_i) = f(x_i), \\quad i = 1, . . . , N$$\nTherefore, we not only hope for good function fitting but also low function complexity. Such a complexity function is what we call the norm of the function. The benefit of this definition is that $\\mathcal{F}$ becomes a Banach space (a complete vector space equipped with a norm), allowing us to use tools from functional analysis to study their properties.\nGeometric Priors To overcome the \u0026ldquo;curse of dimensionality\u0026rdquo; of high-dimensional data, we must utilize the physical characteristics of the data (symmetry and scale separation); although the geometry of the data (Domain) may be complex, the signals defined on it constitute a Hilbert space with good mathematical properties, allowing us to still use tools from linear algebra and functional analysis to handle them.\nFor example, in image processing, data is typically represented as signals (pixel values) defined on a two-dimensional Euclidean space $\\mathbb{R}^2$. This space possesses translation and rotation symmetry, meaning that if we translate or rotate the image, its content and structure remain unchanged. Convolutional Neural Networks (CNNs) exploit this translation symmetry, capturing local features through shared weights, thereby improving learning efficiency and generalization ability.\nSymmetry and Invariance In machine learning, symmetry refers to the property that data or tasks remain unchanged under certain transformations. These transformations can be translation, rotation, scaling, etc. For example, in an image classification task, if we translate or rotate an image, the category of the image usually does not change. This invariance is what we hope the model can capture and utilize.\nSymmetry Groups Symmetry can be formalized through group theory. A group is a set equipped with a binary operation that satisfies properties such as closure, associativity, identity, and inverse.\nSymmetry operations form a group, called a symmetry group, proven as follows:\nClosure: If $g_1$ and $g_2$ are symmetry operations, then their composition $g_1 \\circ g_2$ is also a symmetry operation, because applying two symmetry transformations consecutively still preserves the invariance of the data. Associativity: If $g_1$ and $g_2$ are symmetry operations, then for any $g_3$, $(g_1 \\circ g_2) \\circ g_3 = g_1 \\circ (g_2 \\circ g_3)$, because the order of transformations does not affect the final result. Identity: There exists an identity operation $e$ such that for any symmetry operation $g$, $e \\circ g = g \\circ e = g$. This identity operation corresponds to performing no transformation. Inverse: For every symmetry operation $g$, there exists an inverse operation $g^{-1}$ such that $g \\circ g^{-1} = g^{-1} \\circ g = e$. This inverse operation corresponds to undoing the transformation. Note: $g \\circ h$ denotes applying transformation $g$ first, then transformation $h$.\nLet\u0026rsquo;s take an equilateral triangle as an example to illustrate the concept of symmetry groups: In this example, the symmetry group of the equilateral triangle contains six elements: three rotation operations (0 degrees, 120 degrees, 240 degrees) and three reflection operations (perpendicular lines through each vertex). These operations satisfy the four properties of a group, thus forming a group called $D_3$, the dihedral group of the triangle. Mathematically, we can view these operations as mappings: $$g: \\mathcal{X} \\rightarrow \\mathcal{X}$$ where $\\mathcal{X}$ is the data space. For each element $g \\in G$ in a symmetry group $G$, we have a corresponding mapping $g$. For example, $D_3$ can also be represented as a matrix group: $$ D_3 = \\left\\{ \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 2 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 3 \u0026amp; 1 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 1 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 3 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 1 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 2 \u0026amp; 1 \\end{pmatrix} \\right\\} $$\nReferences Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., \u0026amp; Vandergheynst, P. (2017). Geometric deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4), 18-42. Goodfellow, I., Bengio, Y., \u0026amp; Courville, A. (2016). Deep learning. MIT press. Cohen, T. S., \u0026amp; Welling, M. (2016). Group equivariant convolutional networks. In International conference on machine learning (pp. 2990-2999). PMLR. Kondor, R., \u0026amp; Trivedi, S. (2018). On the generalization of equivariance and convolution in neural networks to the action of compact groups. In International conference on machine learning (pp. 2747-2755). PMLR. Wood, T., \u0026amp; Shawe-Taylor, J. (1996). Representation theory and invariant neural networks. Neural computation, 8(5), 1003-1013. Mallat, S. (2016). Understanding deep convolutional networks. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065), 20150203. Lee, J. M. (2013). Introduction to smooth manifolds. Springer Science \u0026amp; Business Media. ","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"","permalink":"http://localhost:1313/posts/geo_deeplearning1/","summary":"","title":"intro to Geo-deep-learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f} \\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$: $$ \\mathcal{R} = \\mathbb{E}_{(x,y) \\sim P}[L(\\widetilde{f}(x), f(x))] $$\nSuch constructed functions can approximate almost any function (Universal Approximation Theorem), but this does not mean we do not need inductive biases to constrain the learning problem. Given a function class $\\mathcal{F}$ with universal approximation capabilities, we can define a complexity function: $$c: \\mathcal{F} \\rightarrow \\mathbb{R}$$ defining our interpolation problem as: $$\\widetilde{f} = \\arg\\min_{g \\in \\mathcal{F}} c(g) \\quad s.t. \\quad \\widetilde{f}(x_i) = f(x_i), \\quad i = 1, . . . , N$$\nTherefore, we not only hope for good function fitting but also low function complexity. Such a complexity function is what we call the norm of the function. The benefit of this definition is that $\\mathcal{F}$ becomes a Banach space (a complete vector space equipped with a norm), allowing us to use tools from functional analysis to study their properties.\nGeometric Priors To overcome the \u0026ldquo;curse of dimensionality\u0026rdquo; of high-dimensional data, we must utilize the physical characteristics of the data (symmetry and scale separation); although the geometry of the data (Domain) may be complex, the signals defined on it constitute a Hilbert space with good mathematical properties, allowing us to still use tools from linear algebra and functional analysis to handle them.\nFor example, in image processing, data is typically represented as signals (pixel values) defined on a two-dimensional Euclidean space $\\mathbb{R}^2$. This space possesses translation and rotation symmetry, meaning that if we translate or rotate the image, its content and structure remain unchanged. Convolutional Neural Networks (CNNs) exploit this translation symmetry, capturing local features through shared weights, thereby improving learning efficiency and generalization ability.\nSymmetry and Invariance In machine learning, symmetry refers to the property that data or tasks remain unchanged under certain transformations. These transformations can be translation, rotation, scaling, etc. For example, in an image classification task, if we translate or rotate an image, the category of the image usually does not change. This invariance is what we hope the model can capture and utilize.\nSymmetry Groups Symmetry can be formalized through group theory. A group is a set equipped with a binary operation that satisfies properties such as closure, associativity, identity, and inverse.\nSymmetry operations form a group, called a symmetry group, proven as follows:\nClosure: If $g_1$ and $g_2$ are symmetry operations, then their composition $g_1 \\circ g_2$ is also a symmetry operation, because applying two symmetry transformations consecutively still preserves the invariance of the data. Associativity: If $g_1$ and $g_2$ are symmetry operations, then for any $g_3$, $(g_1 \\circ g_2) \\circ g_3 = g_1 \\circ (g_2 \\circ g_3)$, because the order of transformations does not affect the final result. Identity: There exists an identity operation $e$ such that for any symmetry operation $g$, $e \\circ g = g \\circ e = g$. This identity operation corresponds to performing no transformation. Inverse: For every symmetry operation $g$, there exists an inverse operation $g^{-1}$ such that $g \\circ g^{-1} = g^{-1} \\circ g = e$. This inverse operation corresponds to undoing the transformation. Note: $g \\circ h$ denotes applying transformation $g$ first, then transformation $h$.\nLet\u0026rsquo;s take an equilateral triangle as an example to illustrate the concept of symmetry groups: In this example, the symmetry group of the equilateral triangle contains six elements: three rotation operations (0 degrees, 120 degrees, 240 degrees) and three reflection operations (perpendicular lines through each vertex). These operations satisfy the four properties of a group, thus forming a group called $D_3$, the dihedral group of the triangle. Mathematically, we can view these operations as mappings: $$g: \\mathcal{X} \\rightarrow \\mathcal{X}$$ where $\\mathcal{X}$ is the data space. For each element $g \\in G$ in a symmetry group $G$, we have a corresponding mapping $g$. For example, $D_3$ can also be represented as a matrix group: $$ D_3 = \\left\\{ \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 2 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 3 \u0026amp; 1 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 1 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 3 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 1 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 2 \u0026amp; 1 \\end{pmatrix} \\right\\} $$\nReferences Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., \u0026amp; Vandergheynst, P. (2017). Geometric deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4), 18-42. Goodfellow, I., Bengio, Y., \u0026amp; Courville, A. (2016). Deep learning. MIT press. Cohen, T. S., \u0026amp; Welling, M. (2016). Group equivariant convolutional networks. In International conference on machine learning (pp. 2990-2999). PMLR. Kondor, R., \u0026amp; Trivedi, S. (2018). On the generalization of equivariance and convolution in neural networks to the action of compact groups. In International conference on machine learning (pp. 2747-2755). PMLR. Wood, T., \u0026amp; Shawe-Taylor, J. (1996). Representation theory and invariant neural networks. Neural computation, 8(5), 1003-1013. Mallat, S. (2016). Understanding deep convolutional networks. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065), 20150203. Lee, J. M. (2013). Introduction to smooth manifolds. Springer Science \u0026amp; Business Media. ","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"","permalink":"http://localhost:1313/posts/geo_deeplearning1/","summary":"","title":"intro to Geo-deep-learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f} \\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$: $$ \\mathcal{R} = \\mathbb{E}_{(x,y) \\sim P}[L(\\widetilde{f}(x), f(x))] $$\nSuch constructed functions can approximate almost any function (Universal Approximation Theorem), but this does not mean we do not need inductive biases to constrain the learning problem. Given a function class $\\mathcal{F}$ with universal approximation capabilities, we can define a complexity function: $$c: \\mathcal{F} \\rightarrow \\mathbb{R}$$ defining our interpolation problem as: $$\\widetilde{f} = \\arg\\min_{g \\in \\mathcal{F}} c(g) \\quad s.t. \\quad \\widetilde{f}(x_i) = f(x_i), \\quad i = 1, . . . , N$$\nTherefore, we not only hope for good function fitting but also low function complexity. Such a complexity function is what we call the norm of the function. The benefit of this definition is that $\\mathcal{F}$ becomes a Banach space (a complete vector space equipped with a norm), allowing us to use tools from functional analysis to study their properties.\nGeometric Priors To overcome the \u0026ldquo;curse of dimensionality\u0026rdquo; of high-dimensional data, we must utilize the physical characteristics of the data (symmetry and scale separation); although the geometry of the data (Domain) may be complex, the signals defined on it constitute a Hilbert space with good mathematical properties, allowing us to still use tools from linear algebra and functional analysis to handle them.\nFor example, in image processing, data is typically represented as signals (pixel values) defined on a two-dimensional Euclidean space $\\mathbb{R}^2$. This space possesses translation and rotation symmetry, meaning that if we translate or rotate the image, its content and structure remain unchanged. Convolutional Neural Networks (CNNs) exploit this translation symmetry, capturing local features through shared weights, thereby improving learning efficiency and generalization ability.\nSymmetry and Invariance In machine learning, symmetry refers to the property that data or tasks remain unchanged under certain transformations. These transformations can be translation, rotation, scaling, etc. For example, in an image classification task, if we translate or rotate an image, the category of the image usually does not change. This invariance is what we hope the model can capture and utilize.\nSymmetry Groups Symmetry can be formalized through group theory. A group is a set equipped with a binary operation that satisfies properties such as closure, associativity, identity, and inverse.\nSymmetry operations form a group, called a symmetry group, proven as follows:\nClosure: If $g_1$ and $g_2$ are symmetry operations, then their composition $g_1 \\circ g_2$ is also a symmetry operation, because applying two symmetry transformations consecutively still preserves the invariance of the data. Associativity: If $g_1$ and $g_2$ are symmetry operations, then for any $g_3$, $(g_1 \\circ g_2) \\circ g_3 = g_1 \\circ (g_2 \\circ g_3)$, because the order of transformations does not affect the final result. Identity: There exists an identity operation $e$ such that for any symmetry operation $g$, $e \\circ g = g \\circ e = g$. This identity operation corresponds to performing no transformation. Inverse: For every symmetry operation $g$, there exists an inverse operation $g^{-1}$ such that $g \\circ g^{-1} = g^{-1} \\circ g = e$. This inverse operation corresponds to undoing the transformation. Note: $g \\circ h$ denotes applying transformation $g$ first, then transformation $h$.\nLet\u0026rsquo;s take an equilateral triangle as an example to illustrate the concept of symmetry groups: In this example, the symmetry group of the equilateral triangle contains six elements: three rotation operations (0 degrees, 120 degrees, 240 degrees) and three reflection operations (perpendicular lines through each vertex). These operations satisfy the four properties of a group, thus forming a group called $D_3$, the dihedral group of the triangle. Mathematically, we can view these operations as mappings: $$g: \\mathcal{X} \\rightarrow \\mathcal{X}$$ where $\\mathcal{X}$ is the data space. For each element $g \\in G$ in a symmetry group $G$, we have a corresponding mapping $g$. For example, $D_3$ can also be represented as a matrix group: $$ D_3 = \\left\\{ \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 2 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 3 \u0026amp; 1 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 1 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 3 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 1 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 2 \u0026amp; 1 \\end{pmatrix} \\right\\} $$\nReferences Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., \u0026amp; Vandergheynst, P. (2017). Geometric deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4), 18-42. Goodfellow, I., Bengio, Y., \u0026amp; Courville, A. (2016). Deep learning. MIT press. Cohen, T. S., \u0026amp; Welling, M. (2016). Group equivariant convolutional networks. In International conference on machine learning (pp. 2990-2999). PMLR. Kondor, R., \u0026amp; Trivedi, S. (2018). On the generalization of equivariance and convolution in neural networks to the action of compact groups. In International conference on machine learning (pp. 2747-2755). PMLR. Wood, T., \u0026amp; Shawe-Taylor, J. (1996). Representation theory and invariant neural networks. Neural computation, 8(5), 1003-1013. Mallat, S. (2016). Understanding deep convolutional networks. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065), 20150203. Lee, J. M. (2013). Introduction to smooth manifolds. Springer Science \u0026amp; Business Media. ","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"","permalink":"http://localhost:1313/posts/geo_deeplearning1/","summary":"","title":"intro to Geo-deep-learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f} \\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$: $$ \\mathcal{R} = \\mathbb{E}_{(x,y) \\sim P}[L(\\widetilde{f}(x), f(x))] $$\nSuch constructed functions can approximate almost any function (Universal Approximation Theorem), but this does not mean we do not need inductive biases to constrain the learning problem. Given a function class $\\mathcal{F}$ with universal approximation capabilities, we can define a complexity function: $$c: \\mathcal{F} \\rightarrow \\mathbb{R}$$ defining our interpolation problem as: $$\\widetilde{f} = \\arg\\min_{g \\in \\mathcal{F}} c(g) \\quad s.t. \\quad \\widetilde{f}(x_i) = f(x_i), \\quad i = 1, . . . , N$$\nTherefore, we not only hope for good function fitting but also low function complexity. Such a complexity function is what we call the norm of the function. The benefit of this definition is that $\\mathcal{F}$ becomes a Banach space (a complete vector space equipped with a norm), allowing us to use tools from functional analysis to study their properties.\nGeometric Priors To overcome the \u0026ldquo;curse of dimensionality\u0026rdquo; of high-dimensional data, we must utilize the physical characteristics of the data (symmetry and scale separation); although the geometry of the data (Domain) may be complex, the signals defined on it constitute a Hilbert space with good mathematical properties, allowing us to still use tools from linear algebra and functional analysis to handle them.\nFor example, in image processing, data is typically represented as signals (pixel values) defined on a two-dimensional Euclidean space $\\mathbb{R}^2$. This space possesses translation and rotation symmetry, meaning that if we translate or rotate the image, its content and structure remain unchanged. Convolutional Neural Networks (CNNs) exploit this translation symmetry, capturing local features through shared weights, thereby improving learning efficiency and generalization ability.\nSymmetry and Invariance In machine learning, symmetry refers to the property that data or tasks remain unchanged under certain transformations. These transformations can be translation, rotation, scaling, etc. For example, in an image classification task, if we translate or rotate an image, the category of the image usually does not change. This invariance is what we hope the model can capture and utilize.\nSymmetry Groups Symmetry can be formalized through group theory. A group is a set equipped with a binary operation that satisfies properties such as closure, associativity, identity, and inverse.\nSymmetry operations form a group, called a symmetry group, proven as follows:\nClosure: If $g_1$ and $g_2$ are symmetry operations, then their composition $g_1 \\circ g_2$ is also a symmetry operation, because applying two symmetry transformations consecutively still preserves the invariance of the data. Associativity: If $g_1$ and $g_2$ are symmetry operations, then for any $g_3$, $(g_1 \\circ g_2) \\circ g_3 = g_1 \\circ (g_2 \\circ g_3)$, because the order of transformations does not affect the final result. Identity: There exists an identity operation $e$ such that for any symmetry operation $g$, $e \\circ g = g \\circ e = g$. This identity operation corresponds to performing no transformation. Inverse: For every symmetry operation $g$, there exists an inverse operation $g^{-1}$ such that $g \\circ g^{-1} = g^{-1} \\circ g = e$. This inverse operation corresponds to undoing the transformation. Note: $g \\circ h$ denotes applying transformation $g$ first, then transformation $h$.\nLet\u0026rsquo;s take an equilateral triangle as an example to illustrate the concept of symmetry groups: In this example, the symmetry group of the equilateral triangle contains six elements: three rotation operations (0 degrees, 120 degrees, 240 degrees) and three reflection operations (perpendicular lines through each vertex). These operations satisfy the four properties of a group, thus forming a group called $D_3$, the dihedral group of the triangle. Mathematically, we can view these operations as mappings: $$g: \\mathcal{X} \\rightarrow \\mathcal{X}$$ where $\\mathcal{X}$ is the data space. For each element $g \\in G$ in a symmetry group $G$, we have a corresponding mapping $g$. For example, $D_3$ can also be represented as a matrix group: $$ D_3 = \\left\\{ \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 2 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 3 \u0026amp; 1 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 1 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 3 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 1 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 2 \u0026amp; 1 \\end{pmatrix} \\right\\} $$\nReferences Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., \u0026amp; Vandergheynst, P. (2017). Geometric deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4), 18-42. Goodfellow, I., Bengio, Y., \u0026amp; Courville, A. (2016). Deep learning. MIT press. Cohen, T. S., \u0026amp; Welling, M. (2016). Group equivariant convolutional networks. In International conference on machine learning (pp. 2990-2999). PMLR. Kondor, R., \u0026amp; Trivedi, S. (2018). On the generalization of equivariance and convolution in neural networks to the action of compact groups. In International conference on machine learning (pp. 2747-2755). PMLR. Wood, T., \u0026amp; Shawe-Taylor, J. (1996). Representation theory and invariant neural networks. Neural computation, 8(5), 1003-1013. Mallat, S. (2016). Understanding deep convolutional networks. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065), 20150203. Lee, J. M. (2013). Introduction to smooth manifolds. Springer Science \u0026amp; Business Media. ","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"","permalink":"http://localhost:1313/posts/geo_deeplearning1/","summary":"","title":"intro to Geo-deep-learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f} \\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$: $$ \\mathcal{R} = \\mathbb{E}_{(x,y) \\sim P}[L(\\widetilde{f}(x), f(x))] $$\nSuch constructed functions can approximate almost any function (Universal Approximation Theorem), but this does not mean we do not need inductive biases to constrain the learning problem. Given a function class $\\mathcal{F}$ with universal approximation capabilities, we can define a complexity function: $$c: \\mathcal{F} \\rightarrow \\mathbb{R}$$ defining our interpolation problem as: $$\\widetilde{f} = \\arg\\min_{g \\in \\mathcal{F}} c(g) \\quad s.t. \\quad \\widetilde{f}(x_i) = f(x_i), \\quad i = 1, . . . , N$$\nTherefore, we not only hope for good function fitting but also low function complexity. Such a complexity function is what we call the norm of the function. The benefit of this definition is that $\\mathcal{F}$ becomes a Banach space (a complete vector space equipped with a norm), allowing us to use tools from functional analysis to study their properties.\nGeometric Priors To overcome the \u0026ldquo;curse of dimensionality\u0026rdquo; of high-dimensional data, we must utilize the physical characteristics of the data (symmetry and scale separation); although the geometry of the data (Domain) may be complex, the signals defined on it constitute a Hilbert space with good mathematical properties, allowing us to still use tools from linear algebra and functional analysis to handle them.\nFor example, in image processing, data is typically represented as signals (pixel values) defined on a two-dimensional Euclidean space $\\mathbb{R}^2$. This space possesses translation and rotation symmetry, meaning that if we translate or rotate the image, its content and structure remain unchanged. Convolutional Neural Networks (CNNs) exploit this translation symmetry, capturing local features through shared weights, thereby improving learning efficiency and generalization ability.\nSymmetry and Invariance In machine learning, symmetry refers to the property that data or tasks remain unchanged under certain transformations. These transformations can be translation, rotation, scaling, etc. For example, in an image classification task, if we translate or rotate an image, the category of the image usually does not change. This invariance is what we hope the model can capture and utilize.\nSymmetry Groups Symmetry can be formalized through group theory. A group is a set equipped with a binary operation that satisfies properties such as closure, associativity, identity, and inverse.\nSymmetry operations form a group, called a symmetry group, proven as follows:\nClosure: If $g_1$ and $g_2$ are symmetry operations, then their composition $g_1 \\circ g_2$ is also a symmetry operation, because applying two symmetry transformations consecutively still preserves the invariance of the data. Associativity: If $g_1$ and $g_2$ are symmetry operations, then for any $g_3$, $(g_1 \\circ g_2) \\circ g_3 = g_1 \\circ (g_2 \\circ g_3)$, because the order of transformations does not affect the final result. Identity: There exists an identity operation $e$ such that for any symmetry operation $g$, $e \\circ g = g \\circ e = g$. This identity operation corresponds to performing no transformation. Inverse: For every symmetry operation $g$, there exists an inverse operation $g^{-1}$ such that $g \\circ g^{-1} = g^{-1} \\circ g = e$. This inverse operation corresponds to undoing the transformation. Note: $g \\circ h$ denotes applying transformation $g$ first, then transformation $h$.\nLet\u0026rsquo;s take an equilateral triangle as an example to illustrate the concept of symmetry groups: In this example, the symmetry group of the equilateral triangle contains six elements: three rotation operations (0 degrees, 120 degrees, 240 degrees) and three reflection operations (perpendicular lines through each vertex). These operations satisfy the four properties of a group, thus forming a group called $D_3$, the dihedral group of the triangle. Mathematically, we can view these operations as mappings: $$g: \\mathcal{X} \\rightarrow \\mathcal{X}$$ where $\\mathcal{X}$ is the data space. For each element $g \\in G$ in a symmetry group $G$, we have a corresponding mapping $g$. For example, $D_3$ can also be represented as a matrix group: $$ D_3 = \\left\\{ \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 2 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 3 \u0026amp; 1 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 1 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 3 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 1 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 2 \u0026amp; 1 \\end{pmatrix} \\right\\} $$\nReferences Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., \u0026amp; Vandergheynst, P. (2017). Geometric deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4), 18-42. Goodfellow, I., Bengio, Y., \u0026amp; Courville, A. (2016). Deep learning. MIT press. Cohen, T. S., \u0026amp; Welling, M. (2016). Group equivariant convolutional networks. In International conference on machine learning (pp. 2990-2999). PMLR. Kondor, R., \u0026amp; Trivedi, S. (2018). On the generalization of equivariance and convolution in neural networks to the action of compact groups. In International conference on machine learning (pp. 2747-2755). PMLR. Wood, T., \u0026amp; Shawe-Taylor, J. (1996). Representation theory and invariant neural networks. Neural computation, 8(5), 1003-1013. Mallat, S. (2016). Understanding deep convolutional networks. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065), 20150203. Lee, J. M. (2013). Introduction to smooth manifolds. Springer Science \u0026amp; Business Media. ","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"","permalink":"http://localhost:1313/posts/geo_deeplearning1/","summary":"","title":"intro to Geo-deep-learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f} \\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$: $$ \\mathcal{R} = \\mathbb{E}_{(x,y) \\sim P}[L(\\widetilde{f}(x), f(x))] $$\nSuch constructed functions can approximate almost any function (Universal Approximation Theorem), but this does not mean we do not need inductive biases to constrain the learning problem. Given a function class $\\mathcal{F}$ with universal approximation capabilities, we can define a complexity function: $$c: \\mathcal{F} \\rightarrow \\mathbb{R}$$ defining our interpolation problem as: $$\\widetilde{f} = \\arg\\min_{g \\in \\mathcal{F}} c(g) \\quad s.t. \\quad \\widetilde{f}(x_i) = f(x_i), \\quad i = 1, . . . , N$$\nTherefore, we not only hope for good function fitting but also low function complexity. Such a complexity function is what we call the norm of the function. The benefit of this definition is that $\\mathcal{F}$ becomes a Banach space (a complete vector space equipped with a norm), allowing us to use tools from functional analysis to study their properties.\nGeometric Priors To overcome the \u0026ldquo;curse of dimensionality\u0026rdquo; of high-dimensional data, we must utilize the physical characteristics of the data (symmetry and scale separation); although the geometry of the data (Domain) may be complex, the signals defined on it constitute a Hilbert space with good mathematical properties, allowing us to still use tools from linear algebra and functional analysis to handle them.\nFor example, in image processing, data is typically represented as signals (pixel values) defined on a two-dimensional Euclidean space $\\mathbb{R}^2$. This space possesses translation and rotation symmetry, meaning that if we translate or rotate the image, its content and structure remain unchanged. Convolutional Neural Networks (CNNs) exploit this translation symmetry, capturing local features through shared weights, thereby improving learning efficiency and generalization ability.\nSymmetry and Invariance In machine learning, symmetry refers to the property that data or tasks remain unchanged under certain transformations. These transformations can be translation, rotation, scaling, etc. For example, in an image classification task, if we translate or rotate an image, the category of the image usually does not change. This invariance is what we hope the model can capture and utilize.\nSymmetry Groups Symmetry can be formalized through group theory. A group is a set equipped with a binary operation that satisfies properties such as closure, associativity, identity, and inverse.\nSymmetry operations form a group, called a symmetry group, proven as follows:\nClosure: If $g_1$ and $g_2$ are symmetry operations, then their composition $g_1 \\circ g_2$ is also a symmetry operation, because applying two symmetry transformations consecutively still preserves the invariance of the data. Associativity: If $g_1$ and $g_2$ are symmetry operations, then for any $g_3$, $(g_1 \\circ g_2) \\circ g_3 = g_1 \\circ (g_2 \\circ g_3)$, because the order of transformations does not affect the final result. Identity: There exists an identity operation $e$ such that for any symmetry operation $g$, $e \\circ g = g \\circ e = g$. This identity operation corresponds to performing no transformation. Inverse: For every symmetry operation $g$, there exists an inverse operation $g^{-1}$ such that $g \\circ g^{-1} = g^{-1} \\circ g = e$. This inverse operation corresponds to undoing the transformation. Note: $g \\circ h$ denotes applying transformation $g$ first, then transformation $h$.\nLet\u0026rsquo;s take an equilateral triangle as an example to illustrate the concept of symmetry groups: In this example, the symmetry group of the equilateral triangle contains six elements: three rotation operations (0 degrees, 120 degrees, 240 degrees) and three reflection operations (perpendicular lines through each vertex). These operations satisfy the four properties of a group, thus forming a group called $D_3$, the dihedral group of the triangle. Mathematically, we can view these operations as mappings: $$g: \\mathcal{X} \\rightarrow \\mathcal{X}$$ where $\\mathcal{X}$ is the data space. For each element $g \\in G$ in a symmetry group $G$, we have a corresponding mapping $g$. For example, $D_3$ can also be represented as a matrix group: $$ D_3 = \\left\\{ \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 2 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 3 \u0026amp; 1 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 1 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 3 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 1 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 2 \u0026amp; 1 \\end{pmatrix} \\right\\} $$\nReferences Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., \u0026amp; Vandergheynst, P. (2017). Geometric deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4), 18-42. Goodfellow, I., Bengio, Y., \u0026amp; Courville, A. (2016). Deep learning. MIT press. Cohen, T. S., \u0026amp; Welling, M. (2016). Group equivariant convolutional networks. In International conference on machine learning (pp. 2990-2999). PMLR. Kondor, R., \u0026amp; Trivedi, S. (2018). On the generalization of equivariance and convolution in neural networks to the action of compact groups. In International conference on machine learning (pp. 2747-2755). PMLR. Wood, T., \u0026amp; Shawe-Taylor, J. (1996). Representation theory and invariant neural networks. Neural computation, 8(5), 1003-1013. Mallat, S. (2016). Understanding deep convolutional networks. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065), 20150203. Lee, J. M. (2013). Introduction to smooth manifolds. Springer Science \u0026amp; Business Media. ","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"Group Actions In geometric deep learning, we are often more concerned with how groups act on our data. We assume the data comes from some domain $\\Omega$. We denote the group as $\\mathfrak{G}$, and define the group action on the domain $\\Omega$ as a map: $$ (\\mathfrak{g},u) \\mapsto \\mathfrak{g}.u$$ As one might imagine, when we apply $\\mathfrak{G}$ to $\\Omega$, we simultaneously obtain the action of $\\mathfrak{G}$ on $\\mathcal{X}(\\Omega)$, given by: $$\\mathfrak{g}.x(u) = x(\\mathfrak{g}^{-1}.u)$$\nThe group actions we will encounter most frequently are linear group actions, which satisfy: $$\\mathfrak{g}.(\\alpha x + \\beta x\u0026rsquo;) = \\alpha (\\mathfrak{g}.x) + \\beta (\\mathfrak{g}.x\u0026rsquo;)$$\nCurrying Original definition: View the transformation as a function taking two arguments: $f(\\mathfrak{g},x) \\mapsto \\mathfrak{g}.x$ (input an action and a signal, output the transformed signal). After Currying: View it as a function taking only one argument $\\mathfrak{g}$, but returning another function $\\rho(\\mathfrak{g})$. This $\\rho(\\mathfrak{g})$ is specifically responsible for processing the signal $x$. Mathematically, this $\\rho$ is called a Group Representation.\nFormal Definition of Group Representation An $n$-dimensional real representation is a map $\\rho: \\mathfrak{G} \\to \\mathbb{R}^{n \\times n}$ that must satisfy the following homomorphism condition: $$\\rho(\\mathfrak{gh}) = \\rho(\\mathfrak{g})\\rho(\\mathfrak{h})$$\nThis ensures that the algebraic structure of the group (the order of actions) is perfectly preserved in matrix operations. Unitary/Orthogonal Representations: If $\\rho(\\mathfrak{g})$ is always a unitary or orthogonal matrix, the representation is called a unitary or orthogonal representation. Such representations are very important in physics and stability analysis because they preserve the norm of the signal. Thus, written in the language of group representations, the action of $\\mathfrak{G}$ on $x \\in \\mathcal{X}(\\Omega)$ can also be defined as $\\rho (\\mathfrak{g})x(u) = x(\\mathfrak{g}^{-1}.u)$.\nInvariance and Equivariant Functions Since the domain $\\Omega$ of the data distribution possesses symmetry, this provides us with a strong inductive bias, reducing the need for unnecessary interpolation.\nIf $f(\\rho(\\mathfrak{g})x) = f(x), \\forall x \\in \\mathcal{X}(\\Omega), \\mathfrak{g} \\in \\mathfrak{G}$, we say that the function $f$ is $\\mathfrak{G}$-invariant. In other words, the group action does not affect the output result.\nThe figure above clearly illustrates the relationship between group representations, symmetry groups, signals, domains, and equivariant functions.\nIf $f(\\rho(\\mathfrak{g})x) = \\rho(\\mathfrak{g})f(x), \\forall x \\in \\mathcal{X}(\\Omega), \\mathfrak{g} \\in \\mathfrak{G}$, we say that the function $f$ is $\\mathfrak{G}$-equivariant. That is, applying the group action to the input or the output makes no difference.\nTaking CNNs (Convolutional Neural Networks) as an example, let $\\mathfrak{t} \\in \\mathfrak{T}$ be a translation operator. Clearly, we have: $$conv(\\mathfrak{t} * x) = \\mathfrak{t} * conv(x)$$ This is because the convolution kernel is globally shared across translations (the convolution operation involves the kernel sliding across the image acting on pixels). Here, the convolution operation is what we call $f$, meaning $f$ is $\\mathfrak{T}$-equivariant.\nIsomorphism and Automorphism Set Level View the domain $\\Omega$ as a set. Its only attribute is \u0026ldquo;Cardinality\u0026rdquo;, i.e., how many elements are in the set. Structure-preserving map: Bijection.\nTopological Level $\\Omega$ is a topological space, and we start to care about \u0026ldquo;proximity\u0026rdquo; between points. Structure-preserving map: Homeomorphism.\nDifferential Level $\\Omega$ is a differentiable manifold. We require not only continuity but also the ability to perform calculus. Structure-preserving map: Diffeomorphism, denoted as Diff($\\Omega$).\nAs the level increases, the symmetry group becomes smaller. In fact, increasing the level is equivalent to finding subgroups (a subset satisfying group operation rules).\nDeformation Stability Stability to Signal Deformations When dealing with signals (such as images $x$), we intuitively believe that slight deformations should not change the output $f(x)$. However, we face two dilemmas in mathematical definitions:\nSmall deformations do not form a \u0026ldquo;group\u0026rdquo;: Although small deformations look like some kind of symmetry, composing multiple small deformations can result in a large deformation. Mathematically, these \u0026ldquo;small deformations\u0026rdquo; themselves do not constitute a closed transformation group.\nThe full diffeomorphism group is too strong: If we require invariance under all differentiable transformations ($Diff(\\Omega)$), this is too excessive. Because drastic deformations change the semantics of an image (e.g., stretching the digit \u0026ldquo;3\u0026rdquo; into an \u0026ldquo;8\u0026rdquo;), we do not want the model to be \u0026ldquo;unresponsive\u0026rdquo; to such drastic changes.\nTherefore, we no longer pursue absolute Invariance, but rather Geometric Stability. The core idea is: the degree of change in the output should be bounded by the \u0026ldquo;magnitude\u0026rdquo; of the deformation. $$|| f(\\rho(\\tau)x) - f(x)|| \\leq C c(\\tau) ||x||$$ $c(\\tau)$: The \u0026ldquo;complexity\u0026rdquo; or \u0026ldquo;magnitude\u0026rdquo; of the deformation. Here, we usually assume $\\tau$ is a diffeomorphism. $C$: A constant. Intuitive understanding: If the deformation $\\tau$ is very small (belonging to some symmetry subgroup $G$, such as translation, where $c(\\tau)=0$), then the output remains almost unchanged. If the deformation is large, the output is allowed to change accordingly, but we know the upper bound of this change.\nHow to measure the magnitude of deformation? (Dirichlet Energy) The second image gives a specific measurement method, namely Dirichlet energy.\nFor images defined on the Euclidean plane, if we view the deformation as a displacement field $\\tau(u)$ (i.e., point $u$ is moved to $u+\\tau(u)$), then the cost of deformation can be defined as: $$c^2(\\tau) := \\int_{\\Omega}||\\nabla\\tau(u)||^2du$$ This metric measures the Elasticity of $\\tau$. It actually measures the degree to which the deformation deviates from a \u0026ldquo;constant vector field (i.e., pure translation)\u0026rdquo;. If $\\tau$ is just a simple translation (constant displacement), $\\nabla\\tau$ is 0, so $c(\\tau)$ is 0; if $\\tau$ causes drastic local distortion in the image, $\\nabla\\tau$ will be large, leading to an increased penalty term.\nThe figure above shows the relationship between deformations of different degrees. $Aut(\\Omega)$ is the automorphism group. Requiring $\\mathfrak{G}$-invariance or $\\mathfrak{G}$-equivariance is too harsh for images and not very applicable to real-world situations. The reason we pursue geometric stability is to find a larger group $\\mathfrak{G}\u0026rsquo;$ that satisfies geometric stability.\nStability to Domain Deformations What if the domain of the data itself changes? Application scenarios:\nGraphs: Social networks add or remove relationships (edges) over time. Manifolds: A 3D character performing non-rigid motion (like bending over), where the geometric structure of its surface changes. Domain distance $d(\\Omega, \\tilde{\\Omega})$: To measure how \u0026ldquo;similar\u0026rdquo; two domains are. Metrics between domains are introduced: For graphs, Graph Edit Distance can be used. For manifolds, Gromov-Hausdorff distance can be used. Domain stability formula: Formula (5) is a generalization of signal stability: $$ |f(x, \\Omega) - f(\\tilde{x}, \\tilde{\\Omega})| \\leq C |x| d_{\\mathcal{D}}(\\Omega, \\tilde{\\Omega}) $$ This means: If two graphs or two manifold structures are very close, the results obtained by the model processing them should also be very close.\nWe will discuss stability regarding domain deformations in future posts.\nReferences Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., \u0026amp; Vandergheynst, P. (2017). Geometric deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4), 18-42. Cohen, T. S., \u0026amp; Welling, M. (2016). Group equivariant convolutional networks. In International conference on machine learning (pp. 2990-2999). PMLR. Mallat, S. (2012). Group invariant scattering. Communications on Pure and Applied Mathematics, 65(10), 1331-1398. Kondor, R., \u0026amp; Trivedi, S. (2018). On the generalization of equivariance and convolution in neural networks to the action of compact groups. In International conference on machine learning (pp. 2747-2755). PMLR. ","permalink":"http://localhost:1313/posts/geo_deeplearning1/","summary":"\u003ch2 id=\"group-actions\"\u003eGroup Actions\u003c/h2\u003e\n\u003cp\u003eIn geometric deep learning, we are often more concerned with how groups act on our data. We assume the data comes from some domain $\\Omega$.\nWe denote the group as $\\mathfrak{G}$, and define the group action on the domain $\\Omega$ as a map:\n$$ (\\mathfrak{g},u) \\mapsto \\mathfrak{g}.u$$\nAs one might imagine, when we apply $\\mathfrak{G}$ to $\\Omega$, we simultaneously obtain the action of $\\mathfrak{G}$ on $\\mathcal{X}(\\Omega)$, given by:\n$$\\mathfrak{g}.x(u) = x(\\mathfrak{g}^{-1}.u)$$\u003c/p\u003e","title":"Introduction to Geometric Deep Learning (Continued)"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f} \\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$: $$ \\mathcal{R} = \\mathbb{E}_{(x,y) \\sim P}[L(\\widetilde{f}(x), f(x))] $$\nSuch constructed functions can approximate almost any function (Universal Approximation Theorem), but this does not mean we do not need inductive biases to constrain the learning problem. Given a function class $\\mathcal{F}$ with universal approximation capabilities, we can define a complexity function: $$c: \\mathcal{F} \\rightarrow \\mathbb{R}$$ defining our interpolation problem as: $$\\widetilde{f} = \\arg\\min_{g \\in \\mathcal{F}} c(g) \\quad s.t. \\quad \\widetilde{f}(x_i) = f(x_i), \\quad i = 1, . . . , N$$\nTherefore, we not only hope for good function fitting but also low function complexity. Such a complexity function is what we call the norm of the function. The benefit of this definition is that $\\mathcal{F}$ becomes a Banach space (a complete vector space equipped with a norm), allowing us to use tools from functional analysis to study their properties.\nGeometric Priors To overcome the \u0026ldquo;curse of dimensionality\u0026rdquo; of high-dimensional data, we must utilize the physical characteristics of the data (symmetry and scale separation); although the geometry of the data (Domain) may be complex, the signals defined on it constitute a Hilbert space with good mathematical properties, allowing us to still use tools from linear algebra and functional analysis to handle them.\nFor example, in image processing, data is typically represented as signals (pixel values) defined on a two-dimensional Euclidean space $\\mathbb{R}^2$. This space possesses translation and rotation symmetry, meaning that if we translate or rotate the image, its content and structure remain unchanged. Convolutional Neural Networks (CNNs) exploit this translation symmetry, capturing local features through shared weights, thereby improving learning efficiency and generalization ability.\nSymmetry and Invariance In machine learning, symmetry refers to the property that data or tasks remain unchanged under certain transformations. These transformations can be translation, rotation, scaling, etc. For example, in an image classification task, if we translate or rotate an image, the category of the image usually does not change. This invariance is what we hope the model can capture and utilize.\nSymmetry Groups Symmetry can be formalized through group theory. A group is a set equipped with a binary operation that satisfies properties such as closure, associativity, identity, and inverse.\nSymmetry operations form a group, called a symmetry group, proven as follows:\nClosure: If $g_1$ and $g_2$ are symmetry operations, then their composition $g_1 \\circ g_2$ is also a symmetry operation, because applying two symmetry transformations consecutively still preserves the invariance of the data. Associativity: If $g_1$ and $g_2$ are symmetry operations, then for any $g_3$, $(g_1 \\circ g_2) \\circ g_3 = g_1 \\circ (g_2 \\circ g_3)$, because the order of transformations does not affect the final result. Identity: There exists an identity operation $e$ such that for any symmetry operation $g$, $e \\circ g = g \\circ e = g$. This identity operation corresponds to performing no transformation. Inverse: For every symmetry operation $g$, there exists an inverse operation $g^{-1}$ such that $g \\circ g^{-1} = g^{-1} \\circ g = e$. This inverse operation corresponds to undoing the transformation. Note: $g \\circ h$ denotes applying transformation $g$ first, then transformation $h$.\nLet\u0026rsquo;s take an equilateral triangle as an example to illustrate the concept of symmetry groups: In this example, the symmetry group of the equilateral triangle contains six elements: three rotation operations (0 degrees, 120 degrees, 240 degrees) and three reflection operations (perpendicular lines through each vertex). These operations satisfy the four properties of a group, thus forming a group called $D_3$, the dihedral group of the triangle. Mathematically, we can view these operations as mappings: $$g: \\mathcal{X} \\rightarrow \\mathcal{X}$$ where $\\mathcal{X}$ is the data space. For each element $g \\in G$ in a symmetry group $G$, we have a corresponding mapping $g$. For example, $D_3$ can also be represented as a matrix group: $$ D_3 = \\left\\{ \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 2 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 3 \u0026amp; 1 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 1 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 3 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 1 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 2 \u0026amp; 1 \\end{pmatrix} \\right\\} $$\nReferences Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., \u0026amp; Vandergheynst, P. (2017). Geometric deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4), 18-42. Goodfellow, I., Bengio, Y., \u0026amp; Courville, A. (2016). Deep learning. MIT press. Cohen, T. S., \u0026amp; Welling, M. (2016). Group equivariant convolutional networks. In International conference on machine learning (pp. 2990-2999). PMLR. Kondor, R., \u0026amp; Trivedi, S. (2018). On the generalization of equivariance and convolution in neural networks to the action of compact groups. In International conference on machine learning (pp. 2747-2755). PMLR. Wood, T., \u0026amp; Shawe-Taylor, J. (1996). Representation theory and invariant neural networks. Neural computation, 8(5), 1003-1013. Mallat, S. (2016). Understanding deep convolutional networks. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065), 20150203. Lee, J. M. (2013). Introduction to smooth manifolds. Springer Science \u0026amp; Business Media. ","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"Group Actions In geometric deep learning, we are often more concerned with how groups act on our data. We assume the data comes from some domain $\\Omega$. We denote the group as $\\mathfrak{G}$, and define the group action on the domain $\\Omega$ as a map: $$ (\\mathfrak{g},u) \\mapsto \\mathfrak{g}.u$$ As one might imagine, when we apply $\\mathfrak{G}$ to $\\Omega$, we simultaneously obtain the action of $\\mathfrak{G}$ on $\\mathcal{X}(\\Omega)$, given by: $$\\mathfrak{g}.x(u) = x(\\mathfrak{g}^{-1}.u)$$\nThe group actions we will encounter most frequently are linear group actions, which satisfy: $$\\mathfrak{g}.(\\alpha x + \\beta x\u0026rsquo;) = \\alpha (\\mathfrak{g}.x) + \\beta (\\mathfrak{g}.x\u0026rsquo;)$$\nCurrying Original definition: View the transformation as a function taking two arguments: $f(\\mathfrak{g},x) \\mapsto \\mathfrak{g}.x$ (input an action and a signal, output the transformed signal). After Currying: View it as a function taking only one argument $\\mathfrak{g}$, but returning another function $\\rho(\\mathfrak{g})$. This $\\rho(\\mathfrak{g})$ is specifically responsible for processing the signal $x$. Mathematically, this $\\rho$ is called a Group Representation.\nFormal Definition of Group Representation An $n$-dimensional real representation is a map $\\rho: \\mathfrak{G} \\to \\mathbb{R}^{n \\times n}$ that must satisfy the following homomorphism condition: $$\\rho(\\mathfrak{gh}) = \\rho(\\mathfrak{g})\\rho(\\mathfrak{h})$$\nThis ensures that the algebraic structure of the group (the order of actions) is perfectly preserved in matrix operations. Unitary/Orthogonal Representations: If $\\rho(\\mathfrak{g})$ is always a unitary or orthogonal matrix, the representation is called a unitary or orthogonal representation. Such representations are very important in physics and stability analysis because they preserve the norm of the signal. Thus, written in the language of group representations, the action of $\\mathfrak{G}$ on $x \\in \\mathcal{X}(\\Omega)$ can also be defined as $\\rho (\\mathfrak{g})x(u) = x(\\mathfrak{g}^{-1}.u)$.\nInvariance and Equivariant Functions Since the domain $\\Omega$ of the data distribution possesses symmetry, this provides us with a strong inductive bias, reducing the need for unnecessary interpolation.\nIf $f(\\rho(\\mathfrak{g})x) = f(x), \\forall x \\in \\mathcal{X}(\\Omega), \\mathfrak{g} \\in \\mathfrak{G}$, we say that the function $f$ is $\\mathfrak{G}$-invariant. In other words, the group action does not affect the output result.\nThe figure above clearly illustrates the relationship between group representations, symmetry groups, signals, domains, and equivariant functions.\nIf $f(\\rho(\\mathfrak{g})x) = \\rho(\\mathfrak{g})f(x), \\forall x \\in \\mathcal{X}(\\Omega), \\mathfrak{g} \\in \\mathfrak{G}$, we say that the function $f$ is $\\mathfrak{G}$-equivariant. That is, applying the group action to the input or the output makes no difference.\nTaking CNNs (Convolutional Neural Networks) as an example, let $\\mathfrak{t} \\in \\mathfrak{T}$ be a translation operator. Clearly, we have: $$conv(\\mathfrak{t} * x) = \\mathfrak{t} * conv(x)$$ This is because the convolution kernel is globally shared across translations (the convolution operation involves the kernel sliding across the image acting on pixels). Here, the convolution operation is what we call $f$, meaning $f$ is $\\mathfrak{T}$-equivariant.\nIsomorphism and Automorphism Set Level View the domain $\\Omega$ as a set. Its only attribute is \u0026ldquo;Cardinality\u0026rdquo;, i.e., how many elements are in the set. Structure-preserving map: Bijection.\nTopological Level $\\Omega$ is a topological space, and we start to care about \u0026ldquo;proximity\u0026rdquo; between points. Structure-preserving map: Homeomorphism.\nDifferential Level $\\Omega$ is a differentiable manifold. We require not only continuity but also the ability to perform calculus. Structure-preserving map: Diffeomorphism, denoted as Diff($\\Omega$).\nAs the level increases, the symmetry group becomes smaller. In fact, increasing the level is equivalent to finding subgroups (a subset satisfying group operation rules).\nDeformation Stability Stability to Signal Deformations When dealing with signals (such as images $x$), we intuitively believe that slight deformations should not change the output $f(x)$. However, we face two dilemmas in mathematical definitions:\nSmall deformations do not form a \u0026ldquo;group\u0026rdquo;: Although small deformations look like some kind of symmetry, composing multiple small deformations can result in a large deformation. Mathematically, these \u0026ldquo;small deformations\u0026rdquo; themselves do not constitute a closed transformation group.\nThe full diffeomorphism group is too strong: If we require invariance under all differentiable transformations ($Diff(\\Omega)$), this is too excessive. Because drastic deformations change the semantics of an image (e.g., stretching the digit \u0026ldquo;3\u0026rdquo; into an \u0026ldquo;8\u0026rdquo;), we do not want the model to be \u0026ldquo;unresponsive\u0026rdquo; to such drastic changes.\nTherefore, we no longer pursue absolute Invariance, but rather Geometric Stability. The core idea is: the degree of change in the output should be bounded by the \u0026ldquo;magnitude\u0026rdquo; of the deformation. $$|| f(\\rho(\\tau)x) - f(x)|| \\leq C c(\\tau) ||x||$$ $c(\\tau)$: The \u0026ldquo;complexity\u0026rdquo; or \u0026ldquo;magnitude\u0026rdquo; of the deformation. Here, we usually assume $\\tau$ is a diffeomorphism. $C$: A constant. Intuitive understanding: If the deformation $\\tau$ is very small (belonging to some symmetry subgroup $G$, such as translation, where $c(\\tau)=0$), then the output remains almost unchanged. If the deformation is large, the output is allowed to change accordingly, but we know the upper bound of this change.\nHow to measure the magnitude of deformation? (Dirichlet Energy) The second image gives a specific measurement method, namely Dirichlet energy.\nFor images defined on the Euclidean plane, if we view the deformation as a displacement field $\\tau(u)$ (i.e., point $u$ is moved to $u+\\tau(u)$), then the cost of deformation can be defined as: $$c^2(\\tau) := \\int_{\\Omega}||\\nabla\\tau(u)||^2du$$ This metric measures the Elasticity of $\\tau$. It actually measures the degree to which the deformation deviates from a \u0026ldquo;constant vector field (i.e., pure translation)\u0026rdquo;. If $\\tau$ is just a simple translation (constant displacement), $\\nabla\\tau$ is 0, so $c(\\tau)$ is 0; if $\\tau$ causes drastic local distortion in the image, $\\nabla\\tau$ will be large, leading to an increased penalty term.\nThe figure above shows the relationship between deformations of different degrees. $Aut(\\Omega)$ is the automorphism group. Requiring $\\mathfrak{G}$-invariance or $\\mathfrak{G}$-equivariance is too harsh for images and not very applicable to real-world situations. The reason we pursue geometric stability is to find a larger group $\\mathfrak{G}\u0026rsquo;$ that satisfies geometric stability.\nStability to Domain Deformations What if the domain of the data itself changes? Application scenarios:\nGraphs: Social networks add or remove relationships (edges) over time. Manifolds: A 3D character performing non-rigid motion (like bending over), where the geometric structure of its surface changes. Domain distance $d(\\Omega, \\tilde{\\Omega})$: To measure how \u0026ldquo;similar\u0026rdquo; two domains are. Metrics between domains are introduced: For graphs, Graph Edit Distance can be used. For manifolds, Gromov-Hausdorff distance can be used. Domain stability formula: Formula (5) is a generalization of signal stability: $$ |f(x, \\Omega) - f(\\tilde{x}, \\tilde{\\Omega})| \\leq C |x| d_{\\mathcal{D}}(\\Omega, \\tilde{\\Omega}) $$ This means: If two graphs or two manifold structures are very close, the results obtained by the model processing them should also be very close.\nWe will discuss stability regarding domain deformations in future posts.\nReferences Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., \u0026amp; Vandergheynst, P. (2017). Geometric deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4), 18-42. Cohen, T. S., \u0026amp; Welling, M. (2016). Group equivariant convolutional networks. In International conference on machine learning (pp. 2990-2999). PMLR. Mallat, S. (2012). Group invariant scattering. Communications on Pure and Applied Mathematics, 65(10), 1331-1398. Kondor, R., \u0026amp; Trivedi, S. (2018). On the generalization of equivariance and convolution in neural networks to the action of compact groups. In International conference on machine learning (pp. 2747-2755). PMLR. ","permalink":"http://localhost:1313/posts/geo_deeplearning1/","summary":"\u003ch2 id=\"group-actions\"\u003eGroup Actions\u003c/h2\u003e\n\u003cp\u003eIn geometric deep learning, we are often more concerned with how groups act on our data. We assume the data comes from some domain $\\Omega$.\nWe denote the group as $\\mathfrak{G}$, and define the group action on the domain $\\Omega$ as a map:\n$$ (\\mathfrak{g},u) \\mapsto \\mathfrak{g}.u$$\nAs one might imagine, when we apply $\\mathfrak{G}$ to $\\Omega$, we simultaneously obtain the action of $\\mathfrak{G}$ on $\\mathcal{X}(\\Omega)$, given by:\n$$\\mathfrak{g}.x(u) = x(\\mathfrak{g}^{-1}.u)$$\u003c/p\u003e","title":"Introduction to Geometric Deep Learning (Continued)"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f} \\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$: $$ \\mathcal{R} = \\mathbb{E}_{(x,y) \\sim P}[L(\\widetilde{f}(x), f(x))] $$\nSuch constructed functions can approximate almost any function (Universal Approximation Theorem), but this does not mean we do not need inductive biases to constrain the learning problem. Given a function class $\\mathcal{F}$ with universal approximation capabilities, we can define a complexity function: $$c: \\mathcal{F} \\rightarrow \\mathbb{R}$$ defining our interpolation problem as: $$\\widetilde{f} = \\arg\\min_{g \\in \\mathcal{F}} c(g) \\quad s.t. \\quad \\widetilde{f}(x_i) = f(x_i), \\quad i = 1, . . . , N$$\nTherefore, we not only hope for good function fitting but also low function complexity. Such a complexity function is what we call the norm of the function. The benefit of this definition is that $\\mathcal{F}$ becomes a Banach space (a complete vector space equipped with a norm), allowing us to use tools from functional analysis to study their properties.\nGeometric Priors To overcome the \u0026ldquo;curse of dimensionality\u0026rdquo; of high-dimensional data, we must utilize the physical characteristics of the data (symmetry and scale separation); although the geometry of the data (Domain) may be complex, the signals defined on it constitute a Hilbert space with good mathematical properties, allowing us to still use tools from linear algebra and functional analysis to handle them.\nFor example, in image processing, data is typically represented as signals (pixel values) defined on a two-dimensional Euclidean space $\\mathbb{R}^2$. This space possesses translation and rotation symmetry, meaning that if we translate or rotate the image, its content and structure remain unchanged. Convolutional Neural Networks (CNNs) exploit this translation symmetry, capturing local features through shared weights, thereby improving learning efficiency and generalization ability.\nSymmetry and Invariance In machine learning, symmetry refers to the property that data or tasks remain unchanged under certain transformations. These transformations can be translation, rotation, scaling, etc. For example, in an image classification task, if we translate or rotate an image, the category of the image usually does not change. This invariance is what we hope the model can capture and utilize.\nSymmetry Groups Symmetry can be formalized through group theory. A group is a set equipped with a binary operation that satisfies properties such as closure, associativity, identity, and inverse.\nSymmetry operations form a group, called a symmetry group, proven as follows:\nClosure: If $g_1$ and $g_2$ are symmetry operations, then their composition $g_1 \\circ g_2$ is also a symmetry operation, because applying two symmetry transformations consecutively still preserves the invariance of the data. Associativity: If $g_1$ and $g_2$ are symmetry operations, then for any $g_3$, $(g_1 \\circ g_2) \\circ g_3 = g_1 \\circ (g_2 \\circ g_3)$, because the order of transformations does not affect the final result. Identity: There exists an identity operation $e$ such that for any symmetry operation $g$, $e \\circ g = g \\circ e = g$. This identity operation corresponds to performing no transformation. Inverse: For every symmetry operation $g$, there exists an inverse operation $g^{-1}$ such that $g \\circ g^{-1} = g^{-1} \\circ g = e$. This inverse operation corresponds to undoing the transformation. Note: $g \\circ h$ denotes applying transformation $g$ first, then transformation $h$.\nLet\u0026rsquo;s take an equilateral triangle as an example to illustrate the concept of symmetry groups: In this example, the symmetry group of the equilateral triangle contains six elements: three rotation operations (0 degrees, 120 degrees, 240 degrees) and three reflection operations (perpendicular lines through each vertex). These operations satisfy the four properties of a group, thus forming a group called $D_3$, the dihedral group of the triangle. Mathematically, we can view these operations as mappings: $$g: \\mathcal{X} \\rightarrow \\mathcal{X}$$ where $\\mathcal{X}$ is the data space. For each element $g \\in G$ in a symmetry group $G$, we have a corresponding mapping $g$. For example, $D_3$ can also be represented as a matrix group: $$ D_3 = \\left\\{ \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 2 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 3 \u0026amp; 1 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 1 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 3 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 1 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 2 \u0026amp; 1 \\end{pmatrix} \\right\\} $$\nReferences Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., \u0026amp; Vandergheynst, P. (2017). Geometric deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4), 18-42. Goodfellow, I., Bengio, Y., \u0026amp; Courville, A. (2016). Deep learning. MIT press. Cohen, T. S., \u0026amp; Welling, M. (2016). Group equivariant convolutional networks. In International conference on machine learning (pp. 2990-2999). PMLR. Kondor, R., \u0026amp; Trivedi, S. (2018). On the generalization of equivariance and convolution in neural networks to the action of compact groups. In International conference on machine learning (pp. 2747-2755). PMLR. Wood, T., \u0026amp; Shawe-Taylor, J. (1996). Representation theory and invariant neural networks. Neural computation, 8(5), 1003-1013. Mallat, S. (2016). Understanding deep convolutional networks. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065), 20150203. Lee, J. M. (2013). Introduction to smooth manifolds. Springer Science \u0026amp; Business Media. ","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"Group Actions In geometric deep learning, we are often more concerned with how groups act on our data. We assume the data comes from some domain $\\Omega$. We denote the group as $\\mathfrak{G}$, and define the group action on the domain $\\Omega$ as a map: $$ (\\mathfrak{g},u) \\mapsto \\mathfrak{g}.u$$ As one might imagine, when we apply $\\mathfrak{G}$ to $\\Omega$, we simultaneously obtain the action of $\\mathfrak{G}$ on $\\mathcal{X}(\\Omega)$, given by: $$\\mathfrak{g}.x(u) = x(\\mathfrak{g}^{-1}.u)$$\nThe group actions we will encounter most frequently are linear group actions, which satisfy: $$\\mathfrak{g}.(\\alpha x + \\beta x\u0026rsquo;) = \\alpha (\\mathfrak{g}.x) + \\beta (\\mathfrak{g}.x\u0026rsquo;)$$\nCurrying Original definition: View the transformation as a function taking two arguments: $f(\\mathfrak{g},x) \\mapsto \\mathfrak{g}.x$ (input an action and a signal, output the transformed signal). After Currying: View it as a function taking only one argument $\\mathfrak{g}$, but returning another function $\\rho(\\mathfrak{g})$. This $\\rho(\\mathfrak{g})$ is specifically responsible for processing the signal $x$. Mathematically, this $\\rho$ is called a Group Representation.\nFormal Definition of Group Representation An $n$-dimensional real representation is a map $\\rho: \\mathfrak{G} \\to \\mathbb{R}^{n \\times n}$ that must satisfy the following homomorphism condition: $$\\rho(\\mathfrak{gh}) = \\rho(\\mathfrak{g})\\rho(\\mathfrak{h})$$\nThis ensures that the algebraic structure of the group (the order of actions) is perfectly preserved in matrix operations. Unitary/Orthogonal Representations: If $\\rho(\\mathfrak{g})$ is always a unitary or orthogonal matrix, the representation is called a unitary or orthogonal representation. Such representations are very important in physics and stability analysis because they preserve the norm of the signal. Thus, written in the language of group representations, the action of $\\mathfrak{G}$ on $x \\in \\mathcal{X}(\\Omega)$ can also be defined as $\\rho (\\mathfrak{g})x(u) = x(\\mathfrak{g}^{-1}.u)$.\nInvariance and Equivariant Functions Since the domain $\\Omega$ of the data distribution possesses symmetry, this provides us with a strong inductive bias, reducing the need for unnecessary interpolation.\nIf $f(\\rho(\\mathfrak{g})x) = f(x), \\forall x \\in \\mathcal{X}(\\Omega), \\mathfrak{g} \\in \\mathfrak{G}$, we say that the function $f$ is $\\mathfrak{G}$-invariant. In other words, the group action does not affect the output result.\nThe figure above clearly illustrates the relationship between group representations, symmetry groups, signals, domains, and equivariant functions.\nIf $f(\\rho(\\mathfrak{g})x) = \\rho(\\mathfrak{g})f(x), \\forall x \\in \\mathcal{X}(\\Omega), \\mathfrak{g} \\in \\mathfrak{G}$, we say that the function $f$ is $\\mathfrak{G}$-equivariant. That is, applying the group action to the input or the output makes no difference.\nTaking CNNs (Convolutional Neural Networks) as an example, let $\\mathfrak{t} \\in \\mathfrak{T}$ be a translation operator. Clearly, we have: $$conv(\\mathfrak{t} * x) = \\mathfrak{t} * conv(x)$$ This is because the convolution kernel is globally shared across translations (the convolution operation involves the kernel sliding across the image acting on pixels). Here, the convolution operation is what we call $f$, meaning $f$ is $\\mathfrak{T}$-equivariant.\nIsomorphism and Automorphism Set Level View the domain $\\Omega$ as a set. Its only attribute is \u0026ldquo;Cardinality\u0026rdquo;, i.e., how many elements are in the set. Structure-preserving map: Bijection.\nTopological Level $\\Omega$ is a topological space, and we start to care about \u0026ldquo;proximity\u0026rdquo; between points. Structure-preserving map: Homeomorphism.\nDifferential Level $\\Omega$ is a differentiable manifold. We require not only continuity but also the ability to perform calculus. Structure-preserving map: Diffeomorphism, denoted as Diff($\\Omega$).\nAs the level increases, the symmetry group becomes smaller. In fact, increasing the level is equivalent to finding subgroups (a subset satisfying group operation rules).\nDeformation Stability Stability to Signal Deformations When dealing with signals (such as images $x$), we intuitively believe that slight deformations should not change the output $f(x)$. However, we face two dilemmas in mathematical definitions:\nSmall deformations do not form a \u0026ldquo;group\u0026rdquo;: Although small deformations look like some kind of symmetry, composing multiple small deformations can result in a large deformation. Mathematically, these \u0026ldquo;small deformations\u0026rdquo; themselves do not constitute a closed transformation group.\nThe full diffeomorphism group is too strong: If we require invariance under all differentiable transformations ($Diff(\\Omega)$), this is too excessive. Because drastic deformations change the semantics of an image (e.g., stretching the digit \u0026ldquo;3\u0026rdquo; into an \u0026ldquo;8\u0026rdquo;), we do not want the model to be \u0026ldquo;unresponsive\u0026rdquo; to such drastic changes.\nTherefore, we no longer pursue absolute Invariance, but rather Geometric Stability. The core idea is: the degree of change in the output should be bounded by the \u0026ldquo;magnitude\u0026rdquo; of the deformation. $$|| f(\\rho(\\tau)x) - f(x)|| \\leq C c(\\tau) ||x||$$ $c(\\tau)$: The \u0026ldquo;complexity\u0026rdquo; or \u0026ldquo;magnitude\u0026rdquo; of the deformation. Here, we usually assume $\\tau$ is a diffeomorphism. $C$: A constant. Intuitive understanding: If the deformation $\\tau$ is very small (belonging to some symmetry subgroup $G$, such as translation, where $c(\\tau)=0$), then the output remains almost unchanged. If the deformation is large, the output is allowed to change accordingly, but we know the upper bound of this change.\nHow to measure the magnitude of deformation? (Dirichlet Energy) The second image gives a specific measurement method, namely Dirichlet energy.\nFor images defined on the Euclidean plane, if we view the deformation as a displacement field $\\tau(u)$ (i.e., point $u$ is moved to $u+\\tau(u)$), then the cost of deformation can be defined as: $$c^2(\\tau) := \\int_{\\Omega}||\\nabla\\tau(u)||^2du$$ This metric measures the Elasticity of $\\tau$. It actually measures the degree to which the deformation deviates from a \u0026ldquo;constant vector field (i.e., pure translation)\u0026rdquo;. If $\\tau$ is just a simple translation (constant displacement), $\\nabla\\tau$ is 0, so $c(\\tau)$ is 0; if $\\tau$ causes drastic local distortion in the image, $\\nabla\\tau$ will be large, leading to an increased penalty term.\nThe figure above shows the relationship between deformations of different degrees. $Aut(\\Omega)$ is the automorphism group. Requiring $\\mathfrak{G}$-invariance or $\\mathfrak{G}$-equivariance is too harsh for images and not very applicable to real-world situations. The reason we pursue geometric stability is to find a larger group $\\mathfrak{G}\u0026rsquo;$ that satisfies geometric stability.\nStability to Domain Deformations What if the domain of the data itself changes? Application scenarios:\nGraphs: Social networks add or remove relationships (edges) over time. Manifolds: A 3D character performing non-rigid motion (like bending over), where the geometric structure of its surface changes. Domain distance $d(\\Omega, \\tilde{\\Omega})$: To measure how \u0026ldquo;similar\u0026rdquo; two domains are. Metrics between domains are introduced: For graphs, Graph Edit Distance can be used. For manifolds, Gromov-Hausdorff distance can be used. Domain stability formula: Formula (5) is a generalization of signal stability: $$ |f(x, \\Omega) - f(\\tilde{x}, \\tilde{\\Omega})| \\leq C |x| d_{\\mathcal{D}}(\\Omega, \\tilde{\\Omega}) $$ This means: If two graphs or two manifold structures are very close, the results obtained by the model processing them should also be very close.\nWe will discuss stability regarding domain deformations in future posts.\nReferences Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., \u0026amp; Vandergheynst, P. (2017). Geometric deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4), 18-42. Cohen, T. S., \u0026amp; Welling, M. (2016). Group equivariant convolutional networks. In International conference on machine learning (pp. 2990-2999). PMLR. Mallat, S. (2012). Group invariant scattering. Communications on Pure and Applied Mathematics, 65(10), 1331-1398. Kondor, R., \u0026amp; Trivedi, S. (2018). On the generalization of equivariance and convolution in neural networks to the action of compact groups. In International conference on machine learning (pp. 2747-2755). PMLR. ","permalink":"http://localhost:1313/posts/geo_deeplearning1/","summary":"\u003ch2 id=\"group-actions\"\u003eGroup Actions\u003c/h2\u003e\n\u003cp\u003eIn geometric deep learning, we are often more concerned with how groups act on our data. We assume the data comes from some domain $\\Omega$.\nWe denote the group as $\\mathfrak{G}$, and define the group action on the domain $\\Omega$ as a map:\n$$ (\\mathfrak{g},u) \\mapsto \\mathfrak{g}.u$$\nAs one might imagine, when we apply $\\mathfrak{G}$ to $\\Omega$, we simultaneously obtain the action of $\\mathfrak{G}$ on $\\mathcal{X}(\\Omega)$, given by:\n$$\\mathfrak{g}.x(u) = x(\\mathfrak{g}^{-1}.u)$$\u003c/p\u003e","title":"Introduction to Geometric Deep Learning (Continued)"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"}]